# Path D Editor Instructions

æœ¬æ–‡æª”èªªæ˜å¦‚ä½•åœ¨ Editor æ¨¡å¼ä¸‹ä¿®æ”¹å’Œæ“´å…… Path D Engineã€‚

---

## ğŸ“ æª”æ¡ˆçµæ§‹

```
jgod/path_d/
â”œâ”€â”€ path_d_types.py          # å‹åˆ¥å®šç¾©ï¼ˆState, Action, Configï¼‰
â”œâ”€â”€ rl_state_encoder.py      # State ç·¨ç¢¼å™¨
â”œâ”€â”€ rl_action_space.py       # Action ç©ºé–“ï¼ˆåƒæ•¸èª¿æ•´é‚è¼¯ï¼‰
â”œâ”€â”€ rl_reward.py             # Reward å‡½æ•¸
â”œâ”€â”€ rl_agent.py              # RL Agentï¼ˆå¯æ›¿æ›ï¼‰
â”œâ”€â”€ rl_training_loop.py      # è¨“ç·´è¿´åœˆ
â””â”€â”€ path_d_engine.py         # ä¸»å¼•æ“ï¼ˆAPI å…¥å£ï¼‰
```

---

## ğŸ”§ å¸¸è¦‹ä¿®æ”¹å ´æ™¯

### 1. æ–°å¢ Action ç¶­åº¦

**ç›®æ¨™**ï¼šè®“ Agent å¯ä»¥èª¿æ•´æ›´å¤šåƒæ•¸

**æ­¥é©Ÿ**ï¼š

1. ä¿®æ”¹ `path_d_types.py` çš„ `PathDAction`ï¼š
   ```python
   @dataclass
   class PathDAction:
       # ... ç¾æœ‰æ¬„ä½ ...
       delta_new_param: float = 0.0  # æ–°å¢æ¬„ä½
   ```

2. ä¿®æ”¹ `rl_action_space.py` çš„ `apply_action_to_params()`ï¼š
   ```python
   def apply_action_to_params(...):
       # ... ç¾æœ‰é‚è¼¯ ...
       new_param = current_params.get("new_param", 0.0) + action.delta_new_param * scale
       new_params["new_param"] = np.clip(new_param, min_val, max_val)
   ```

3. æ›´æ–° `rl_agent.py` çš„ `action_dim`ï¼š
   ```python
   action_dim = 6  # å¾ 5 æ”¹ç‚º 6
   ```

4. åœ¨ `rl_state_encoder.py` ä¸­ï¼Œå¦‚æœæ–°åƒæ•¸éœ€è¦åŠ å…¥ stateï¼Œæ›´æ–° `PathDState` å’Œ `encode_state_to_vector()`ã€‚

### 2. èª¿æ•´ Reward æ¬Šé‡

**ç›®æ¨™**ï¼šæ”¹è®Š Agent çš„å­¸ç¿’ç›®æ¨™

**æ­¥é©Ÿ**ï¼š

ä¿®æ”¹ `rl_reward.py` çš„ `compute_reward()` å‡½æ•¸ï¼š

```python
def compute_reward(...):
    base = sharpe
    
    # èª¿æ•´ penalty ä¿‚æ•¸
    penalty_breach = -10.0 * breach_ratio  # å¾ -5.0 æ”¹ç‚º -10.0ï¼ˆæ›´é‡è¦– breachï¼‰
    
    # æˆ–æ–°å¢å…¶ä»–æŒ‡æ¨™
    penalty_new_metric = -0.5 * new_metric
    
    reward = base + penalty_dd + penalty_breach + penalty_turnover + penalty_new_metric
    return reward
```

### 3. æ“´å…… State Space

**ç›®æ¨™**ï¼šè®“ Agent è§€å¯Ÿæ›´å¤šè³‡è¨Š

**æ­¥é©Ÿ**ï¼š

1. ä¿®æ”¹ `path_d_types.py` çš„ `PathDState`ï¼š
   ```python
   @dataclass
   class PathDState:
       # ... ç¾æœ‰æ¬„ä½ ...
       new_metric: float = 0.0  # æ–°å¢æ¬„ä½
   ```

2. ä¿®æ”¹ `rl_state_encoder.py` çš„ `build_pathd_state_from_pathb()`ï¼š
   ```python
   def build_pathd_state_from_pathb(...):
       # ... ç¾æœ‰é‚è¼¯ ...
       new_metric = extract_from_window_result(window_result)
       state = PathDState(..., new_metric=new_metric)
   ```

3. ä¿®æ”¹ `encode_state_to_vector()`ï¼š
   ```python
   def encode_state_to_vector(state: PathDState) -> np.ndarray:
       vector = np.array([
           # ... ç¾æœ‰æ¬„ä½ ...
           state.new_metric,  # æ–°å¢
       ], dtype=np.float32)
   ```

4. æ›´æ–° `rl_agent.py` çš„ `state_dim` å’Œ `rl_training_loop.py` çš„ state_dim è¨­å®šã€‚

### 4. å‡ç´š RL æ¼”ç®—æ³•

**ç›®æ¨™**ï¼šæ›¿æ›ç°¡åŒ–ç‰ˆ REINFORCE ç‚ºæ›´å…ˆé€²çš„æ–¹æ³•

**æ­¥é©Ÿ**ï¼š

1. å»ºç«‹æ–°çš„ Agent é¡åˆ¥ï¼ˆä¾‹å¦‚ `PPOAgent`ï¼‰ï¼š
   ```python
   class PPOAgent:
       def __init__(self, state_dim, action_dim, ...):
           # ä½¿ç”¨ PyTorch å¯¦ä½œ
           ...
       
       def select_action(self, state, deterministic=False):
           ...
       
       def train_step(self):
           ...
   ```

2. ä¿®æ”¹ `rl_agent.py`ï¼Œå°‡ `SimpleGaussianPolicyAgent` æ›¿æ›ç‚ºæ–°å¯¦ä½œï¼Œæˆ–ä¿ç•™å…©å€‹é¸é …ï¼š
   ```python
   # åœ¨ __init__.py ä¸­
   from jgod.path_d.rl_agent import SimpleGaussianPolicyAgent
   # æˆ–
   from jgod.path_d.rl_agent_ppo import PPOAgent
   ```

3. æ›´æ–° `rl_training_loop.py` å’Œ `path_d_engine.py` ä¸­çš„ Agent åˆå§‹åŒ–ã€‚

**æ³¨æ„**ï¼šå‡ç´šåˆ°æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼ˆPyTorch/TensorFlowï¼‰å¾Œï¼Œéœ€è¦æ›´æ–°ä¾è³´é …ã€‚

### 5. èª¿æ•´åƒæ•¸ç¯„åœ

**ç›®æ¨™**ï¼šæ”¹è®Š Agent å¯èª¿æ•´çš„åƒæ•¸ç¯„åœ

**æ­¥é©Ÿ**ï¼š

ä¿®æ”¹ `rl_action_space.py` çš„ `apply_action_to_params()` ä¸­çš„ clip ç¯„åœï¼š

```python
# ä¾‹å¦‚ï¼šæ”¾å¯¬ Sharpe é–€æª»ç¯„åœ
new_sharpe_floor = np.clip(new_sharpe_floor, -2.0, 5.0)  # å¾ [-1.0, 3.0] æ”¹ç‚º [-2.0, 5.0]
```

### 6. ä¿®æ”¹è¨“ç·´è¿´åœˆé‚è¼¯

**ç›®æ¨™**ï¼šæ”¹è®Šè¨“ç·´æµç¨‹ï¼ˆä¾‹å¦‚åŠ å…¥é©—è­‰éšæ®µï¼‰

**æ­¥é©Ÿ**ï¼š

ä¿®æ”¹ `rl_training_loop.py` çš„ `train_path_d()` å‡½æ•¸ï¼š

```python
def train_path_d(config: PathDTrainConfig) -> PathDTrainResult:
    # ... ç¾æœ‰è¨“ç·´è¿´åœˆ ...
    
    # æ¯ N å€‹ episode åŸ·è¡Œä¸€æ¬¡é©—è­‰
    if episode % 10 == 0:
        validation_reward = evaluate_on_validation_set(agent, validation_config)
        logger.info(f"Validation reward: {validation_reward:.2f}")
```

---

## ğŸ§ª æ¸¬è©¦å»ºè­°

ä¿®æ”¹å¾Œï¼Œå»ºè­°åŸ·è¡Œä»¥ä¸‹æ¸¬è©¦ï¼š

1. **å–®å…ƒæ¸¬è©¦**ï¼š
   ```bash
   PYTHONPATH=. pytest tests/path_d/test_state_encoder.py -v
   PYTHONPATH=. pytest tests/path_d/test_reward_function.py -v
   ```

2. **Smoke Test**ï¼š
   ```bash
   PYTHONPATH=. pytest tests/path_d/test_path_d_engine_smoke.py -v
   ```

3. **å®Œæ•´æ¸¬è©¦**ï¼š
   ```bash
   PYTHONPATH=. pytest tests/path_d -q -v
   ```

---

## ğŸ“ ç¨‹å¼ç¢¼é¢¨æ ¼

è«‹éµå¾ªä»¥ä¸‹è¦ç¯„ï¼š

1. **å‹åˆ¥è¨»è§£**ï¼šæ‰€æœ‰å‡½æ•¸åƒæ•¸å’Œè¿”å›å€¼éƒ½è¦æœ‰å‹åˆ¥è¨»è§£
2. **Docstring**ï¼šæ‰€æœ‰å…¬é–‹å‡½æ•¸å’Œé¡åˆ¥éƒ½è¦æœ‰ docstring
3. **PEP 8**ï¼šéµå¾ª Python ç¨‹å¼ç¢¼é¢¨æ ¼è¦ç¯„
4. **è¨»è§£**ï¼šåœ¨è¤‡é›œé‚è¼¯è™•åŠ ä¸Šè¨»è§£èªªæ˜

---

## ğŸ” Debug æŠ€å·§

### 1. æª¢æŸ¥ State å‘é‡

åœ¨ `rl_state_encoder.py` ä¸­åŠ å…¥ debug printï¼š

```python
def encode_state_to_vector(state: PathDState) -> np.ndarray:
    vector = ...
    if np.isnan(vector).any():
        logger.warning(f"NaN detected in state vector: {vector}")
    return vector
```

### 2. æª¢æŸ¥ Reward è¨ˆç®—

åœ¨ `rl_reward.py` ä¸­åŠ å…¥è©³ç´°æ—¥èªŒï¼š

```python
def compute_reward(...):
    reward = ...
    logger.debug(f"Reward breakdown: base={base}, penalty_dd={penalty_dd}, ...")
    return reward
```

### 3. æª¢æŸ¥ Agent å‹•ä½œ

åœ¨ `rl_training_loop.py` ä¸­åŠ å…¥å‹•ä½œçµ±è¨ˆï¼š

```python
action_vec = agent.select_action(state_vec)
logger.debug(f"Action: {action_vec}, Mean: {action_vec.mean()}, Std: {action_vec.std()}")
```

---

## ğŸ“š åƒè€ƒè³‡æº

- **RL åŸºç¤**ï¼š[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/)
- **Policy Gradient**ï¼š[Policy Gradient Methods](https://spinningup.openai.com/en/latest/algorithms/vpg.html)
- **Path B è¦æ ¼**ï¼š`spec/JGOD_PathBEngine_Spec.md`

---

## âš ï¸ é‡è¦æé†’

1. **ä¿æŒå‘å¾Œå…¼å®¹**ï¼šä¿®æ”¹æ™‚ç›¡é‡ä¸ç ´å£ç¾æœ‰ API
2. **æ›´æ–°æ–‡ä»¶**ï¼šä¿®æ”¹å¾Œè¨˜å¾—æ›´æ–°ç›¸é—œæ–‡ä»¶
3. **æ¸¬è©¦è¦†è“‹**ï¼šæ–°å¢åŠŸèƒ½æ™‚ï¼Œè¨˜å¾—æ–°å¢å°æ‡‰çš„æ¸¬è©¦
4. **ç‰ˆæœ¬æ§åˆ¶**ï¼šé‡å¤§è®Šæ›´æ™‚ï¼Œè€ƒæ…®å¢åŠ ç‰ˆæœ¬è™Ÿ

