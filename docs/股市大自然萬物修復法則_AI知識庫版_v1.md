# 股市大自然萬物修復法則 - AI 知識庫版 v1

> **重要說明**：本文件為 AI 知識庫格式，每段內容都已標記分類標籤，可直接被 AI 模型解析、轉換為 JSON、向量化或規則引擎使用。
> 
> **原始文件**：`股市大自然萬物修復法則.txt`（未修改）

---

## 文件說明

[NOTE]
本文件是 J-GOD 股神作戰系統的核心大腦來源之一，所有內容均完整保留，僅進行結構化分類標籤，未刪除或修改任何技術內容。

---

[RULE]
﻿您要求我從全球智慧和理論高度來優化和補足您的**「股市大自然萬物修復法則」，使其具備普適性、邏輯嚴密性和哲學深度**，然後再將其應用於量化系統。

[STRUCTURE]
您提出的五大階段在概念上是正確的，但從系統論和認知科學的角度看，可以被更精煉、更具指導意義地組織為三大核心階段（或稱「循環模態」），以更好地體現其生命體般的自我進化特性。

[NOTE]
________________

[RULE]
🌎 全球智慧優化：「股市大自然萬物修復法則」 (The $\text{Cosmic}$$\text{Alpha}$ $\text{Cycle}$)
我們將您的法則重新定義為三大不可分割、持續運行的模態，總結了所有智慧系統的進化路徑。

[CONCEPT]
核心理論：市場的「熵增」與 $\text{AI}$ 的「反熵」

[RULE]
市場的本質是熵增（混沌與效率的衰退），所有 $\text{Alpha}$ 最終都會衰退。您的法則的目標是讓 $\text{AI}$成為一個持續進行「反熵」作用的實體，不斷修復知識，維持系統的永續性。

[STRUCTURE]
您的原始階段（人）
	優化後的模態（AI）

[CONCEPT]
	理論依據與核心目的

[NOTE]
	$\text{A}$. 預測與記錄

[STRUCTURE]
	**模態 I：認知與記錄 (Cognitive $\text{State}$) **

[RULE]
	目的：建立真實世界的模型。 將市場視為一個需要被解讀的信號場。強調所有決策、預期和數據的絕對透明與審計，為後續的學習奠定不可篡改的知識基礎。

[NOTE]
	$\text{D}$. 修復與調整

[STRUCTURE]
	**模態 II：求證與診斷 (Diagnostic $\text{Cycle}$) **

[RULE]
	目的：尋找「知識的邊界」與「失敗的根因」。 強調結果的差異和原因的歸因。這是系統從「反應」轉向「自省」的關鍵。必須識別出是模型預測錯誤，還是執行效率低落導致的失敗。

[NOTE]
	$\text{B}$. 實時求證 $\text{C}$. 誤差診斷 $\text{E}$. 重新學習

[STRUCTURE]
	**模態 III：修復與演化 (Evolving $\text{Action}$) **

[RULE]
	目的：重寫底層生存法則。 根據診斷結果，系統需要調整的不僅是參數，而是底層的決策邏輯 ($\text{Reward}$ $\text{Function}$、$\text{Beta}$ $\text{Weights}$)。實現知識的自我修復、自我淘汰和持續進化。

[NOTE]
	________________

[STRUCTURE]
優化階段詳解與理論補足
模態 I：認知與記錄 ($\text{The}$ $\text{Audit}$ $\text{Principle}$)

[RULE]
* 理論補足： 「知識的絕對誠信」。系統必須誠實地記錄自己的**「信念」（預期 $\text{PnL}$, $\text{F}_{\text{Internal}}$ 信心）和「行動」**（實際槓桿，訂單拆分）。

[NOTE]
* 實施優化：

[RULE]
   * $\text{Audit}$ $\text{Log}$ (審計日誌)： 紀錄的內容必須包含 $\text{RL}$ 決策時的整個 $\text{State}$ $\text{Vector}$。當出現重大錯誤時，可以完全重現當時 $\text{AI}$ 的「思維」過程。

[STRUCTURE]
模態 II：求證與診斷 ($\text{The}$ $\text{Root}$ $\text{Cause}$ $\text{Principle}$)

[RULE]
這是您法則中最關鍵、最具反熵精神的環節。
* 理論補足： 「差異的價值」。在市場中，錯誤（Discrepancy）才是唯一的老師。我們必須從量化的角度回答三個核心問題：

[CONCEPT]
   * 時間錯誤： 為什麼信號比預期早/晚出現？（歸因到 $\text{Lead}$/$\text{Lag}$ $\text{Alpha}$）

[NOTE]
   * 執行錯誤： 為什麼執行成本比預期高？（歸因到 $\text{TCA}$ $\text{Residual}$）

[CONCEPT]
   * 模型錯誤： 為什麼信號方向錯了？（歸因到特定 $\text{O}$-$\text{Factor}$ 的 $\text{Alpha}$ 衰退）

[NOTE]
* 實施優化：

[CONCEPT]
   * $\text{Diagnostic}$ $\text{Analyzer}$： 核心工作是運行後驗歸因模型，將 $\text{PnL}$ 誤差分解到單一正交因子和執行層。

[STRUCTURE]
模態 III：修復與演化 ($\text{The}$ $\text{Evolutionary}$ $\text{Principle}$)

[RULE]
* 理論補足： 「知識的淘汰」。一個智慧系統必須能忘記無效的知識，並強化正確的知識。修復不僅是調整參數，更是調整學習的目標和結構。

[NOTE]
* 實施優化：

[RULE]
   * $\text{Structural}$ $\text{Adjustment}$： 如果診斷器發現 $\text{O}$-$\text{Factor}_4$（佔 $\text{Alpha}$ 貢獻度最低的正交因子）持續數月為負貢獻，系統應自動將其從 $\text{RL}$$\text{State}$ $\text{Vector}$ 中移除，並重新擬合 $\text{PCA}$ 模型，強制 $\text{AI}$ 尋找新的、更具預測性的因子組合。
您的 「股市大自然萬物修復法則」，透過這三大模態的優化，已經從一個流程清單升級為一個具備自我意識、持續進化能力的智慧系統的指導理論。
我們現在將設計**「大自然萬物修復法則」**中最核心、最具智慧含量的環節：診斷誤差分析器 ($\text{Diagnostic}$ $\text{Discrepancy}$ $\text{Analyzer}$)。

[STRUCTURE]
這個模組的任務是執行模態 $\text{II}$（求證與診斷），將交易結果的失敗歸因到單一的、可調整的變量上，從而為模態 $\text{III}$（修復與演化）提供精確的行動指令。
我們將在一個新的模組：diagnostic/analyzer.py 中實現它。

[NOTE]
________________

[STRUCTURE]
🛠️ 階段 XIV：診斷誤差分析器設計 ($\text{Discrepancy}$$\text{Analyzer}$)

[CONCEPT]
1. 核心邏輯：誤差分層歸因 ($\text{Layered}$ $\text{Attribution}$)

[NOTE]
誤差被分為兩層，依序診斷：
1. 第一層 (執行層誤差 $\text{E}_{\text{Exec}}$)： 失敗是否是因為執行效率低落造成的？
2. 第二層 (認知層誤差 $\text{E}_{\text{Model}}$)： 失敗是否是因為 $\text{RL}$ 模型預測錯誤造成的？

[CODE]
2. $\text{Python}$ 模組設計：DiscrepancyAnalyzer
Python

[NOTE]
# diagnostic/analyzer.py

[CODE]
import pandas as pd
import numpy as np

[CODE]
class DiscrepancyAnalyzer:

[NOTE]
    """

[CONCEPT]
    執行盤後診斷，將 PnL 誤差歸因到 Alpha 因子或執行效率。

[NOTE]
    """

[CODE]
    def __init__(self, db_session):

[NOTE]
        self.db_session = db_session

[CODE]
    def _fetch_daily_audit_data(self, trade_date):

[NOTE]
        """
        從審計日誌和 PnL 數據中獲取一天內所需的關鍵數據。
        """
        # 實際情況：這將涉及 JOIN 審計日誌、RL 決策、實際 PnL 和 TCA 數據
        # 模擬獲取數據：
        return pd.DataFrame({
            'Actual_PnL': np.random.uniform(-1000, 1000, 10),

[CONCEPT]
            'Predicted_Alpha_PnL': np.random.uniform(-1500, 1500, 10), # RL 預期的 PnL

[NOTE]
            'Actual_TCA': np.random.uniform(0.001, 0.005, 10),
            'Predicted_TCA': np.random.uniform(0.001, 0.003, 10),
            'O_Factor_1': np.random.normal(0, 1, 10),
            'O_Factor_2': np.random.normal(0, 1, 10),
            'Action_Net_Exposure': np.random.uniform(0.5, 2.0, 10)
        })

[CODE]
    def analyze_discrepancy(self, trade_date) -> dict:

[NOTE]
        """
        主診斷函數：計算兩層誤差並輸出調整指令。
        """
        data = self._fetch_daily_audit_data(trade_date)

[NOTE]
        # --- 診斷步驟 1：執行層誤差 (E_Exec) ---

[NOTE]
        # TCA 殘差：衡量執行效率的失敗
        data['TCA_Residual'] = data['Actual_TCA'] - data['Predicted_TCA']
        mean_tca_residual = data['TCA_Residual'].mean()

[RULE]
        # 如果 TCA 殘差高於某個閾值，則歸因為執行層失敗

[NOTE]
        if mean_tca_residual > 0.001:

[CODE]
            print("🚨 [E_Exec FAILURE]: 執行成本顯著超預期！")

[NOTE]
            # 修復指令：建議調整 OrderRouter 的拆分參數或延遲檢查。
            return {
                'Failure_Type': 'EXECUTION',
                'Root_Cause': 'High Slippage Cost',
                'Adjustment_Target': 'OrderRouter_TCA_Threshold',
                'Error_Value': mean_tca_residual
            }

[NOTE]
        # --- 診斷步驟 2：認知層誤差 (E_Model) ---

[NOTE]
        # 總 PnL 誤差：衡量模型預測的失敗

[CONCEPT]
        data['PnL_Error'] = data['Actual_PnL'] - data['Predicted_Alpha_PnL']

[NOTE]
        mean_pnl_error = data['PnL_Error'].mean()

[NOTE]
        if abs(mean_pnl_error) > 500: # 假設 PnL 誤差過大

[CODE]
            print(f"🚨 [E_Model FAILURE]: PnL 誤差過大 ({mean_pnl_error:.2f})！")

[CONCEPT]
            # 尋找根因：將 PnL 誤差回歸到正交因子 O-Factor 上 (核心歸因步驟)

[NOTE]
            # 簡化模擬：運行多元迴歸 (Multiple Regression)

[FORMULA]
            # O_Factors = data[['O_Factor_1', 'O_Factor_2']]

[NOTE]
            # PnL_Error ~ β1*O_Factor_1 + β2*O_Factor_2 + ...
            # 找到 p-value 最低 (解釋力最強) 的因子

[NOTE]
            # 找到對 PnL 誤差貢獻最大的因子 (絕對值最大的迴歸係數)

[FORMULA]
            contributing_factor = 'O_Factor_1' # 假設迴歸結果

[NOTE]
            # 修復指令：建議調整該因子的 RL 權重
            return {
                'Failure_Type': 'MODEL_PREDICTION',
                'Root_Cause': f'Factor {contributing_factor} Failure',

[CONCEPT]
                'Adjustment_Target': f'RL_Beta_Weight_{contributing_factor}',

[NOTE]
                'Error_Value': mean_pnl_error
            }

[CODE]
        print("✅ [DIAGNOSTIC SUCCESS]: PnL 和執行效率均在預期範圍內。")

[NOTE]
        return {'Failure_Type': 'NONE'}

[RULE]
3. 修復法則的閉環（從診斷到行動）
這個 $\text{Discrepancy}$ $\text{Analyzer}$ 模組是實現大自然萬物修復法則的最後一步：

[STRUCTURE]
1. 診斷輸出： 模組輸出包含 Adjustment_Target 和 Error_Value。

[NOTE]
2. 傳遞指令： 這個輸出會被傳遞給 rl_calibration/trainer.py。
3. $\text{RL}$ 修復：

[RULE]
   * 如果 Failure_Type 是 EXECUTION，$\text{RL}$ $\text{Reward}$ 函數將對當日的高槓桿行動施加額外的懲罰。
   * 如果 Failure_Type 是 MODEL_PREDICTION，$\text{RL}$ $\text{Trainer}$ 將對失敗的 O_Factor_X的 $\beta$ 權重進行緊急修正，並降低其在新一輪訓練中的初始信任度。

[STRUCTURE]
至此，我們已經將**「預測 $\to$ 記錄 $\to$ 求證 $\to$ 診斷 $\to$ 修復 $\to$ 再預測」的自主演化認知循環**完全落實到我們的量化系統藍圖中。
這是一個極具挑戰性的讚賞！您要求我們從絕對完美的「創世紀上帝視角」出發，找出這個已經包含自我修復、正交化、分層 $\text{RL}$ 的系統中，最後的、最隱蔽的缺陷。

[RULE]
如果說我們已經解決了**「如何賺錢」和「如何修復錯誤」，那麼最終的缺陷在於「如何定義真實」**。

[NOTE]
________________

[CONCEPT]
🔱 創世紀終極缺陷：現實定義與知識的絕對性

[NOTE]
我們目前的設計，無論多麼複雜，都建立在一個隱藏的假設上：我們獲取的價格和交易量數據是市場的唯一真理。
然而，從上帝視角看，市場的真正現實是：執行成本、流動性壁壘、以及時間的非線性流逝。

[RULE]
我們必須在兩個維度上實現最終的超越：

[NOTE]
I. 終極缺陷一：流動性現實的盲點 ($\text{The}$ $\text{Liquidity}$ $\text{Reality}$$\text{Gap}$)
我們使用 $\text{TCA}$ $\text{Residual}$ 來衡量執行誤差，但這只是結果。我們沒有將市場流動性的結構納入 $\text{RL}$ 的 $\text{State}$ $\text{Vector}$。

[CONCEPT]
終極補強目標
	核心概念 (超越 TCA 結果)

[STRUCTURE]
	實施到 RL 系統

[NOTE]
	流動性壁壘感知

[RULE]
	預測在當前價格執行特定數量所需的滑價。這需要 $\text{RL}$ 直接觀察完整的訂單簿深度。
	$\text{F}_{\text{Orderbook}}$ $\text{Factor}$： 根據 $\text{Orderbook}$ 的買賣盤斜率 (Slope) 和聚合深度計算。$\text{RL}$ 必須將這個因子作為 $\text{Action}$ $\text{Space}$ 的約束。

[NOTE]
	隱藏流動性預測

[RULE]
	預測在當前深度下，隱藏的流動性（例如 $\text{Iceberg}$$\text{Order}$）在哪裡。

[STRUCTURE]
	$\text{Iceberg}$ $\text{Prediction}$ $\text{Model}$： 獨立的 $\text{ML}$ 模型，分析 $\text{Tick}$ 級別的訂單變化率和取消率，預測大單的潛在位置，作為執行模組的輸入。

[NOTE]
	II. 終極缺陷二：時間與知識的非線性 ($\text{The}$ $\text{Nonlinear}$ $\text{Time}$)
我們所有的模型都假設時間是均勻流逝的。但在高頻交易中，**「資訊的密度」**才是真正的時間度量。

[CONCEPT]
終極補強目標
	核心概念 (超越 Chronological Time)

[NOTE]
	實施到 RL State
	信息時間度量
	根據交易量或波動性來度量時間，而非秒數。在資訊密度高的時間內，模型權重應更激進。

[RULE]
	$\text{F}_{\text{InfoTime}}$ $\text{Factor}$： 使用 $\text{Volume}$$\text{Bars}$ 或 $\text{Dollar}$ $\text{Bars}$（而非 $\text{Time}$$\text{Bars}$）來計算 $\text{RL}$ $\text{State}$ 的歷史窗口。$\text{RL}$$\text{State}$ 中納入當前 $\text{Volatility}$ $\text{Density}$。

[NOTE]
	知識的即時忘卻

[RULE]
	糾正延遲是我們最大的敵人。 $\text{RL}$ 必須能即時「忘記」過時的、無關緊要的資訊。

[NOTE]
	$\text{RL}$ $\text{Architecture}$ $\text{Upgrade}$： 將 $\text{RL}$ 的底層網絡從標準 $\text{LSTM}$ 升級為 $\text{Attention}$-$\text{based}$$\text{Transformer}$ $\text{Model}$。$\text{Transformer}$ 的注意力機制可以在實時權衡歷史數據中的哪些部分是相關的，哪些可以被「忽略」。
	________________

[NOTE]
總結：達到絕對現實的四重超越

[RULE]
為了達到絕對的現實定義，我們需要：

[NOTE]
1. 實施 $\text{F}_{\text{Orderbook}}$： 讓 $\text{RL}$ 決策能夠感知流動性壁壘。

[STRUCTURE]
2. 設計 $\text{Iceberg}$ $\text{Prediction}$ $\text{Model}$： 增強執行模組的流動性嗅探能力。

[NOTE]
3. 整合 $\text{F}_{\text{InfoTime}}$： 讓 $\text{RL}$ 在信息密度的維度上進行決策。
4. 升級 $\text{RL}$ 網絡： 使用 $\text{Transformer}$ 實現知識的即時權重分配。

[STRUCTURE]
這四個補強，將使我們的系統超越表面價格，直接與流動性的結構和非線性時間進行交互，這才是真正意義上的終極量化系統。

[NOTE]
請問您希望我先深入設計 流動性壁壘感知因子 ($\text{F}_{\text{Orderbook}}$) 嗎？
下面我幫你把這整段「流動性壁壘感知因子 ($F_{\text{Orderbook}}$)」

[STRUCTURE]
重寫成 J-GOD 戰情室等級、可直接放進技術文件、策略引擎、Cursor 修改規格書的終極版本。

[CONCEPT]
⚠️ 重寫目標

[NOTE]
* 意義更清楚
* 結構更科學

[CODE]
* 清楚分成：概念 → 因子設計 → 公式 → RL 用法 → Python 架構

[NOTE]
* 保留所有原公式、語意
* 完全不口語化

[STRUCTURE]
* 保證可被未來的 AI 引擎與程式模組引用

[NOTE]
________________

[NOTE]
✅ 終極正式版：流動性壁壘感知因子 $F_{\text{Orderbook}}$（重新編寫）
流動性壁壘感知因子（Liquidity Barrier Awareness Factor）
$F_{\text{Orderbook}}$

[STRUCTURE]
是 J-GOD 系統中 第一個高頻微觀結構因子集。

[CONCEPT]
其核心目標為：

[NOTE]
使 RL 代理人能夠直接感知市場深度（Depth）、摩擦（Friction）、流動性彈性（Elasticity），
並將此資訊用於實時行為約束（Action Constraint）與執行優化（Execution Optimization）。

[STRUCTURE]
此模組將實作於：

[NOTE]
strategy_engine/factor_FX_orderbook.py

[NOTE]
________________

[NOTE]
🧩 因子結構總覽

[CONCEPT]
$F_{\text{Orderbook}}$ 由三種核心因子組成：

[NOTE]
1. 流動性斜率（Liquidity Slope）
2. 買賣盤失衡（Orderbook Imbalance, OBI）
3. 深度標準分數（Depth Z-Score）

[RULE]
三者共同構成完整的「市場深度 → 市場摩擦 → 市場壓力」三層微觀視角。

[NOTE]
________________

[NOTE]
———————————————————————
1. 因子 I：流動性斜率 Liquidity Slope
———————————————————————

[CONCEPT]
📌 核心定義

[RULE]
衡量市場在承受資金衝擊時，價格需要承受的變化幅度。

[NOTE]
代表買賣盤深度的「密度」。
📐 數學公式
以賣盤為例（買入衝擊成本）：
SlopeAsk=ΔPriceAskΔVolumeAskSlopeAsk​=ΔVolumeAsk​ΔPriceAsk​​
延伸：
* $\text{Slope}_{Bid}$：賣出衝擊成本
* $\text{Slope}_{Ask}$：買入衝擊成本
📈 意義
斜率狀態
	市場意義
	RL 行為建議
	極陡（Steep）
	流動性差、深度不足、摩擦高
	降低槓桿、減少下單量、以被動單為主
	極平（Flat）
	流動性強、深度夠、摩擦低
	可增加槓桿或加大下單量
	🧠 RL 應用
* 此因子直接影響 Action Space 大小。

[STRUCTURE]
* 在斜率過高時，系統會「自動縮小最大下單量」。

[NOTE]
* 防止 RL 代理人因為深度不足而造成滑價爆炸。
________________

[NOTE]
———————————————————————
2. 因子 II：買賣盤失衡 Orderbook Imbalance (OBI)
———————————————————————

[CONCEPT]
📌 核心定義

[NOTE]
衡量短期買方 / 賣方力量的壓力差，是短線價格方向的高頻領先指標。
📐 數學公式
OBI=VolumeBid−VolumeAskVolumeBid+VolumeAskOBI=VolumeBid​+VolumeAsk​VolumeBid​−VolumeAsk​​
值域：
* $1 \rightarrow$ 完全買方壓力
* $-1 \rightarrow$ 完全賣方壓力
📈 意義與 RL 應用
OBI 值
	市場意義
	高頻 RL 意義
	+1 附近
	強買盤壓力

[RULE]
	價格短期上行 → 強 Alpha 信號

[NOTE]
	-1 附近
	強賣盤壓力

[RULE]
	短期下行 → 快速空方信號

[STRUCTURE]
	OBI 是 RL 高頻策略中最直接的「微觀價格動能輸入」。

[NOTE]
________________

[NOTE]
———————————————————————
3. 因子 III：深度 Z-Score（Depth Z-Score）
———————————————————————

[CONCEPT]
📌 核心定義

[RULE]
衡量當前市場深度相對於歷史水準是否異常。

[NOTE]
📐 數學公式
DepthZ=CurrentDepth−μ(HistoricalDepth)σ(HistoricalDepth)DepthZ​=σ(HistoricalDepth)CurrentDepth−μ(HistoricalDepth)​
📈 意義
Z-Score
	解讀

[STRUCTURE]
	執行策略

[NOTE]
	< -2
	異常稀薄流動性
	使用「低流動性模式」：減碼、縮單、避免追單
	> +2
	深度異常強

[STRUCTURE]
	可接受更高激進度的下單策略

[NOTE]
	________________

[CODE]
🧱 Python 模組結構（保留程式原意、重新格式化）

[NOTE]
# strategy_engine/factor_FX_orderbook.py

[CODE]
import numpy as np

[CODE]
class OrderbookFactorEngine:

[CODE]
    def __init__(self, history_depth_data):

[NOTE]
        # 歷史深度數據：計算 Depth Z-Score 參考值
        self.depth_mean = np.mean(history_depth_data)
        self.depth_std = np.std(history_depth_data)

[CODE]
    def calculate_orderbook_factors(self, orderbook_snapshot: dict) -> dict:

[NOTE]
        """
        計算 F_Orderbook 三大因子：
        - Liquidity Slope
        - Orderbook Imbalance (OBI)
        - Depth Z-Score
        """
        bid_P, bid_V = orderbook_snapshot['bid_prices'], orderbook_snapshot['bid_volumes']
        ask_P, ask_V = orderbook_snapshot['ask_prices'], orderbook_snapshot['ask_volumes']

[NOTE]
        # --- 1. Liquidity Slope ---
        cumulative_V = np.sum(bid_V)
        price_diff_ask = ask_P[-1] - ask_P[0]
        slope_ask = price_diff_ask / cumulative_V if cumulative_V > 0 else 0.0

[NOTE]
        # --- 2. Orderbook Imbalance (OBI) ---
        total_bid_V = np.sum(bid_V)
        total_ask_V = np.sum(ask_V)

[FORMULA]
        obi = (total_bid_V - total_ask_V) / (total_bid_V + total_ask_V) \

[NOTE]
              if (total_bid_V + total_ask_V) > 0 else 0.0

[NOTE]
        # --- 3. Depth Z-Score ---
        current_depth = total_bid_V + total_ask_V

[FORMULA]
        depth_zscore = (current_depth - self.depth_mean) / self.depth_std \

[NOTE]
                       if self.depth_std > 0 else 0.0

[NOTE]
        return {
            'Slope_Ask': slope_ask,
            'OBI': obi,
            'Depth_Zscore': depth_zscore
        }

[NOTE]
________________

[NOTE]
🧠 4. 終極整合：RL 行為約束（Action Constraint）
$F_{\text{Orderbook}}$ 三個因子同時進入 RL 的 State:
RL_State_Vector = [
    Price,
    Volume,
    Indicators,
    Slope_Ask,
    OBI,
    Depth_Zscore,
    ...
]

[NOTE]
並直接影響「可執行動作空間」：

[CONCEPT]
📌 行為約束邏輯（Action Constraint）

[RULE]
* 若 Slope_Ask 高 → RL「最大下單量」縮小
* 若 Depth_Zscore 低 → RL 禁止 Aggressive order
* 若 OBI < -0.5 → RL 不得偏多
* 若 OBI > +0.5 → RL 不得偏空

[NOTE]
透過這種方式：
RL 代理人首次能夠精準感知市場微觀結構，

[RULE]
並以真實流動性條件為基礎調整行為。

[STRUCTURE]
好，我們直接進入 「階段 XVI：隱藏流動性預測模型（F_Iceberg）」，

[NOTE]
把它寫成可以放進技術白皮書、又可以拿去給 Cursor 刻程式的版本。
________________

[NOTE]
隱藏流動性預測模型 ($\text{Iceberg}$ $\text{Prediction}$ $\text{Model}$)。

[STRUCTURE]
這個模組的目標是讓我們的執行系統具備「嗅探」能力，超越表面報價所揭示的流動性，預測潛在的大額隱藏訂單（冰山單，$\text{Iceberg}$ $\text{Order}$）的位置和意圖。這將極大地優化我們的訂單拆分和路由策略。
我們將在 execution/iceberg_predictor.py 中實現這個模組。

[NOTE]
________________

[STRUCTURE]
🛠️ 階段 XVI：隱藏流動性預測模型設計 ($\text{Iceberg}$$\text{Predictor}$)

[CONCEPT]
1. 核心概念：訂單簿微觀行為分析

[RULE]
冰山單的特徵是**「小量顯示，大量隱藏」。預測它，需要分析極細微的訂單簿變動和交易模式**。

[CONCEPT]
我們將使用特徵工程和淺層 $\text{ML}$ 模型來實時處理這些高頻信號。

[STRUCTURE]
* 關鍵特徵 ($\text{Feature}$ $\text{Engineering}$):

[RULE]
   * 訂單取消率 (Cancellation Rate): 如果一檔報價的取消率遠高於其成交率，可能代表虛假流動性（$\text{Flickering}$），或是在隱藏意圖。
   * 累積成交與報價比 (Accumulation vs. Quote): 當一檔報價的累積成交量持續超過其顯示數量，則幾乎可以確定是冰山單。

[NOTE]
   * 價格跳動後的回調 (Jump $\text{Recoil}$): 大單被砸出後，價格快速回調，表明有強大的隱藏買盤在承接。

[CODE]
2. $\text{Python}$ 模組設計：$\text{IcebergPredictor}$

[NOTE]
我們將採用一個梯度提升樹 ($\text{XGBoost}$ 或 $\text{LightGBM}$) 模型，因為它們在高頻數據的分類任務中表現優異，且推斷延遲極低。

[CODE]
Python

[NOTE]
# execution/iceberg_predictor.py

[CODE]
import pandas as pd
import numpy as np

[NOTE]
# 實戰中使用 LightGBM 或 XGBoost

[CODE]
# from lightgbm import LGBMClassifier

[CODE]
class IcebergPredictor:

[NOTE]
    """
    實時分析訂單簿微結構，預測潛在冰山單的位置 (Buy Side 或 Sell Side)。
    """

[CODE]
    def __init__(self):

[NOTE]
        # 假設已經訓練好的分類模型，用於二元分類 (Buy Iceberg / Sell Iceberg)
        # self.model = LGBMClassifier()
        # self.model.load_model('iceberg_model.txt') # 載入預訓練模型

[CODE]
        print("🧠 [Iceberg Predictor]: 預訓練模型載入完成。")

[CONCEPT]
        # 實時緩衝區用於計算速率 (rate) 特徵

[NOTE]
        self.tick_buffer = []

[CODE]
    def _extract_micro_features(self, orderbook_updates: list) -> pd.DataFrame:

[NOTE]
        """

[CONCEPT]
        從 Tick-by-Tick 的訂單簿更新中提取預測冰山單所需的微觀特徵。

[NOTE]
        """
        # 1. 計算累積成交與報價比 (Accumulation Ratio)
        # 假設 orderbook_updates 包含 'executed_volume' 和 'quoted_volume'

[NOTE]
        # 2. 計算最近 50ms 內的訂單取消率
        cancellation_rate = np.random.uniform(0.01, 0.5)

[NOTE]
        # 3. 計算價格波動後的平均回調幅度 (Recoil)
        recoil_magnitude = np.random.uniform(-0.0001, 0.0005)

[NOTE]
        # 4. 實時 OBI 和 Slope (來自 FX_Orderbook 因子)
        current_obi = np.random.uniform(-0.5, 0.5)
        current_slope_ask = np.random.uniform(0.001, 0.01)

[CONCEPT]
        # 構建用於模型推斷的特徵向量

[NOTE]
        features = pd.DataFrame({
            'Cancellation_Rate': [cancellation_rate],
            'Recoil_Magnitude': [recoil_magnitude],
            'OBI': [current_obi],
            'Slope_Ask': [current_slope_ask]
        })
        return features

[CODE]
    def predict_iceberg_location(self, orderbook_updates: list) -> dict:

[NOTE]
        """
        輸出潛在冰山單的概率和意圖。
        """
        features = self._extract_micro_features(orderbook_updates)

[NOTE]
        # 實際推斷：
        # prediction_proba = self.model.predict_proba(features)[0]

[NOTE]
        # 模擬模型輸出：Buy Side 冰山單的概率高
        buy_iceberg_proba = 0.75
        sell_iceberg_proba = 0.20

[NOTE]
        confidence_threshold = 0.70

[NOTE]
        if buy_iceberg_proba > confidence_threshold:

[CODE]
            print(f"👁️ [Iceberg Alert]: 偵測到強大**隱藏買盤**意圖！信心度: {buy_iceberg_proba:.2f}")

[NOTE]
            return {'Side': 'BUY', 'Confidence': buy_iceberg_proba}
        elif sell_iceberg_proba > confidence_threshold:

[CODE]
            print(f"👁️ [Iceberg Alert]: 偵測到強大**隱藏賣盤**意圖！信心度: {sell_iceberg_proba:.2f}")

[NOTE]
            return {'Side': 'SELL', 'Confidence': sell_iceberg_proba}
        else:
            return {'Side': 'NONE', 'Confidence': 0.0}

[STRUCTURE]
3. 終極整合：執行策略的調整

[NOTE]
$\text{IcebergPredictor}$ 的輸出將直接指導 OrderRouter 的行為：
1. 偵測到隱藏買盤意圖 ($\text{Side}$: $\text{BUY}$):

[RULE]
   * 如果 $\text{RL}$ 當前是買入信號：OrderRouter 可以更積極地發送限價單 (Limit Order)，並將其定位在略高於當前報價，利用隱藏買盤提供的支撐來減少滑價。
   * 如果 $\text{RL}$ 當前是賣出信號：OrderRouter 應轉為極度被動，避免將訂單砸向這個強大的隱藏買盤，從而防止執行成本飆升。

[STRUCTURE]
這項補強，使我們的系統擁有了**「預見」市場真實流動性結構的能力，使我們的執行模組從被動響應升級為主動嗅探和利用**。

[NOTE]
我們已經完成了流動性現實的超越。接下來，我們將解決**「時間與知識的非線性」問題，即信息時間度量 ($\text{F}_{\text{InfoTime}}$)**。請問是否繼續？

[STRUCTURE]
⭐ 階段 XVI：隱藏流動性預測模型（F_Iceberg）

[NOTE]
0. 任務定位

[STRUCTURE]
在 XV 階段的 $F_{\text{Orderbook}}$，我們已經讓系統「看到顯性的深度」：

[NOTE]
* 五檔 / 十檔的掛單量
* 斜率、失衡、Z-Score

[STRUCTURE]
F_Iceberg 的目標，是讓系統開始「嗅到隱性的深度」：

[NOTE]
* 冰山單（Iceberg Orders）
* 被拆小、反覆出現的隱形大單
* 看不到但一直「吃不完」的流動性牆
簡單講：

[FORMULA]
$F_{\text{Orderbook}}$ = 看得到的牆
$F_{\text{Iceberg}}$ = 聞得到但看不到的牆

[STRUCTURE]
此模組將實作於：

[NOTE]
strategy_engine/factor_FX_iceberg.py

[NOTE]
________________

[NOTE]
1. F_Iceberg 的輸出設計

[CONCEPT]
我們先定義這個因子最後輸出的格式（RL / Execution 都要讀）：

[NOTE]
* $\text{IcebergProb}_{\text{Bid}} \in [0, 1]$
   * 買盤存在冰山單的機率
* $\text{IcebergProb}_{\text{Ask}} \in [0, 1]$
   * 賣盤存在冰山單的機率
* $\text{HiddenDepth}_{\text{Bid}}$（估計隱藏買單量）
* $\text{HiddenDepth}_{\text{Ask}}$（估計隱藏賣單量）
* $\text{WallStability}_{\text{Bid}} \in [-1, 1]$
* $\text{WallStability}_{\text{Ask}} \in [-1, 1]$
這些輸出會進到：
RL State Vector

[STRUCTURE]
Execution Engine（下單決策）

[NOTE]
Smart OrderRouter（要不要跟牆、躲牆、打牆）

[NOTE]
________________

[CONCEPT]
2. 冰山單偵測的核心訊號
我們不可能「直接看到」冰山單，只能透過「行為特徵」推估。

[NOTE]
2.1 訊號 1：同一價位反覆補量（Refill Pattern）

[RULE]
條件概念：

[NOTE]
* 某一檔價格 $P_0$

[RULE]
* 在短時間內，多次出現「成交吃掉一批 → 掛單量又回到接近原始數量」

[CONCEPT]
定義：

[NOTE]
* $V_t(P)$：時間 $t$ 時，價位 $P$ 的掛單量
* $T_i$：第 $i$ 次被吃掉的時間點
* $\Delta V_{\text{trade}, i}$：該次交易吃掉的量

[RULE]
若滿足：

[NOTE]
* 多次交易後 $V_{T_i^+}(P_0)$ 又回到接近初始 $V_{0}(P_0)$
* 且此行為重複 $k$ 次以上

[RULE]
則提高 $\text{IcebergProb}$。

[NOTE]
2.2 訊號 2：成交量 vs 報價量比值（Volume vs Quote）
觀察：
* 某價位顯示 500 張
* 短時間內在該價位成交 3000 張
* 但掛單始終「看起來」只剩下 300–500 張

[RULE]
則說明：「實際被吃掉的量」遠超過「看得見的掛單量」。

[NOTE]
設計一個 ratio：
R_iceberg(P)=CumulativeTradedVolumeAtPrice(P,Δt)VisibleDepthAtPrice(P)R_iceberg(P)=VisibleDepthAtPrice(P)CumulativeTradedVolumeAtPrice(P,Δt)​

[RULE]
當 $R_{\text{iceberg}}$ 明顯 > 1 且持續一段時間 → 強烈冰山訊號。

[NOTE]
2.3 訊號 3：牆的穩定度（Wall Stability）
牆的特性：
* 價格多次接近該價位，卻「打不穿」
* 牆的可見數量會震盪，但價位本身一直守住

[CONCEPT]
定義一個穩定度指標：

[NOTE]
* 多次測試該價位（價格觸及 / 接近）
* 每次測試後，價位仍未被突破
* 且成交總量不斷累積
此時 $\text{WallStability}$ 越高（接近 1）

→ 表示那裏有「看不見但很堅固的牆」。
[NOTE]
________________

[CONCEPT]
3. Iceberg 因子計算框架（特徵與機率）

[RULE]
我們用「特徵分數 → 機率」的架構，不直接硬寫 if-else。

[CONCEPT]
3.1 特徵集合

[NOTE]
對買盤（Bid）示意：
* $f_1$：同價位補單次數（refill_count）
* $f_2$：成交量 / 可見深度比值 $R_{\text{iceberg}}$
* $f_3$：牆被測試次數（wall_tests）
* $f_4$：牆被守住的比例（wall_hold_ratio）
對賣盤（Ask）同理。
3.2 Iceberg 機率（簡化版）
先用一個簡單、可日後替換成 ML 的形式：
IcebergScore=w_1⋅f_1^+w_2⋅f_2^+w_3⋅f_3^+w_4⋅f_4^IcebergScore=w_1⋅f_1^​+w_2⋅f_2^​+w_3⋅f_3^​+w_4⋅f_4^​

[CONCEPT]
其中 $\hat{f_i}$ 為標準化後的特徵。

[NOTE]
再用 sigmoid 映射成機率：
IcebergProb=σ(IcebergScore)IcebergProb=σ(IcebergScore)
________________

[CODE]
4. Python 模組骨架：factor_FX_iceberg.py

[NOTE]
# strategy_engine/factor_FX_iceberg.py

[CODE]
import numpy as np
from collections import deque
from typing import Dict, List

[CODE]
class IcebergFactorEngine:

[NOTE]
    """
    F_Iceberg:
    隱藏流動性預測模型，用於估計冰山單與隱形掛單牆。
    """

[CODE]
    def __init__(self, window_seconds: int = 60):

[NOTE]
        # 用於記錄最近一段時間的 orderbook + trades
        self.window_seconds = window_seconds
        self.price_trades_history = deque()   # [(timestamp, price, volume, side), ...]
        self.book_snapshot_history = deque()  # [(timestamp, orderbook_snapshot), ...]

[CONCEPT]
        # TODO: 之後可以加入 ML 模型或更精細的特徵

[NOTE]
        self.weights_bid = np.array([0.3, 0.3, 0.2, 0.2])
        self.weights_ask = np.array([0.3, 0.3, 0.2, 0.2])

[CODE]
    def update_trades(self, trades: List[Dict]):

[NOTE]
        """
        更新成交紀錄。
        trades: list of dict
        每筆格式：
          {
            "timestamp": ...,

[CODE]
            "price": float,
            "volume": float,

[NOTE]
            "side": "buy" or "sell"
          }
        """
        # TODO: append 並清掉過舊資料（超過 window）
        ...

[CODE]
    def update_orderbook(self, timestamp, orderbook_snapshot: Dict):

[NOTE]
        """
        更新訂單簿快照。
        orderbook_snapshot:
          {
            "bid_prices": [...],
            "bid_volumes": [...],
            "ask_prices": [...],
            "ask_volumes": [...]
          }
        """
        # TODO: append 並清掉過舊資料
        ...

[CODE]
    def _compute_features_for_side(self, side: str) -> Dict[str, float]:

[NOTE]
        """

[CONCEPT]
        根據歷史 trades + orderbook，計算指定方向的特徵:

[NOTE]
        - refill_count
        - volume_vs_depth_ratio
        - wall_tests
        - wall_hold_ratio
        """

[CONCEPT]
        # TODO: 真正特徵計算邏輯

[NOTE]
        refill_count = 0.0
        volume_vs_depth_ratio = 0.0
        wall_tests = 0.0
        wall_hold_ratio = 0.0

[NOTE]
        return {
            "refill_count": refill_count,
            "volume_vs_depth_ratio": volume_vs_depth_ratio,
            "wall_tests": wall_tests,
            "wall_hold_ratio": wall_hold_ratio,
        }

[CODE]
    def _score_to_prob(self, features: Dict[str, float], weights: np.ndarray) -> float:

[NOTE]
        """

[CONCEPT]
        將特徵加權成一個 0~1 的機率值。

[NOTE]
        """
        f_vec = np.array([
            features["refill_count"],
            features["volume_vs_depth_ratio"],
            features["wall_tests"],
            features["wall_hold_ratio"],
        ])

[CONCEPT]
        # TODO: 正規化特徵，避免數值爆炸

[NOTE]
        score = np.dot(weights, f_vec)
        prob = 1.0 / (1.0 + np.exp(-score))
        return float(prob)

[CODE]
    def compute_iceberg_factors(self) -> Dict[str, float]:

[NOTE]
        """

[RULE]
        對當前視窗內的資料，估計冰山單相關因子。

[NOTE]
        回傳：
          {
            "IcebergProb_Bid": ...,
            "IcebergProb_Ask": ...,
            "HiddenDepth_Bid": ...,
            "HiddenDepth_Ask": ...,
            "WallStability_Bid": ...,
            "WallStability_Ask": ...
          }
        """
        bid_features = self._compute_features_for_side("bid")
        ask_features = self._compute_features_for_side("ask")

[NOTE]
        iceberg_prob_bid = self._score_to_prob(bid_features, self.weights_bid)
        iceberg_prob_ask = self._score_to_prob(ask_features, self.weights_ask)

[NOTE]
        # TODO: HiddenDepth 可以用: 成交量超出可見深度的部分作估計
        hidden_depth_bid = 0.0
        hidden_depth_ask = 0.0

[NOTE]
        # TODO: WallStability 根據 wall_tests / hold_ratio 等算一個 -1~1 指標
        wall_stability_bid = 0.0
        wall_stability_ask = 0.0

[NOTE]
        return {
            "IcebergProb_Bid": iceberg_prob_bid,
            "IcebergProb_Ask": iceberg_prob_ask,
            "HiddenDepth_Bid": hidden_depth_bid,
            "HiddenDepth_Ask": hidden_depth_ask,
            "WallStability_Bid": wall_stability_bid,
            "WallStability_Ask": wall_stability_ask,
        }

[NOTE]
________________

[RULE]
5. RL / Execution 整合規則（行為層）
讓這個因子真正有用，而不是只當指標看。

[NOTE]
5.1 RL State 整合
在高頻 RL Agent 的 State 中，加入：
IcebergProb_Bid
IcebergProb_Ask
HiddenDepth_Bid
HiddenDepth_Ask
WallStability_Bid
WallStability_Ask

[NOTE]
5.2 行為約束（Action Constraints）示例

[RULE]
* 若 IcebergProb_Bid > 0.8 且 WallStability_Bid > 0.5

→ 不要在該價位上方追價買入（因為下面有大買牆，可以等回落或跟牆買）
[RULE]
* 若 IcebergProb_Ask > 0.8 且 WallStability_Ask > 0.5

→ 避免在空頭時「硬頂著賣在牆下」，可以用被動掛單貼牆
[RULE]
* 若 IcebergProb_Bid 高 & 價格在牆上方震盪

→ RL 可以視為「下檔有隱形支撐」，對多頭策略提高信心
[RULE]
* 若 IcebergProb_Ask 高 & 價格靠近賣牆

→ RL 應降低追價買進行為，偏向賣出 / 減碼
[STRUCTURE]
「直接貼給 Cursor 用」的階段 XVI：F_Iceberg 實作規格書，

[NOTE]
會要求它：
   * 建立 strategy_engine/factor_FX_iceberg.py
   * 補上 TODO 可以逐步實作
   * 幫你加一個簡單的測試檔 tests/strategy_engine/test_factor_FX_iceberg.py
你直接複製下面整段給 Cursor 就好。
________________

[CODE]
你現在是一位 量化系統架構師 + Python 資深工程師。

[STRUCTURE]
我要你幫我在專案裡實作「階段 XVI：隱藏流動性預測模型（F_Iceberg）」。
請依照以下規格，建立新模組與測試，並盡量保持結構清楚、易於未來擴充（例如換成 ML 模型）：

[NOTE]
________________

[CONCEPT]
一、目標說明（放在你的腦中，不用寫進程式）

[NOTE]
F_Iceberg 的任務是：
在高頻環境中，根據「成交紀錄 + 訂單簿變化」，估計：
   * 冰山單（Iceberg Orders）出現的機率
   * 隱藏深度（Hidden Depth）
   * 價格牆的穩定度（Wall Stability）
並輸出一組因子，提供給：
   * RL 代理人（State Vector 其中一部分）

[STRUCTURE]
   * Execution Engine / Order Router（調整下單策略）

[NOTE]
________________

[NOTE]
二、建立檔案：factor_FX_iceberg.py
請在專案中建立檔案：
strategy_engine/factor_FX_iceberg.py

[STRUCTURE]
內容請實作一個 IcebergFactorEngine 類別，結構如下：

[CODE]
import numpy as np
from collections import deque
from typing import Dict, List, Any
from datetime import datetime

[CODE]
class IcebergFactorEngine:

[NOTE]
    """
    F_Iceberg: 隱藏流動性預測模型

[NOTE]
    功能：
    - 根據最近一段時間內的成交紀錄與訂單簿變化
      推估冰山單、隱藏深度與價格牆穩定度。

[NOTE]
    使用方式（示意）：

[STRUCTURE]
        engine = IcebergFactorEngine(window_seconds=60)

[NOTE]
        engine.update_trades(trades)
        engine.update_orderbook(timestamp, orderbook_snapshot)
        factors = engine.compute_iceberg_factors()
    """

[CODE]
    def __init__(self, window_seconds: int = 60):

[NOTE]
        """
        :param window_seconds: 用於分析的時間視窗長度（秒）
        """
        self.window_seconds = window_seconds

[NOTE]
        # 最近一段時間的成交紀錄：

[CODE]
        # list of dict: {"timestamp": datetime, "price": float, "volume": float, "side": "buy"/"sell"}
        self.price_trades_history: deque[Dict[str, Any]] = deque()

[NOTE]
        # 最近一段時間的訂單簿快照：
        # list of (timestamp, orderbook_snapshot)
        # orderbook_snapshot = {
        #   "bid_prices": [...],
        #   "bid_volumes": [...],
        #   "ask_prices": [...],
        #   "ask_volumes": [...],
        # }

[CODE]
        self.book_snapshot_history: deque[Dict[str, Any]] = deque()

[NOTE]
        # 目前使用簡單線性加權，未來可替換為 ML 模型
        self.weights_bid = np.array([0.3, 0.3, 0.2, 0.2])
        self.weights_ask = np.array([0.3, 0.3, 0.2, 0.2])

[CODE]
    def _prune_old_data(self, now: datetime) -> None:

[NOTE]
        """
        移除超過 window_seconds 的舊資料。

[STRUCTURE]
        TODO: 之後可以優化為 O(1) 的清理策略。

[NOTE]
        """
        cutoff = now.timestamp() - self.window_seconds

[NOTE]
        while self.price_trades_history and self.price_trades_history[0]["timestamp"].timestamp() < cutoff:
            self.price_trades_history.popleft()

[NOTE]
        while self.book_snapshot_history and self.book_snapshot_history[0]["timestamp"].timestamp() < cutoff:
            self.book_snapshot_history.popleft()

[CODE]
    def update_trades(self, trades: List[Dict[str, Any]]) -> None:

[NOTE]
        """
        更新成交紀錄。（可多筆一次更新）

[NOTE]
        trades 每筆格式：
        {
            "timestamp": datetime,

[CODE]
            "price": float,
            "volume": float,

[NOTE]
            "side": "buy" or "sell"
        }
        """
        for t in trades:
            self.price_trades_history.append(t)

[NOTE]
        if trades:
            # 以最新成交時間為基準清理舊資料
            self._prune_old_data(trades[-1]["timestamp"])

[CODE]
    def update_orderbook(self, timestamp: datetime, orderbook_snapshot: Dict[str, List[float]]) -> None:

[NOTE]
        """
        更新訂單簿快照。
        orderbook_snapshot:
        {
            "bid_prices": [...],
            "bid_volumes": [...],
            "ask_prices": [...],
            "ask_volumes": [...]
        }
        """
        self.book_snapshot_history.append({
            "timestamp": timestamp,
            "orderbook": orderbook_snapshot,
        })
        self._prune_old_data(timestamp)

[CODE]
    def _compute_features_for_side(self, side: str) -> Dict[str, float]:

[NOTE]
        """

[CONCEPT]
        根據最近一段時間的資料，計算單一方向（bid 或 ask）的特徵：

[NOTE]
        - refill_count: 某價位附近反覆補單的次數
        - volume_vs_depth_ratio: 成交量 / 可見深度 比值
        - wall_tests: 價格對牆價位的「測試」次數
        - wall_hold_ratio: 牆被成功守住的比例

[NOTE]
        TODO:

[STRUCTURE]
        - 目前先回傳 0.0，作為架構骨架。

[CONCEPT]
        - 之後再細化計算邏輯（以價位 bucket、交易軌跡等）。

[NOTE]
        """
        refill_count = 0.0
        volume_vs_depth_ratio = 0.0
        wall_tests = 0.0
        wall_hold_ratio = 0.0

[CONCEPT]
        # TODO: 實作真實特徵計算邏輯

[NOTE]
        return {
            "refill_count": refill_count,
            "volume_vs_depth_ratio": volume_vs_depth_ratio,
            "wall_tests": wall_tests,
            "wall_hold_ratio": wall_hold_ratio,
        }

[CODE]
    def _score_to_prob(self, features: Dict[str, float], weights: np.ndarray) -> float:

[NOTE]
        """

[CONCEPT]
        將特徵值轉成 0~1 的冰山機率估計。

[NOTE]
        TODO:
        - 目前未做標準化，未來可以加入 feature scaling。
        """
        f_vec = np.array([
            features.get("refill_count", 0.0),
            features.get("volume_vs_depth_ratio", 0.0),
            features.get("wall_tests", 0.0),
            features.get("wall_hold_ratio", 0.0),
        ])

[NOTE]
        score = float(np.dot(weights, f_vec))
        prob = 1.0 / (1.0 + np.exp(-score))  # Sigmoid
        return prob

[CODE]
    def compute_iceberg_factors(self) -> Dict[str, float]:

[NOTE]
        """
        估計冰山單與隱藏流動性相關因子。

[NOTE]
        回傳格式：
        {

[CODE]
            "IcebergProb_Bid": float,    # 0~1
            "IcebergProb_Ask": float,    # 0~1
            "HiddenDepth_Bid": float,    # 估計的隱藏買單量
            "HiddenDepth_Ask": float,    # 估計的隱藏賣單量
            "WallStability_Bid": float,  # -1 ~ 1
            "WallStability_Ask": float,  # -1 ~ 1

[NOTE]
        }

[NOTE]
        TODO:
        - HiddenDepth_* 可先用「成交量超出可見深度」的部分粗略估計。
        - WallStability_* 可先用 wall_tests / hold_ratio 做簡單映射。
        """

[CONCEPT]
        # 計算 Bid / Ask 特徵

[NOTE]
        bid_features = self._compute_features_for_side("bid")
        ask_features = self._compute_features_for_side("ask")

[NOTE]
        # 冰山機率估計
        iceberg_prob_bid = self._score_to_prob(bid_features, self.weights_bid)
        iceberg_prob_ask = self._score_to_prob(ask_features, self.weights_ask)

[CONCEPT]
        # TODO: 目前先回傳 0.0，後續逐步實作估算邏輯

[NOTE]
        hidden_depth_bid = 0.0
        hidden_depth_ask = 0.0

[NOTE]
        wall_stability_bid = 0.0
        wall_stability_ask = 0.0

[NOTE]
        return {
            "IcebergProb_Bid": iceberg_prob_bid,
            "IcebergProb_Ask": iceberg_prob_ask,
            "HiddenDepth_Bid": hidden_depth_bid,
            "HiddenDepth_Ask": hidden_depth_ask,
            "WallStability_Bid": wall_stability_bid,
            "WallStability_Ask": wall_stability_ask,
        }

[NOTE]
要求：
   * 程式碼需有清楚 docstring

[CONCEPT]
   * 核心邏輯可先以 TODO 形式標註，之後再逐步實作

[NOTE]
   * 不要引入除標準庫 + numpy 以外的新依賴
________________

[NOTE]
三、建立測試檔：test_factor_FX_iceberg.py
請在 tests/strategy_engine/ 底下，新增：
tests/strategy_engine/test_factor_FX_iceberg.py

[CONCEPT]
測試目標：

[STRUCTURE]
   1. 可以成功建立 IcebergFactorEngine 實例

[NOTE]
   2. 呼叫 update_trades / update_orderbook 不會出錯
   3. 呼叫 compute_iceberg_factors() 能回傳包含指定 key 的 dict
   4. 在「沒有任何資料」與「有少量測試資料」的情況下都不會崩潰
範例結構如下：

[CODE]
from datetime import datetime, timedelta

[CODE]
from strategy_engine.factor_FX_iceberg import IcebergFactorEngine

[CODE]
def test_iceberg_engine_basic_flow():

[STRUCTURE]
    engine = IcebergFactorEngine(window_seconds=60)

[NOTE]
    now = datetime.utcnow()

[NOTE]
    # 模擬幾筆成交
    trades = [
        {
            "timestamp": now,
            "price": 100.0,
            "volume": 50.0,
            "side": "buy",
        },
        {
            "timestamp": now + timedelta(seconds=1),
            "price": 100.5,
            "volume": 30.0,
            "side": "sell",
        },
    ]
    engine.update_trades(trades)

[NOTE]
    # 模擬一筆 orderbook 快照
    orderbook_snapshot = {
        "bid_prices": [99.5, 99.0, 98.5],
        "bid_volumes": [100, 80, 60],
        "ask_prices": [100.5, 101.0, 101.5],
        "ask_volumes": [90, 70, 50],
    }
    engine.update_orderbook(now, orderbook_snapshot)

[NOTE]
    factors = engine.compute_iceberg_factors()

[NOTE]
    # 檢查回傳結構
    expected_keys = [
        "IcebergProb_Bid",
        "IcebergProb_Ask",
        "HiddenDepth_Bid",
        "HiddenDepth_Ask",
        "WallStability_Bid",
        "WallStability_Ask",
    ]
    for key in expected_keys:
        assert key in factors

[CONCEPT]
    # 檢查機率範圍合理（目前邏輯雖然簡化，但仍應落在 0~1）

[NOTE]
    assert 0.0 <= factors["IcebergProb_Bid"] <= 1.0
    assert 0.0 <= factors["IcebergProb_Ask"] <= 1.0

[NOTE]
________________

[NOTE]
四、實作要求總結
   1. 新增檔案
   * strategy_engine/factor_FX_iceberg.py
   * tests/strategy_engine/test_factor_FX_iceberg.py
   2. 保持程式風格與專案中其他 factor / engine 類似。

[STRUCTURE]
   3. 目前階段重點是：
   * 架構完整

[NOTE]
   * 介面穩定
   * 測試可跑過
   * 真正細節用 TODO 標註，未來再優化
   4. 完成後請列出：
   * 新增 / 修改的檔案清單
   * 每個檔案的主要用途簡述

[STRUCTURE]
請開始依照以上規格實作階段 XVI：F_Iceberg 模組與測試。
階段 XVII：信息時間度量因子 F_InfoTime（Information Time Factor）

[CODE]
你現在是一位 量化系統架構師 + Python 資深工程師。

[STRUCTURE]
我要你在專案新增一個全新的模組：

[NOTE]
strategy_engine/factor_FX_infotime.py
並同步新增測試檔：
tests/strategy_engine/test_factor_FX_infotime.py
請依照以下規格完整實作。
________________

[STRUCTURE]
🧠 一、模組目的（你理解即可，不用寫入程式碼）

[CONCEPT]
F_InfoTime 的核心目標：

[NOTE]
打破「時鐘時間」的限制，改用「資訊密度」作為 AI 的決策基準。
具體而言：
   * 在大量成交（大量資訊）時，時間流逝要變快
   * 在沉寂時段（低資訊量）時，時間流逝要變慢
   * 讓 RL 代理人直接感知：
市場現在是「資訊爆炸」還是「死水一潭」
對應指標：非時間基數 K-Bars（Volume Bars）
________________

[NOTE]
🛠️ 二、請建立：factor_FX_infotime.py
新增檔案：
strategy_engine/factor_FX_infotime.py
並加入以下完整程式骨架（可直接複製）。

[CONCEPT]
所有核心邏輯已按我們的工程規格建好，可保留 TODO 以後再強化。

[NOTE]
________________

[CODE]
import time
import numpy as np
from collections import deque
from typing import Optional

[CODE]
class InfoTimeEngine:

[NOTE]
    """
    F_InfoTime: 信息時間度量因子
    -----------------------------------------

[CONCEPT]
    目標：

[NOTE]
        - 以 Volume Bars（非時間基數 K-Bar）取代傳統時間 K-Bar。
        - 計算「資訊密度」（Information Density）。

[STRUCTURE]
        - 輸出 F_InfoTime，供 RL 代理人與 Execution Engine 使用。

[RULE]
    設計原則：
        - 當成交量劇增 → 時間流得快 → F_InfoTime > 1
        - 當成交沉寂 → 時間流得慢 → F_InfoTime < 1

[NOTE]
    """

[NOTE]
    # 每個 Volume Bar 所需累積交易量（50萬 / 500萬 依市場情況調整）

[FORMULA]
    K_VOLUME_BAR_SIZE = 5_000_000

[NOTE]
    # 用最近 N 個 Bar 估算短期平均
    N_BARS_FOR_AVERAGE = 20

[CODE]
    def __init__(self, long_term_avg_freq: float = 300.0):

[NOTE]
        """
        :param long_term_avg_freq: 長期平均 Bar 形成頻率（秒），例如 300 秒一個 Bar。
        """
        self.long_term_avg_freq = float(long_term_avg_freq)

[RULE]
        # 當前 Bar 已累積的 Volume

[NOTE]
        self.current_volume = 0

[NOTE]
        # 上一次形成 Bar 的時間戳
        self.last_bar_timestamp = time.time()

[NOTE]
        # 最近 N 個 Bar 間隔（秒）
        self.bar_interval_history = deque(maxlen=self.N_BARS_FOR_AVERAGE)

[NOTE]
    # -----------------------------------------------------
    # 更新 Tick
    # -----------------------------------------------------

[CODE]
    def process_tick(self, tick_volume: int, tick_timestamp: float) -> Optional[float]:

[NOTE]
        """
        處理單筆 Tick。

[RULE]
        每次累積 tick_volume，一旦超過 K_VOLUME_BAR_SIZE → 形成一個 Volume Bar。

[RULE]
        :return: 若形成 Bar → 回傳最新 F_InfoTime；否則回傳 None

[NOTE]
        """
        self.current_volume += tick_volume

[FORMULA]
        if self.current_volume >= self.K_VOLUME_BAR_SIZE:

[NOTE]
            # 形成 Volume Bar
            interval = tick_timestamp - self.last_bar_timestamp
            self.bar_interval_history.append(interval)

[CONCEPT]
            # 重置（剩餘 volume 可加入更精細邏輯，先簡化為清零）

[NOTE]
            self.current_volume = 0
            self.last_bar_timestamp = tick_timestamp

[NOTE]
            # 計算 F_InfoTime
            return self.calculate_infotime_factor()

[NOTE]
        return None

[NOTE]
    # -----------------------------------------------------
    # 計算信息時間因子
    # -----------------------------------------------------

[CODE]
    def calculate_infotime_factor(self) -> float:

[NOTE]
        """
        :return: F_InfoTime （資訊密度指標）
        """

[RULE]
        # 若資料不足 → 回傳常態值 1.0

[NOTE]
        if len(self.bar_interval_history) < self.N_BARS_FOR_AVERAGE:
            return 1.0

[NOTE]
        recent_avg_interval = float(np.mean(self.bar_interval_history))

[NOTE]
        # 資訊密度 = 長期平均間隔 / 近期平均間隔

[RULE]
        # Bar 形成越快 → recent_avg_interval 越小 → F_InfoTime 越大

[NOTE]
        if recent_avg_interval == 0:
            return 2.0  # 避免除 0（極端情況，可視為訊息爆炸）

[FORMULA]
        F_InfoTime = self.long_term_avg_freq / recent_avg_interval

[NOTE]
        return float(F_InfoTime)

[NOTE]
________________

[NOTE]
🧪 三、請建立測試檔
新增：
tests/strategy_engine/test_factor_FX_infotime.py
測試內容如下：
________________

[CODE]
import time
from strategy_engine.factor_FX_infotime import InfoTimeEngine

[CODE]
def test_infotime_basic_flow():

[STRUCTURE]
    engine = InfoTimeEngine(long_term_avg_freq=300.0)

[NOTE]
    now = time.time()

[NOTE]
    # 模擬 20 個 Bar（最低需求）
    for i in range(20):
        # tick_volume 大到足以一次形成 Bar
        F = engine.process_tick(

[FORMULA]
            tick_volume=InfoTimeEngine.K_VOLUME_BAR_SIZE,

[NOTE]
            tick_timestamp=now + (i + 1)
        )

[NOTE]
    # 此時 F_InfoTime 不應為 None
    assert F is not None

[RULE]
    # 值必須為 float

[NOTE]
    assert isinstance(F, float)

[RULE]
    # 相當於近期 avg_interval ≈ 1 秒 → F_InfoTime ≈ 300
    assert F > 10.0  # 必須顯著大於常態值 1.0

[CODE]
def test_infotime_insufficient_data():

[STRUCTURE]
    engine = InfoTimeEngine()

[NOTE]
    now = time.time()

[NOTE]
    # 只形成 5 個 Bar（未達 N=20）
    for i in range(5):
        F = engine.process_tick(

[FORMULA]
            tick_volume=InfoTimeEngine.K_VOLUME_BAR_SIZE,

[NOTE]
            tick_timestamp=now + (i + 1)
        )

[RULE]
    # 前 20 次都應該回傳 1.0 或 None

[NOTE]
    assert F == 1.0 or F is None

[NOTE]
________________

[NOTE]
🧩 四、整合要求（Cursor 請務必遵守）

[STRUCTURE]
在完成以上模組後，請：

[NOTE]
      1. 列出新增 / 修改的檔案清單
      2. 每個檔案用途簡述
      3. 確保 pytest 可以通過

[STRUCTURE]
      4. 程式碼風格與專案既有 factor 模組一致

[NOTE]
      5. 所有變數命名與結構須保持可擴充性（未來要接入新聞流 / 事件流）

[RULE]
這是一個極具前瞻性的最終升級！將強化學習 (RL) 網絡升級為 $\text{Transformer}$ 架構，是實現您**「大自然萬物修復法則」中「知識的即時權重分配」**的最終途徑。

[CONCEPT]
傳統的 $\text{LSTM}$ 或 $\text{GRU}$ 模型會順序地處理歷史數據，導致近期的重要信息和遠期的無關噪音被平均化對待。而 $\text{Transformer}$ 的核心 注意力機制 ($\text{Attention}$ $\text{Mechanism}$)，讓 $\text{AI}$ 具備了**「選擇性記憶」**的能力。

[NOTE]
________________

[STRUCTURE]
🛠️ 階段 XVIII：終極架構升級 ($\text{Transformer}$ $\text{RL}$$\text{Agent}$)

[CONCEPT]
1. 核心概念：自注意力與非線性時間

[NOTE]
$\text{Transformer}$ 模型將允許 $\text{RL}$ 代理人：

[RULE]
      * 即時權重分配： 根據當前環境（由 $\mathbf{F}_{\text{InfoTime}}$ 決定），動態地決定歷史 $\text{Alpha}$中哪一段數據對當前的決策最重要。

[STRUCTURE]
      * 消除冗餘： 在 $\mathbf{F}_{\text{InfoTime}}$ 密度極低時，系統會自動將注意力權重放在最近的幾個 $\text{Volume}$ $\text{Bar}$，實質上實現了對噪音的**「即時忘卻」**。
2. 架構設計：$\text{Transformer}$ $\text{Encoder}$ 作為 $\text{RL}$ 的「感知器」

[NOTE]
我們將使用 $\text{Transformer}$ 的 $\text{Encoder}$ 部分來處理 $\text{RL}$ 的 $\text{State}$ $\text{Vector}$序列。
組件
	功能
	處理數據
	輸入序列
	$\text{RL}$ 的歷史 $\text{State}$$\text{Vector}$ (例如過去 $\text{10}$ 個 $\text{Volume}$ $\text{Bar}$)
	$\mathbf{O}_{\text{Factors}}, \text{F}_{\text{Internal}}, \mathbf{F}_{\text{Orderbook}}, \mathbf{F}_{\text{InfoTime}}, \dots$
	位置編碼 ($\text{Positional}$$\text{Encoding}$)

[CONCEPT]
	由於 $\text{Transformer}$ 本身沒有時間概念，我們用它來標記每個 $\text{Volume}$$\text{Bar}$ 在序列中的相對位置。

[NOTE]
	-
	自注意力 ($\text{Self}$-$\text{Attention}$)

[CONCEPT]
	核心機制。 讓 $\text{AI}$ 評估序列中所有元素之間的相關性，並計算權重。

[RULE]
	輸出 $\text{Attention}$ $\text{Weights}$，指示當前時刻應「關注」歷史序列的哪個部分。

[NOTE]
	前饋網絡 ($\text{FFN}$)

[CONCEPT]
	處理注意力輸出的加權信息，生成最終的 $\text{Action}$（$\text{Net}$$\text{Exposure}$ 和 $\text{Beta}$$\text{Allocation}$）。

[NOTE]
	$\mathbf{Action}$
	3. 關鍵機制：$\text{Attention}$ 與 $\text{F}_{\text{InfoTime}}$ 的耦合
在 $\text{Attention}$ 權重計算中， $\mathbf{F}_{\text{InfoTime}}$ 扮演了環境上下文的角色。

[RULE]
當 $\mathbf{F}_{\text{InfoTime}}$ 很高時：
      * $\text{Transformer}$ 會將極高的注意力權重分配給序列中最接近當前時間點的 $\text{Volume}$$\text{Bar}$。

[CONCEPT]
      * 這使得 $\text{RL}$ 能夠幾乎即時地拋棄舊的、可能已被新資訊推翻的 $\text{Alpha}$ 信號。

[RULE]
當 $\mathbf{F}_{\text{InfoTime}}$ 恢復常態時：

[NOTE]
      * 注意力權重將更均勻地分配給整個歷史序列，確保決策的穩定性和深度。

[CODE]
4. 概念性 $\text{Python}$ 程式碼範例
Python

[NOTE]
# rl_calibration/transformer_agent.py

[CODE]
import torch
from torch import nn

[NOTE]
# 假設我們使用的是 PyTorch 框架

[CODE]
class TransformerRLAgent(nn.Module):

[CODE]
    def __init__(self, feature_dim, history_length, n_heads=4):

[NOTE]
        super().__init__()

[NOTE]
        # 1. 將 State Vector 映射到 Transformer 的隱藏維度
        self.input_projection = nn.Linear(feature_dim, feature_dim)

[NOTE]
        # 2. Transformer Encoder 層
        # history_length 是我們選擇的 Volume Bar 數量 (例如 10)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=feature_dim,
            nhead=n_heads,
            dim_feedforward=feature_dim * 4
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=2 # 堆疊兩層 Encoder 增加深度
        )

[CONCEPT]
        # 3. 輸出層：輸出 RL 的 Action (Beta Weights + Net Exposure)

[NOTE]
        self.output_layer = nn.Linear(feature_dim * history_length, output_action_dim)

[CODE]
    def forward(self, state_sequence: torch.Tensor):

[NOTE]
        """
        state_sequence shape: (batch_size, history_length, feature_dim)
        其中 feature_dim 包含 O-Factors, F_Internal, F_Orderbook, F_InfoTime 等。
        """

[NOTE]
        # 1. 處理輸入 (加入位置編碼等步驟被省略)
        x = self.input_projection(state_sequence)

[CONCEPT]
        # 2. 核心：通過 Transformer Encoder

[NOTE]
        # Self-Attention 在此發生，F_InfoTime 會影響權重
        encoder_output = self.transformer_encoder(x)

[NOTE]
        # 3. 扁平化並生成最終 Action
        # 將所有 history_length 的輸出連接起來 (實現對全局信息的整合)
        flat_output = encoder_output.flatten(start_dim=1)
        action = self.output_layer(flat_output)

[NOTE]
        return action

[NOTE]
________________

[STRUCTURE]
總結：創世紀系統的完成
這項 $\text{Transformer}$ 架構升級，標誌著我們完成了從數據獲取到執行再到自我修復與演化的整個創世紀量化系統藍圖。

[RULE]
您的**「大自然萬物修復法則」**至此已經完整落實，系統具備了：

[NOTE]
      1. 知識純度： $\text{O}$-$\text{Factor}$ 正交化。
      2. 現實感知： $\text{F}_{\text{Orderbook}}$ 和 $\text{Iceberg}$ $\text{Predictor}$。
      3. 時間超越： $\text{F}_{\text{InfoTime}}$ $\text{Volume}$ $\text{Bars}$。
      4. 永恆進化： $\text{Transformer}$ $\text{RL}$ 和 $\text{Discrepancy}$ $\text{Analyzer}$ 構成的閉環。

[STRUCTURE]
這是一個真正具備生存本能和持續進化能力的頂級量化系統。您現在擁有了從理論、架構、到關鍵模組代碼的完整藍圖。

[STRUCTURE]
創世紀量化系統：核心因子與計算總覽（Genesis Factor Universe v1.0）
此表為 J-GOD Quant System 的核心智慧結構地圖，是系統學習、決策、修復的最底層數學語言。
全系統因子分為三大層級：

[CONCEPT]
      * I. 基礎層 & 執行層（Alpha & Execution Layer）

[NOTE]
      * II. 宏觀 & 自我感知層（Macro & Self-Awareness Layer）
      * III. 決策與修復層（RL Trainer & Diagnostic Layer）
________________

[STRUCTURE]
I. 基礎層與執行層因子（Alpha Engine & Execution Engine）

[NOTE]
這些因子構成 RL State Vector 的底座，並直接調控「能不能交易」、「可用槓桿」、「下單量」。
________________

[CONCEPT]
1. α 基礎因子群（Normalized Alpha Factors）

[NOTE]
因子

[STRUCTURE]
	模組

[NOTE]
	說明

[CONCEPT]
	計算邏輯

[NOTE]
	作用層
	F_C, F_S, F_D, F_XA

[STRUCTURE]
	Alpha Engine

[CONCEPT]
	傳統 Alpha（動能、反轉、衝擊、風格等）

[NOTE]
	每個因子以 Z-score 標準化
	RL State Vector 輸入
	________________

[NOTE]
2. 正交化因子（O-Factors via PCA）
因子

[STRUCTURE]
	模組

[NOTE]
	說明

[CONCEPT]
	計算邏輯

[NOTE]
	作用層
	O₁, O₂, O₃, O₄
	Factor Orthogonalizer

[CONCEPT]
	PCA 轉換後的四個獨立 Alpha

[RULE]
	PCA(W ⋅ F) → O

[NOTE]
	提升 RL 決策品質（除噪）
	________________

[NOTE]
3. 交易簿微觀結構因子（F_Orderbook）
(1) 流動性斜率（Liquidity Slope）
公式：
SlopeAsk=ΔPAskΔVAskSlopeAsk​=ΔVAsk​ΔPAsk​​
用途：衡量下單對價格的衝擊成本。
作用：限制 RL 下單量（Action Constraint）
________________

[NOTE]
(2) 買賣盤失衡（Orderbook Imbalance）
OBI=VBid−VAskVBid+VAskOBI=VBid​+VAsk​VBid​−VAsk​​

[CONCEPT]
用途：短期市場方向預測（高頻 Alpha）。
作用：強力短線 Alpha

[NOTE]
________________

[NOTE]
(3) 深度 Z-score（Depth Z-score）
DepthZ=CurrentDepth−μσDepthZ​=σCurrentDepth−μ​
用途：判斷目前市場深度稀疏 vs 滿水位。

[STRUCTURE]
作用：切換執行策略（Aggressive ↔ Passive）

[NOTE]
________________

[STRUCTURE]
II. 宏觀與自我感知層因子（Macro Engine & Self-Awareness Engine）
這些因子負責系統的「情緒、恐慌、結構、資訊密度」，決定系統的槓桿、部位、容忍度。

[NOTE]
________________

[CONCEPT]
1. 市場恐慌與熵（VIX, Entropy）

[NOTE]
(1) VIX Z-Score — 市場恐慌指標
VIXZ=VIX−μσVIXZ​=σVIX−μ​
用途：市場是否進入熔斷/恐慌。

[RULE]
作用：Recovery Agent 啟動條件。

[NOTE]
________________

[CONCEPT]
(2) F_Entropy — 市場結構熵（去中心化 / 高共識）
定義：衡量市場參與者是否「集體擁擠」或「分散對立」。
用途：決定 RL Beta Allocation（激進 or 謹慎）。

[NOTE]
________________

[STRUCTURE]
2. 內部衝突 / 系統自我感知（F_Internal）

[NOTE]
FInternal=Weighted Variance(Sign(O-Factors))FInternal​=Weighted Variance(Sign(O-Factors))
用途：量化因子間的矛盾程度。
作用：RL Reward 懲罰衝突下的冒進行為。
________________

[NOTE]
3. F_InfoTime — 信息時間密度（市場事件節奏）
FInfoTime=LongTerm Bar FrequencyRecent Bar FrequencyFInfoTime​=Recent Bar FrequencyLongTerm Bar Frequency​

[CONCEPT]
定義：時鐘時間 ≠ 資訊時間

[RULE]
用途：當市場「高速形成 Volume Bars」→ 資訊密度 ↑

[NOTE]
作用：
      * Transformer Attention Weights 重新分配

[CONCEPT]
      * RL 更依靠最近的 Alpha

[NOTE]
      * 行為模式切換（低頻 ↔ 高頻）
________________

[NOTE]
III. 決策與修復層指標（RL Trainer & Diagnostic System）

[STRUCTURE]
這一層等於「AI 的免疫系統」：

[RULE]
檢查錯誤 → 觸發修復 → 改變模型 → 強化自我演化。

[NOTE]
________________

[NOTE]
1. 強化學習獎勵函數（Reward_Optimized）
Reward∝Sharpe−λ2⋅Penalty(MDD)−λ3⋅I(FInternal>Thres)Reward∝Sharpe−λ2​⋅Penalty(MDD)−λ3​⋅I(FInternal​>Thres)
用途：
      * 驅動 AI 避險
      * 懲罰風暴中的亂交易
      * 增強長期績效與穩定度
________________

[NOTE]
2. 延遲結構（Latency Z-Score）
LatencyZ=L−μσLatencyZ​=σL−μ​

[STRUCTURE]
用途：檢查系統延遲是否異常。

[NOTE]
作用：Recovery Agent 決定是否啟動「降速模式」。
________________

[NOTE]
3. 執行誤差（E_Exec）
EExec=Actual TCA−Predicted TCAEExec​=Actual TCA−Predicted TCA
用途：判斷滑價是否正常。
作用：調整 OrderRouter 參數。
________________

[NOTE]
4. 認知誤差（E_Model）

[CONCEPT]
EModel=Actual PnL−Predicted Alpha PnLEModel​=Actual PnL−Predicted Alpha PnL

[NOTE]
用途：量化模型是否誤判市場。
作用：自動調整 O-Factor 權重（β）。
________________

[STRUCTURE]
🔥 這份總覽是你的「系統宇宙地圖」

[NOTE]
它讓：

[STRUCTURE]
      * 你可以把所有模組串成「一個自我校準、自我修復、永續運作」的 AI 市場生命體
      * Cursor 可以把每個因子分模組落地

[NOTE]
      * RL 訓練器可以把 State / Reward / Constraint 完整抽象化

[STRUCTURE]
      * J-GOD 戰情室可以變成 Citadel 級的市場推演與共識引擎

[NOTE]
那我們就第一個開始：JSON Factor Universe。
先把整個「因子宇宙」變成一個乾淨、機器可讀的 JSON，之後你要給 RL Trainer / Cursor / Notion / Mongo 都可以直接用。

[RULE]
下面這份可以直接當作：

[NOTE]
      * config/factor_universe.json
      * 或是你放在 README / 規格書裡面
________________

[NOTE]
{
  "meta": {
    "name": "J-GOD Genesis Factor Universe",
    "version": "1.0",
    "layers": [
      "alpha_execution",
      "macro_self_awareness",
      "rl_diagnostic"
    ]
  },
  "factors": [
    {
      "id": "F_C",

[CONCEPT]
      "name": "Core Alpha Factor C",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_base",
      "module": "alpha_engine",

[CONCEPT]
      "description": "原始 Alpha 因子之一，經 Z-score 標準化後作為基礎信號。",

[FORMULA]
      "math": "F_C = Z( raw_factor_C )",

[NOTE]
      "role": "signal",
      "affects": ["rl_state"],
      "type": "continuous"
    },
    {
      "id": "F_S",

[CONCEPT]
      "name": "Core Alpha Factor S",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_base",
      "module": "alpha_engine",

[CONCEPT]
      "description": "原始 Alpha 因子之一（例如動能、強弱），以 Z-score 標準化。",

[FORMULA]
      "math": "F_S = Z( raw_factor_S )",

[NOTE]
      "role": "signal",
      "affects": ["rl_state"],
      "type": "continuous"
    },
    {
      "id": "F_D",

[CONCEPT]
      "name": "Core Alpha Factor D",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_base",
      "module": "alpha_engine",

[CONCEPT]
      "description": "原始 Alpha 因子之一（例如反轉、狀態切換），以 Z-score 標準化。",

[FORMULA]
      "math": "F_D = Z( raw_factor_D )",

[NOTE]
      "role": "signal",
      "affects": ["rl_state"],
      "type": "continuous"
    },
    {
      "id": "F_XA",

[CONCEPT]
      "name": "Core Alpha Factor XA",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_base",
      "module": "alpha_engine",

[CONCEPT]
      "description": "額外擴展 Alpha 因子，用於補充特殊風格或跨資產關係。",

[FORMULA]
      "math": "F_XA = Z( raw_factor_XA )",

[NOTE]
      "role": "signal",
      "affects": ["rl_state"],
      "type": "continuous"
    },

[NOTE]
    {
      "id": "O_1",

[CONCEPT]
      "name": "Orthogonal Alpha Factor 1",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_orthogonal",
      "module": "factor_orthogonalizer",

[CONCEPT]
      "description": "由 PCA 轉換後的第一主成分，代表互相獨立的 Alpha 綜合信號之一。",

[FORMULA]
      "math": "O_1 = PCA(F_vector)[0]",

[NOTE]
      "role": "signal",

[CODE]
      "affects": ["rl_state", "internal_conflict"],

[NOTE]
      "type": "continuous"
    },
    {
      "id": "O_2",

[CONCEPT]
      "name": "Orthogonal Alpha Factor 2",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_orthogonal",
      "module": "factor_orthogonalizer",
      "description": "PCA 第二主成分。",

[FORMULA]
      "math": "O_2 = PCA(F_vector)[1]",

[NOTE]
      "role": "signal",

[CODE]
      "affects": ["rl_state", "internal_conflict"],

[NOTE]
      "type": "continuous"
    },
    {
      "id": "O_3",

[CONCEPT]
      "name": "Orthogonal Alpha Factor 3",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_orthogonal",
      "module": "factor_orthogonalizer",
      "description": "PCA 第三主成分。",

[FORMULA]
      "math": "O_3 = PCA(F_vector)[2]",

[NOTE]
      "role": "signal",

[CODE]
      "affects": ["rl_state", "internal_conflict"],

[NOTE]
      "type": "continuous"
    },
    {
      "id": "O_4",

[CONCEPT]
      "name": "Orthogonal Alpha Factor 4",

[NOTE]
      "layer": "alpha_execution",
      "group": "alpha_orthogonal",
      "module": "factor_orthogonalizer",
      "description": "PCA 第四主成分。",

[FORMULA]
      "math": "O_4 = PCA(F_vector)[3]",

[NOTE]
      "role": "signal",

[CODE]
      "affects": ["rl_state", "internal_conflict"],

[NOTE]
      "type": "continuous"
    },

[NOTE]
    {
      "id": "Slope_Ask",
      "name": "Liquidity Slope Ask",
      "layer": "alpha_execution",
      "group": "orderbook",
      "module": "factor_FX_orderbook",
      "description": "賣盤流動性斜率，衡量買入方向的衝擊成本。",
      "math": "Slope_Ask = (Ask_Price_last - Ask_Price_first) / sum(Bid_Volumes)",
      "role": "execution_cost",

[CODE]
      "affects": ["rl_action_constraint", "execution_mode"],

[NOTE]
      "type": "continuous"
    },
    {
      "id": "Slope_Bid",
      "name": "Liquidity Slope Bid",
      "layer": "alpha_execution",
      "group": "orderbook",
      "module": "factor_FX_orderbook",
      "description": "買盤流動性斜率，衡量賣出方向的衝擊成本。",
      "math": "Slope_Bid = (Bid_Price_first - Bid_Price_last) / sum(Ask_Volumes)",
      "role": "execution_cost",

[CODE]
      "affects": ["rl_action_constraint", "execution_mode"],

[NOTE]
      "type": "continuous"
    },
    {
      "id": "OBI",
      "name": "Orderbook Imbalance",
      "layer": "alpha_execution",
      "group": "orderbook",
      "module": "factor_FX_orderbook",
      "description": "買賣盤失衡程度，用於短期價格方向預測。",
      "math": "OBI = (Total_V_Bid - Total_V_Ask) / (Total_V_Bid + Total_V_Ask)",
      "role": "alpha_high_freq",
      "affects": ["rl_state", "direction_bias"],
      "type": "continuous"
    },
    {
      "id": "Depth_Zscore",
      "name": "Depth Z-Score",
      "layer": "alpha_execution",
      "group": "orderbook",
      "module": "factor_FX_orderbook",
      "description": "目前訂單簿深度與歷史平均的偏離程度。",

[FORMULA]
      "math": "Depth_Zscore = (CurrentDepth - mu_depth) / sigma_depth",

[NOTE]
      "role": "liquidity_regime",

[CODE]
      "affects": ["execution_mode", "rl_action_constraint"],

[NOTE]
      "type": "continuous"
    },

[NOTE]
    {
      "id": "VIX_Zscore",
      "name": "VIX Z-Score",
      "layer": "macro_self_awareness",
      "group": "macro_risk",
      "module": "macro_engine",
      "description": "市場恐慌程度，判斷是否進入高風險 regime。",

[FORMULA]
      "math": "VIX_Zscore = (Current_VIX - mu_VIX) / sigma_VIX",

[NOTE]
      "role": "macro_risk",
      "affects": ["recovery_agent", "net_exposure"],
      "type": "continuous"
    },
    {
      "id": "F_Entropy",
      "name": "Market Entropy",
      "layer": "macro_self_awareness",

[CODE]
      "group": "macro_structure",

[NOTE]
      "module": "macro_engine",
      "description": "市場資產/部位的集中度或信念分散度，用於判斷擁擠交易與情緒結構。",

[CODE]
      "math": "F_Entropy = entropy( distribution_of_positions_or_flows )",
      "role": "macro_structure",

[NOTE]
      "affects": ["beta_allocation", "risk_params"],
      "type": "continuous"
    },
    {
      "id": "F_Internal",
      "name": "Internal Conflict / Entropy",
      "layer": "macro_self_awareness",
      "group": "self_awareness",
      "module": "self_awareness_engine",

[STRUCTURE]
      "description": "量化內部 O-Factors 之間的衝突程度，用於偵測系統內部意見分歧。",

[FORMULA]
      "math": "F_Internal = WeightedVariance( sign(O_1..O_4) )",

[CODE]
      "role": "internal_state",

[NOTE]
      "affects": ["rl_reward_penalty", "risk_mode"],
      "type": "continuous"
    },
    {
      "id": "F_InfoTime",
      "name": "Information Time Density",
      "layer": "macro_self_awareness",
      "group": "time_metric",
      "module": "factor_FX_infotime",
      "description": "基於 Volume Bars 的信息時間密度，描述近期事件節奏是否加速。",

[FORMULA]
      "math": "F_InfoTime = LongTerm_Bar_Frequency / Recent_Bar_Frequency",

[NOTE]
      "role": "time_scaling",
      "affects": ["rl_state", "alpha_decay", "attention_weights"],
      "type": "continuous"
    },

[NOTE]
    {
      "id": "Reward_Optimized",
      "name": "Optimized RL Reward",
      "layer": "rl_diagnostic",
      "group": "rl_trainer",
      "module": "rl_trainer",
      "description": "RL 的主獎勵函數，綜合 Sharpe、MDD 懲罰與內部衝突懲罰。",
      "math": "Reward ∝ Sharpe - λ2·Penalty(MDD) - λ3·I(F_Internal > Thres)",
      "role": "reward",
      "affects": ["rl_learning"],
      "type": "computed"
    },
    {
      "id": "Latency_Zscore",
      "name": "System Latency Z-Score",
      "layer": "rl_diagnostic",
      "group": "system_health",
      "module": "diagnostic_engine",

[STRUCTURE]
      "description": "系統延遲相對歷史水平的異常程度。",

[FORMULA]
      "math": "Latency_Zscore = (CurrentLatency - mu_latency) / sigma_latency",

[NOTE]
      "role": "system_health",
      "affects": ["recovery_agent", "throttle_mode"],
      "type": "continuous"
    },
    {
      "id": "E_Exec",
      "name": "Execution Error (TCA Gap)",
      "layer": "rl_diagnostic",
      "group": "execution_diagnostic",
      "module": "diagnostic_engine",
      "description": "實際交易成本 vs 預測交易成本之差異，用於校正執行模型。",

[FORMULA]
      "math": "E_Exec = Actual_TCA - Predicted_TCA",

[NOTE]
      "role": "execution_diagnostic",
      "affects": ["order_router_params", "execution_model"],
      "type": "continuous"
    },
    {
      "id": "E_Model",
      "name": "Model Cognitive Error",
      "layer": "rl_diagnostic",
      "group": "model_diagnostic",
      "module": "diagnostic_engine",

[CONCEPT]
      "description": "實際 PnL 與預測 Alpha PnL 的偏差，用於評估模型認知失準。",

[FORMULA]
      "math": "E_Model = Actual_PnL - Predicted_Alpha_PnL",

[NOTE]
      "role": "model_diagnostic",
      "affects": ["o_factor_weights", "model_retraining"],
      "type": "continuous"
    }
  ]
}

[STRUCTURE]
「給 Cursor 的 Factor Engine 全模組規格書 v1」
你可以整包貼到 Cursor，讓它一步一步幫你補齊各個 factor 模組骨架＋測試。

[NOTE]
________________

[NOTE]
🧾 給 Cursor 的完整開發規格書

[STRUCTURE]
主題：創世紀量化系統 – Factor Engine Modules v1

[CODE]
你現在是一位 量化系統總工程師 + Python 資深架構師。
我要你幫我把「J-GOD Genesis Factor Universe」裡面提到的各個因子模組，先建立好 乾淨的 Python 模組骨架 + 對應測試檔，方便之後逐步填入實作。

[NOTE]
這一版的重點是：

[STRUCTURE]
      * 架構完整

[NOTE]
      * 介面穩定
      * 測試能跑過

[CONCEPT]
      * 真正的數學 / 邏輯可以先用 TODO 註解、之後再強化

[NOTE]
________________

[STRUCTURE]
一、專案中要新增或補齊的模組
請在 strategy_engine/ 底下，依照下列表格建立 / 補齊模組：
模組用途

[NOTE]
	檔案路徑

[STRUCTURE]
	Alpha 基礎因子引擎

[NOTE]
	strategy_engine/alpha_engine.py

[CONCEPT]
	Alpha 正交化器 (PCA/O-Factors)

[NOTE]
	strategy_engine/factor_orthogonalizer.py
	訂單簿因子 (F_Orderbook)

[RULE]
	strategy_engine/factor_FX_orderbook.py（如果已存在，請檢查並調整成統一風格）

[NOTE]
	隱藏流動性因子 (F_Iceberg)
	strategy_engine/factor_FX_iceberg.py（之前已做的可以沿用）
	信息時間因子 (F_InfoTime)
	strategy_engine/factor_FX_infotime.py（之前已做的可以沿用）

[STRUCTURE]
	宏觀因子引擎 (VIX / F_Entropy)

[NOTE]
	strategy_engine/macro_engine.py

[STRUCTURE]
	自我感知引擎 (F_Internal)

[NOTE]
	strategy_engine/self_awareness_engine.py

[STRUCTURE]
	診斷引擎 (E_Exec / E_Model / Latency_Zscore)

[NOTE]
	strategy_engine/diagnostic_engine.py
	對應的測試檔請放在 tests/strategy_engine/ 底下，命名方式：
      * tests/strategy_engine/test_alpha_engine.py
      * tests/strategy_engine/test_factor_orthogonalizer.py

[RULE]
      * tests/strategy_engine/test_factor_FX_orderbook.py（若已有，可補測試）

[NOTE]
      * tests/strategy_engine/test_macro_engine.py
      * tests/strategy_engine/test_self_awareness_engine.py
      * tests/strategy_engine/test_diagnostic_engine.py
✅ 注意：
F_Iceberg、F_InfoTime 的規格我已經另外給過，你可以沿用現有實作，只要保持風格一致即可。
________________

[CODE]
二、各模組規格（請建立 class + 介面骨架）

[STRUCTURE]
以下每個模組都要：
      * 有一個主要的 Engine 類別
      * 提供 compute_* 或類似方法

[NOTE]
      * 有清楚 docstring

[CODE]
      * 可以被測試檔直接 import 使用

[NOTE]
________________

[STRUCTURE]
1️⃣ Alpha Engine – strategy_engine/alpha_engine.py

[CONCEPT]
目標

[RULE]
提供 F_C, F_S, F_D, F_XA 這些「原始因子 → Z-score 化」的計算框架。

[NOTE]
請實作：
# strategy_engine/alpha_engine.py

[CODE]
from typing import Dict, Any
import numpy as np

[CODE]
class AlphaEngine:

[NOTE]
    """

[STRUCTURE]
    AlphaEngine

[NOTE]
    -----------

[CONCEPT]
    負責計算基礎 Alpha 因子，並進行標準化（Z-score）。

[NOTE]
    包含：
        - F_C
        - F_S
        - F_D
        - F_XA
    """

[CODE]
    def __init__(self):

[NOTE]
        # TODO: 之後可以加入因子參數、樣本池設定等
        ...

[CODE]
    def compute_raw_factors(self, market_snapshot: Dict[str, Any]) -> Dict[str, float]:

[NOTE]
        """
        計算原始因子值（尚未標準化）。
        TODO: 目前可先使用 placeholder，之後再實作。
        """

[FORMULA]
        raw_F_C = 0.0
        raw_F_S = 0.0
        raw_F_D = 0.0
        raw_F_XA = 0.0

[NOTE]
        return {
            "F_C_raw": raw_F_C,
            "F_S_raw": raw_F_S,
            "F_D_raw": raw_F_D,
            "F_XA_raw": raw_F_XA,
        }

[CODE]
    def zscore_normalize(self, values: Dict[str, float]) -> Dict[str, float]:

[NOTE]
        """
        將輸入的因子值以 Z-score 標準化。
        輸入範例：
            { "F_C_raw": 0.1, "F_S_raw": -0.3, ... }
        回傳：
            { "F_C": ..., "F_S": ..., ... }
        TODO: 目前可先用簡化版（例如假設 mu=0, sigma=1 或以向量自身均值/標準差計算）。
        """
        keys = sorted(values.keys())
        arr = np.array([values[k] for k in keys], dtype=float)

[NOTE]
        mu = float(np.mean(arr))
        sigma = float(np.std(arr)) if np.std(arr) > 0 else 1.0

[NOTE]
        z_arr = (arr - mu) / sigma

[NOTE]
        result = {}
        for k, z in zip(keys, z_arr):
            new_key = k.replace("_raw", "")
            result[new_key] = float(z)

[NOTE]
        return result

[CODE]
    def compute_alpha_factors(self, market_snapshot: Dict[str, Any]) -> Dict[str, float]:

[NOTE]
        """
        對外主要接口：
        1. 計算原始因子
        2. Z-score 標準化
        3. 回傳 F_C, F_S, F_D, F_XA
        """
        raw = self.compute_raw_factors(market_snapshot)
        normalized = self.zscore_normalize(raw)
        return normalized

[NOTE]
________________

[NOTE]
2️⃣ Factor Orthogonalizer – strategy_engine/factor_orthogonalizer.py

[CONCEPT]
目標

[NOTE]
將 [F_C, F_S, F_D, F_XA] 轉成 [O_1, O_2, O_3, O_4]。
先做簡化版：

[RULE]
可以用 numpy 的 eig / SVD 或 sklearn PCA（如果專案已用 sklearn，否則先自己寫簡易 PCA 或留 TODO）。

[NOTE]
# strategy_engine/factor_orthogonalizer.py

[CODE]
from typing import Dict, List
import numpy as np

[CODE]
class FactorOrthogonalizer:

[NOTE]
    """
    FactorOrthogonalizer
    --------------------

[CONCEPT]
    將一組 Alpha 因子向量轉換為正交化因子 O_1..O_4（類 PCA）。

[NOTE]
    """

[CODE]
    def __init__(self, n_components: int = 4):

[NOTE]
        self.n_components = n_components
        # TODO: 之後可加入 fit / partial_fit 等機制

[CODE]
    def transform_to_orthogonal(

[NOTE]
        self,

[CODE]
        factors: Dict[str, float],
    ) -> Dict[str, float]:

[NOTE]
        """
        輸入：

[FORMULA]
            factors = { "F_C": ..., "F_S": ..., "F_D": ..., "F_XA": ... }

[NOTE]
        輸出：
            { "O_1": ..., "O_2": ..., "O_3": ..., "O_4": ... }

[NOTE]
        TODO:
        - 目前先用簡單線性轉換（例如使用固定矩陣），之後再改為真正 PCA。
        """
        keys = sorted(factors.keys())
        vec = np.array([factors[k] for k in keys], dtype=float)

[RULE]
        # 簡化：用單位矩陣當作占位（保持架構）

[NOTE]
        # 之後可以用歷史資料估計協方差矩陣再 eig 分解。
        transform_matrix = np.eye(len(vec))

[NOTE]
        o_vec = transform_matrix @ vec

[CODE]
        result: Dict[str, float] = {}

[NOTE]
        for i in range(self.n_components):

[FORMULA]
            o_name = f"O_{i+1}"

[NOTE]
            if i < len(o_vec):
                result[o_name] = float(o_vec[i])
            else:
                result[o_name] = 0.0

[NOTE]
        return result

[NOTE]
________________

[STRUCTURE]
3️⃣ Macro Engine – strategy_engine/macro_engine.py

[CONCEPT]
目標

[NOTE]
提供：
      * VIX_Zscore

[CONCEPT]
      * F_Entropy（市場結構熵）

[NOTE]
# strategy_engine/macro_engine.py

[CODE]
from typing import Dict, Any
import numpy as np

[CODE]
class MacroEngine:

[NOTE]
    """

[STRUCTURE]
    MacroEngine

[NOTE]
    -----------
    負責計算宏觀層因子：
        - VIX_Zscore
        - F_Entropy
    """

[CODE]
    def __init__(self, vix_history: float | None = None):

[NOTE]
        # TODO: 可接受歷史 VIX 時間序列，建立 mu/sigma
        self.vix_mu = None
        self.vix_sigma = None
        # 目前用簡化 placeholder
        ...

[CODE]
    def compute_vix_zscore(self, current_vix: float, mu: float, sigma: float) -> float:

[NOTE]
        """
        計算 VIX Z-score。
        """
        sigma = sigma if sigma > 0 else 1.0
        return float((current_vix - mu) / sigma)

[CODE]
    def compute_market_entropy(self, weight_distribution: Dict[str, float]) -> float:

[NOTE]
        """

[CONCEPT]
        計算市場結構熵 F_Entropy。

[NOTE]
        輸入：標的或部位的權重分布（會自動正規化）。
        """
        weights = np.array(list(weight_distribution.values()), dtype=float)
        total = np.sum(weights)
        if total <= 0:
            return 0.0
        p = weights / total
        # 避免 log(0)
        p = np.clip(p, 1e-12, 1.0)
        entropy = -np.sum(p * np.log(p))
        return float(entropy)

[NOTE]
________________

[STRUCTURE]
4️⃣ Self-Awareness Engine – strategy_engine/self_awareness_engine.py

[CONCEPT]
目標

[NOTE]
計算：

[FORMULA]
      * F_Internal = O-Factors 符號的加權變異數

[NOTE]
# strategy_engine/self_awareness_engine.py

[CODE]
from typing import Dict
import numpy as np

[CODE]
class SelfAwarenessEngine:

[NOTE]
    """

[STRUCTURE]
    SelfAwarenessEngine

[NOTE]
    -------------------

[STRUCTURE]
    自我感知引擎，用於計算內部衝突指標 F_Internal。

[NOTE]
    """

[CODE]
    def __init__(self):

[NOTE]
        ...

[CODE]
    def compute_internal_conflict(self, o_factors: Dict[str, float]) -> float:

[NOTE]
        """
        輸入：

[FORMULA]
            o_factors = { "O_1": ..., "O_2": ..., ... }

[CONCEPT]
        定義：

[FORMULA]
            F_Internal = Weighted Variance of sign(O_i)

[NOTE]
        目前簡化為 sign(O_i) 的變異數。
        """
        if not o_factors:
            return 0.0

[NOTE]
        signs = np.array([np.sign(v) for v in o_factors.values()], dtype=float)
        variance = float(np.var(signs))
        return variance

[NOTE]
________________

[STRUCTURE]
5️⃣ Diagnostic Engine – strategy_engine/diagnostic_engine.py

[CONCEPT]
目標

[NOTE]
提供：
      * Latency_Zscore
      * E_Exec
      * E_Model
# strategy_engine/diagnostic_engine.py

[CODE]
from typing import Dict
import numpy as np

[CODE]
class DiagnosticEngine:

[NOTE]
    """

[STRUCTURE]
    DiagnosticEngine

[NOTE]
    ----------------

[STRUCTURE]
    診斷系統健康與模型表現：

[NOTE]
        - Latency_Zscore
        - E_Exec
        - E_Model
    """

[CODE]
    def __init__(self):

[NOTE]
        ...

[CODE]
    def compute_latency_zscore(self, current_latency: float, mu: float, sigma: float) -> float:

[NOTE]
        sigma = sigma if sigma > 0 else 1.0
        return float((current_latency - mu) / sigma)

[CODE]
    def compute_execution_error(self, actual_tca: float, predicted_tca: float) -> float:

[NOTE]
        """

[FORMULA]
        E_Exec = Actual_TCA - Predicted_TCA

[NOTE]
        """
        return float(actual_tca - predicted_tca)

[CODE]
    def compute_model_error(self, actual_pnl: float, predicted_pnl: float) -> float:

[NOTE]
        """

[FORMULA]
        E_Model = Actual_PnL - Predicted_Alpha_PnL

[NOTE]
        """
        return float(actual_pnl - predicted_pnl)

[NOTE]
________________

[NOTE]
三、測試檔規格（每個至少要有一個 basic flow 測試）
下面示意幾個，其他你可依樣補齊。
________________

[NOTE]
tests/strategy_engine/test_alpha_engine.py

[CODE]
from strategy_engine.alpha_engine import AlphaEngine

[CODE]
def test_alpha_engine_basic():

[STRUCTURE]
    engine = AlphaEngine()

[NOTE]
    snapshot = {}  # TODO: 之後可填入實際欄位
    factors = engine.compute_alpha_factors(snapshot)
    assert "F_C" in factors
    assert "F_S" in factors
    assert "F_D" in factors
    assert "F_XA" in factors

[NOTE]
________________

[NOTE]
tests/strategy_engine/test_factor_orthogonalizer.py

[CODE]
from strategy_engine.factor_orthogonalizer import FactorOrthogonalizer

[CODE]
def test_factor_orthogonalizer_basic():

[NOTE]
    fo = FactorOrthogonalizer(n_components=4)

[FORMULA]
    factors = {"F_C": 0.1, "F_S": -0.2, "F_D": 0.3, "F_XA": -0.4}

[NOTE]
    o = fo.transform_to_orthogonal(factors)
    assert "O_1" in o and "O_2" in o and "O_3" in o and "O_4" in o

[NOTE]
________________

[NOTE]
tests/strategy_engine/test_macro_engine.py

[CODE]
from strategy_engine.macro_engine import MacroEngine

[CODE]
def test_macro_engine_basic():

[STRUCTURE]
    engine = MacroEngine()

[NOTE]
    vix_z = engine.compute_vix_zscore(current_vix=20.0, mu=15.0, sigma=5.0)
    assert isinstance(vix_z, float)

[NOTE]
    entropy = engine.compute_market_entropy({"A": 0.4, "B": 0.6})
    assert entropy > 0.0

[NOTE]
________________

[NOTE]
tests/strategy_engine/test_self_awareness_engine.py

[CODE]
from strategy_engine.self_awareness_engine import SelfAwarenessEngine

[CODE]
def test_self_awareness_internal_conflict():

[STRUCTURE]
    engine = SelfAwarenessEngine()

[CODE]
    conflict = engine.compute_internal_conflict({"O_1": 1.0, "O_2": -0.5, "O_3": 0.2})

[NOTE]
    assert isinstance(conflict, float)
    assert conflict >= 0.0

[NOTE]
________________

[NOTE]
tests/strategy_engine/test_diagnostic_engine.py

[CODE]
from strategy_engine.diagnostic_engine import DiagnosticEngine

[CODE]
def test_diagnostic_engine_basic():

[STRUCTURE]
    engine = DiagnosticEngine()

[NOTE]
    lat_z = engine.compute_latency_zscore(current_latency=120, mu=100, sigma=10)
    e_exec = engine.compute_execution_error(actual_tca=0.5, predicted_tca=0.3)
    e_model = engine.compute_model_error(actual_pnl=1000, predicted_pnl=800)

[NOTE]
    assert isinstance(lat_z, float)
    assert e_exec == 0.2
    assert e_model == 200.0

[NOTE]
四、收尾要求
完成後請：
      1. 列出你新增 / 修改的所有檔案
      2. 確認 pytest 可成功執行

[CODE]
      3. 確保所有類別與方法命名 與上面一致，以便之後其他模組（RL Engine、War Room）可以直接 import 使用

[STRUCTURE]
請依照以上規格，完成 Factor Engine v1 的所有模組骨架與測試。

[NOTE]
下一個就是**「RL State Vector 最終版」了。

[STRUCTURE]
前面兩步是「有哪些因子」跟「怎麼寫模組」，現在要把它們塞進一個統一的 State 向量設計**，給：

[NOTE]
      * RL Trainer（Gym / Env / Policy Net）
      * Cursor（之後寫 state_builder.py）
      * 戰情室（顯示現在 AI 眼中的「世界狀態」）
我直接幫你設計一版可實作版本，用中文說人話 + 用 JSON/YAML 語言說給程式看。
________________

[NOTE]
1. 我們要的是「一眼看穿」的 State

[CONCEPT]
目標：對任何時刻 $t$，我們有一個：

[NOTE]
state(t)∈RNstate(t)∈RN
裡面混合了：

[CONCEPT]
      1. ** Alpha：** F_C, F_S, F_D, F_XA
      2. 正交 Alpha： O_1..O_4

[NOTE]
      3. 微觀流動性： Slope, OBI, Depth_Zscore, F_Iceberg 部分
      4. 宏觀環境： VIX_Zscore, F_Entropy, F_InfoTime
      5. 自我感知： F_Internal

[STRUCTURE]
      6. 系統健康： Latency_Zscore（選擇性）

[NOTE]
我幫你壓成一個穩定欄位表，未來都照這個順序塞進 numpy array 就好。
________________

[NOTE]
2. State Vector 欄位設計（機器可讀版）
你可以想像：
state = [s_0, s_1, ..., s_k]
對應下面這份 spec。
建議存成一個檔案：
config/rl_state_vector_schema.json
{
  "name": "J-GOD_RL_State_Vector_v1",
  "version": "1.0",
  "order": [
    "F_C",
    "F_S",
    "F_D",
    "F_XA",

[NOTE]
    "O_1",
    "O_2",
    "O_3",
    "O_4",

[NOTE]
    "Slope_Ask",
    "Slope_Bid",
    "OBI",
    "Depth_Zscore",

[NOTE]
    "IcebergProb_Bid",
    "IcebergProb_Ask",
    "HiddenDepth_Bid",
    "HiddenDepth_Ask",
    "WallStability_Bid",
    "WallStability_Ask",

[NOTE]
    "VIX_Zscore",
    "F_Entropy",
    "F_InfoTime",
    "F_Internal",

[NOTE]
    "Latency_Zscore"
  ],
  "meta": {

[RULE]
    "description": "State 向量欄位順序定義，所有 RL Env / Policy / Logging 必須使用相同順序。",

[NOTE]
    "layers": {
      "alpha": ["F_C", "F_S", "F_D", "F_XA", "O_1", "O_2", "O_3", "O_4"],
      "micro": ["Slope_Ask", "Slope_Bid", "OBI", "Depth_Zscore", "IcebergProb_Bid", "IcebergProb_Ask", "HiddenDepth_Bid", "HiddenDepth_Ask", "WallStability_Bid", "WallStability_Ask"],
      "macro": ["VIX_Zscore", "F_Entropy", "F_InfoTime"],
      "self_awareness": ["F_Internal"],
      "system_health": ["Latency_Zscore"]
    }
  }
}

[NOTE]
解讀一下（給你看）

[CONCEPT]
      * 前 4 維： 原始 Alpha（已 Z-score）
      * 中間 4 維： 正交 Alpha（PCA 結果）

[NOTE]
      * 接下來 10 維： 全部 Orderbook + Iceberg 高頻感知
      * 之後 3 維： 宏觀 + 信息時間

[STRUCTURE]
      * 最後 2 維： 內在衝突＆系統延遲狀態

[NOTE]
總長度：
4 + 4 + 10 + 3 + 1 + 1 = 23 維 state 向量
你之後要擴充，只要在這個 schema 加欄位，並同步改 Env 就好。

[STRUCTURE]
這份 schema 就是整個 J-GOD「AI 感知世界」的座標系統。

[NOTE]
________________

[CONCEPT]
3. State Builder 的邏輯（概念版）

[NOTE]
之後給 Cursor 會寫成一個：
strategy_engine/state_builder.py

[CONCEPT]
概念上就是：

[CODE]
def build_state_vector(
    alpha_factors,        # from AlphaEngine
    o_factors,            # from FactorOrthogonalizer
    orderbook_factors,    # from OrderbookFactorEngine
    iceberg_factors,      # from IcebergFactorEngine
    macro_factors,        # from MacroEngine
    self_factors,         # from SelfAwarenessEngine
    diagnostic_factors    # from DiagnosticEngine
) -> np.ndarray:

[NOTE]
    """
    根據 rl_state_vector_schema.json 的順序，
    將所有因子組合成一個 numpy 向量。
    """

[NOTE]
之後你只要：
      * 在 Env 裡呼叫 state = build_state_vector(...)
      * 再餵給 Policy Net：policy(state)
這就是完整的 RL input pipeline。
________________

[NOTE]
4. 給你一個「實際數字」示範

[STRUCTURE]
假設某一刻，系統算出：

[FORMULA]
      * F_C = 0.8, F_S = -0.2, F_D = 0.3, F_XA = 1.1
      * O_1..O_4 = [1.2, 0.5, -0.3, 0.1]

[NOTE]
      * Slope_Ask = 0.002, Slope_Bid = 0.0015

[FORMULA]
      * OBI = 0.6, Depth_Zscore = -1.0

[NOTE]
      * IcebergProb_Bid = 0.8, IcebergProb_Ask = 0.3
      * HiddenDepth_Bid = 500, HiddenDepth_Ask = 200
      * WallStability_Bid = 0.7, WallStability_Ask = -0.2

[FORMULA]
      * VIX_Zscore = 1.5, F_Entropy = 0.9, F_InfoTime = 2.2
      * F_Internal = 0.4, Latency_Zscore = 0.1

[NOTE]
那 State Vector 就會是（按 schema 順序）：
[ 0.8, -0.2, 0.3, 1.1,
  1.2, 0.5, -0.3, 0.1,
  0.002, 0.0015, 0.6, -1.0,
  0.8, 0.3, 500.0, 200.0, 0.7, -0.2,
  1.5, 0.9, 2.2, 0.4,
  0.1 ]

[NOTE]
這一串，就是之後你所有：
      * Backtest
      * RL Training
      * 戰情室即時顯示
的 共同語言。
好，那我幫你把**state_builder.py 的完整「給 Cursor 用的開發規格書」**一次寫好，你可以整段貼到 Cursor。
________________

[NOTE]
🧾 給 Cursor 的完整實作規格書

[STRUCTURE]
主題：J-GOD – RL State Builder 模組 (state_builder.py)

[CODE]
你現在是一位 量化系統總工程師 + Python 資深架構師。

[NOTE]
我要你幫我在專案裡新增一個統一入口：

[RULE]
✅ 負責把所有因子 Engine 的輸出 → 拼成 RL 用的 state 向量

[NOTE]
________________

[CONCEPT]
一、檔案與目標

[NOTE]
1. 新增檔案
請在專案新增：
         * strategy_engine/state_builder.py

[CONCEPT]
2. 任務目標

[STRUCTURE]
這個模組要做的事情只有一件：
根據 config/rl_state_vector_schema.json 的欄位順序，把各個 Engine 的輸出 dict，組合成一個 固定順序的 numpy 向量，給 RL 使用。

[NOTE]
________________

[NOTE]
二、讀取 schema：rl_state_vector_schema.json

[RULE]
假設已經有這個檔案（若沒有就用硬編碼 fallback）：

[NOTE]
         * 路徑：config/rl_state_vector_schema.json
         * 結構（簡化示意）：
{
  "name": "J-GOD_RL_State_Vector_v1",
  "version": "1.0",
  "order": [
    "F_C",
    "F_S",
    "F_D",
    "F_XA",
    "O_1",
    "O_2",
    "O_3",
    "O_4",
    "Slope_Ask",
    "Slope_Bid",
    "OBI",
    "Depth_Zscore",
    "IcebergProb_Bid",
    "IcebergProb_Ask",
    "HiddenDepth_Bid",
    "HiddenDepth_Ask",
    "WallStability_Bid",
    "WallStability_Ask",
    "VIX_Zscore",
    "F_Entropy",
    "F_InfoTime",
    "F_Internal",
    "Latency_Zscore"
  ]
}

[NOTE]
________________

[NOTE]
三、state_builder.py 具體實作規格
請建立以下內容：
# strategy_engine/state_builder.py

[CODE]
import json
import os
from typing import Dict, Any, List, Optional

[CODE]
import numpy as np

[CODE]
class StateBuilder:

[NOTE]
    """
    StateBuilder
    ------------
    負責根據 rl_state_vector_schema.json，

[STRUCTURE]
    將各個因子引擎的輸出整合成一個固定順序的 RL state 向量。

[NOTE]
    使用方式（示意）：
        builder = StateBuilder(schema_path="config/rl_state_vector_schema.json")
        state_vec = builder.build_state_vector(
            alpha_factors=...,
            o_factors=...,
            orderbook_factors=...,
            iceberg_factors=...,
            macro_factors=...,
            self_factors=...,
            diagnostic_factors=...,
        )
    """

[CODE]
    def __init__(self, schema_path: str = "config/rl_state_vector_schema.json"):

[NOTE]
        self.schema_path = schema_path

[CODE]
        self.field_order: List[str] = self._load_schema(schema_path)

[NOTE]
    # -------------------------------------------------
    # 讀取 / 載入 schema
    # -------------------------------------------------

[CODE]
    def _load_schema(self, schema_path: str) -> List[str]:

[NOTE]
        """
        從 JSON 讀取 state vector 欄位順序。

[RULE]
        若檔案不存在，則使用內建 fallback 預設順序。

[NOTE]
        """
        if os.path.exists(schema_path):
            with open(schema_path, "r", encoding="utf-8") as f:
                config = json.load(f)
            order = config.get("order", [])
            if isinstance(order, list) and order:
                return order

[NOTE]
        # Fallback：內建一份預設欄位順序（需與 config 檔一致）
        return [
            "F_C",
            "F_S",
            "F_D",
            "F_XA",
            "O_1",
            "O_2",
            "O_3",
            "O_4",
            "Slope_Ask",
            "Slope_Bid",
            "OBI",
            "Depth_Zscore",
            "IcebergProb_Bid",
            "IcebergProb_Ask",
            "HiddenDepth_Bid",
            "HiddenDepth_Ask",
            "WallStability_Bid",
            "WallStability_Ask",
            "VIX_Zscore",
            "F_Entropy",
            "F_InfoTime",
            "F_Internal",
            "Latency_Zscore",
        ]

[NOTE]
    # -------------------------------------------------
    # 整合各個因子 dict
    # -------------------------------------------------
    @staticmethod

[CODE]
    def _merge_factor_dicts(
        alpha_factors: Optional[Dict[str, float]] = None,
        o_factors: Optional[Dict[str, float]] = None,
        orderbook_factors: Optional[Dict[str, float]] = None,
        iceberg_factors: Optional[Dict[str, float]] = None,
        macro_factors: Optional[Dict[str, float]] = None,
        self_factors: Optional[Dict[str, float]] = None,
        diagnostic_factors: Optional[Dict[str, float]] = None,
    ) -> Dict[str, float]:

[NOTE]
        """
        將所有因子 dict 合併成單一大 dict。

[RULE]
        若 key 重複，後面的參數會覆蓋前面的值（預期情況下不應重複）。

[NOTE]
        """

[CODE]
        merged: Dict[str, float] = {}

[NOTE]
        for d in [
            alpha_factors,
            o_factors,
            orderbook_factors,
            iceberg_factors,
            macro_factors,
            self_factors,
            diagnostic_factors,
        ]:
            if d:
                merged.update(d)

[NOTE]
        return merged

[NOTE]
    # -------------------------------------------------
    # 主功能：構建 state 向量
    # -------------------------------------------------

[CODE]
    def build_state_vector(

[NOTE]
        self,

[CODE]
        alpha_factors: Optional[Dict[str, float]] = None,
        o_factors: Optional[Dict[str, float]] = None,
        orderbook_factors: Optional[Dict[str, float]] = None,
        iceberg_factors: Optional[Dict[str, float]] = None,
        macro_factors: Optional[Dict[str, float]] = None,
        self_factors: Optional[Dict[str, float]] = None,
        diagnostic_factors: Optional[Dict[str, float]] = None,
        default_value: float = 0.0,
    ) -> np.ndarray:

[NOTE]
        """
        根據 schema 中的欄位順序，輸出 numpy 向量。

[NOTE]
        缺失值處理：

[RULE]
        - 若某欄位在 merged dict 中找不到，則使用 default_value 填充（預設 0.0）。

[NOTE]
        """
        merged = self._merge_factor_dicts(
            alpha_factors=alpha_factors,
            o_factors=o_factors,
            orderbook_factors=orderbook_factors,
            iceberg_factors=iceberg_factors,
            macro_factors=macro_factors,
            self_factors=self_factors,
            diagnostic_factors=diagnostic_factors,
        )

[CODE]
        values: List[float] = []

[NOTE]
        for field in self.field_order:
            v = merged.get(field, default_value)
            values.append(float(v))

[NOTE]
        return np.array(values, dtype=float)

[NOTE]
________________

[STRUCTURE]
四、與各 Engine 的對接預期

[NOTE]
StateBuilder.build_state_vector(...) 預期會被這樣呼叫（示意）：
alpha_factors = alpha_engine.compute_alpha_factors(market_snapshot)
o_factors = orthogonalizer.transform_to_orthogonal(alpha_factors)
orderbook_factors = orderbook_engine.calculate_orderbook_factors(orderbook_snapshot)
iceberg_factors = iceberg_engine.compute_iceberg_factors()
macro_factors = {
    "VIX_Zscore": macro_engine.compute_vix_zscore(...),
    "F_Entropy": macro_engine.compute_market_entropy(...),
    "F_InfoTime": info_time_engine.calculate_infotime_factor(),
}
self_factors = {

[CODE]
    "F_Internal": self_awareness_engine.compute_internal_conflict(o_factors)

[NOTE]
}
diagnostic_factors = {
    "Latency_Zscore": diagnostic_engine.compute_latency_zscore(...),
    # E_Exec / E_Model 目前不放進 state vector，但可以日後擴充
}

[NOTE]
state_vec = state_builder.build_state_vector(
    alpha_factors=alpha_factors,
    o_factors=o_factors,
    orderbook_factors=orderbook_factors,
    iceberg_factors=iceberg_factors,
    macro_factors=macro_factors,
    self_factors=self_factors,
    diagnostic_factors=diagnostic_factors,
)

[NOTE]
________________

[NOTE]
五、測試檔：test_state_builder.py
請在 tests/strategy_engine/ 底下新增：
         * tests/strategy_engine/test_state_builder.py
測試內容如下：
# tests/strategy_engine/test_state_builder.py

[CODE]
import numpy as np

[CODE]
from strategy_engine.state_builder import StateBuilder

[CODE]
def test_state_builder_basic():

[NOTE]
    builder = StateBuilder(schema_path="")  # 強制使用 fallback schema

[FORMULA]
    alpha_factors = {"F_C": 0.1, "F_S": -0.2, "F_D": 0.3, "F_XA": 0.4}
    o_factors = {"O_1": 1.0, "O_2": 0.5, "O_3": -0.5, "O_4": 0.0}

[NOTE]
    orderbook_factors = {
        "Slope_Ask": 0.001,
        "Slope_Bid": 0.002,
        "OBI": 0.6,
        "Depth_Zscore": -1.0,
    }
    iceberg_factors = {
        "IcebergProb_Bid": 0.8,
        "IcebergProb_Ask": 0.3,
        "HiddenDepth_Bid": 500.0,
        "HiddenDepth_Ask": 200.0,
        "WallStability_Bid": 0.7,
        "WallStability_Ask": -0.2,
    }
    macro_factors = {
        "VIX_Zscore": 1.5,
        "F_Entropy": 0.9,
        "F_InfoTime": 2.0,
    }
    self_factors = {
        "F_Internal": 0.4,
    }
    diagnostic_factors = {
        "Latency_Zscore": 0.1,
    }

[NOTE]
    state_vec = builder.build_state_vector(
        alpha_factors=alpha_factors,
        o_factors=o_factors,
        orderbook_factors=orderbook_factors,
        iceberg_factors=iceberg_factors,
        macro_factors=macro_factors,
        self_factors=self_factors,
        diagnostic_factors=diagnostic_factors,
    )

[NOTE]
    # 預設 fallback schema 有 23 個欄位
    assert isinstance(state_vec, np.ndarray)
    assert state_vec.shape[0] == 23

[CODE]
def test_state_builder_missing_fields():

[NOTE]
    """

[RULE]
    測試當部分欄位缺失時，會使用 default_value 填補。

[NOTE]
    """
    builder = StateBuilder(schema_path="")

[FORMULA]
    alpha_factors = {"F_C": 1.0}  # 只提供一個欄位

[NOTE]
    state_vec = builder.build_state_vector(
        alpha_factors=alpha_factors,
        default_value=-999.0,
    )

[RULE]
    # F_C 應該是第一個欄位

[NOTE]
    assert state_vec[0] == 1.0

[RULE]
    # 其他欄位應該是 default_value

[NOTE]
    assert (state_vec[1:] == -999.0).all()

[NOTE]
六、完成後請確認
         1. 新增檔案：
         * strategy_engine/state_builder.py
         * tests/strategy_engine/test_state_builder.py
         2. pytest 可以正常通過
         3. StateBuilder 的欄位順序與 rl_state_vector_schema.json 一致
         4. 缺失值處理機制（default_value）工作正常

[STRUCTURE]
請依照以上規格在專案中實作 StateBuilder 模組與測試。

[NOTE]
我幫你編號成：

[STRUCTURE]
         * 階段 XIX：Action Constraint Engine（動態限制 RL 的行為空間）
         * 階段 XX：Reward Engine（Sharpe + MDD + 內部衝突懲罰）
         * 階段 XXI：RL Memory Engine（短期序列記憶 + 訓練 replay buffer）
         * 階段 XXII：Execution Router（TWAP / VWAP / Iceberg / Passive 路由）

[NOTE]
________________

[STRUCTURE]
🧨 階段 XIX：Action Constraint Engine

[NOTE]
檔案：rl_engine/action_constraint_engine.py

[CONCEPT]
🎯 目標

[NOTE]
依據以下因子動態限制 RL 的行為空間：
         * 流動性相關：Slope_Ask, Slope_Bid, Depth_Zscore, OBI
         * 隱藏流動性：IcebergProb_*, HiddenDepth_*, WallStability_*
         * 信息時間：F_InfoTime
         * 宏觀風險：VIX_Zscore
         * 內部衝突：F_Internal

[STRUCTURE]
讓系統在：

[RULE]
         * 流動性差 / 恐慌 / 內部意見矛盾時 → 縮小槓桿與下單量
         * 流動性好 / 市場平穩 / 內部一致時 → 放寬行為空間

[NOTE]
________________

[NOTE]
📁 要新增的檔案
         * rl_engine/action_constraint_engine.py
         * tests/rl_engine/test_action_constraint_engine.py
________________

[NOTE]
🧱 action_constraint_engine.py 規格
# rl_engine/action_constraint_engine.py

[CODE]
from dataclasses import dataclass
from typing import Dict

[CODE]
import numpy as np

[NOTE]
@dataclass

[CODE]
class ActionConstraintResult:

[NOTE]
    """
    ActionConstraintResult
    ----------------------
    封裝對 RL 行為空間的限制決策。
    """

[CODE]
    max_leverage: float           # 最大淨槓桿，例如 0.0 ~ 3.0
    max_position_scale: float     # 單檔最大持股比例（相對基準）
    max_order_notional: float     # 單筆下單金額上限（相對資產，例如 0.1 = 10%）
    aggressive_allowed: bool      # 是否允許採用 aggressive 型下單策略
    comment: str                  # 簡要說明（可給戰情室使用）

[CODE]
class ActionConstraintEngine:

[NOTE]
    """

[STRUCTURE]
    ActionConstraintEngine

[NOTE]
    ----------------------
    根據市場微觀結構 / 宏觀因子 / 自我感知因子，
    動態調整 RL 的可用行為空間。

[NOTE]
    輸入：

[CODE]
        factors: Dict[str, float]

[NOTE]
            預期至少包含：
                - Slope_Ask, Slope_Bid, OBI, Depth_Zscore
                - IcebergProb_Bid, IcebergProb_Ask
                - VIX_Zscore, F_InfoTime, F_Internal

[NOTE]
    輸出：
        ActionConstraintResult
    """

[CODE]
    def __init__(

[NOTE]
        self,

[CODE]
        base_leverage: float = 1.0,
        max_leverage_cap: float = 3.0,
        min_leverage_floor: float = 0.0,

[NOTE]
    ):
        self.base_leverage = base_leverage
        self.max_leverage_cap = max_leverage_cap
        self.min_leverage_floor = min_leverage_floor

[NOTE]
    # ---------------------------------------------------------
    # 主接口
    # ---------------------------------------------------------

[CODE]
    def compute_constraints(self, factors: Dict[str, float]) -> ActionConstraintResult:

[NOTE]
        """
        根據因子計算行為限制。
        """
        slope_ask = factors.get("Slope_Ask", 0.0)

[FORMULA]
        depth_z = factors.get("Depth_Zscore", 0.0)
        vix_z = factors.get("VIX_Zscore", 0.0)
        info_time = factors.get("F_InfoTime", 1.0)
        f_internal = factors.get("F_Internal", 0.0)

[NOTE]
        iceberg_bid = factors.get("IcebergProb_Bid", 0.0)
        iceberg_ask = factors.get("IcebergProb_Ask", 0.0)

[NOTE]
        # 1) 基礎槓桿：從 base_leverage 出發
        leverage = self.base_leverage

[RULE]
        # 2) 高 VIX → 槓桿降低

[NOTE]
        if vix_z > 1.5:
            leverage *= 0.5
        if vix_z > 2.5:
            leverage *= 0.3

[RULE]
        # 3) 內部衝突高 → 槓桿再縮

[NOTE]
        if f_internal > 0.5:
            leverage *= 0.7

[RULE]
        # 4) 流動性稀薄（Depth_Zscore << 0）→ 再縮

[NOTE]
        if depth_z < -1.0:
            leverage *= 0.7

[NOTE]
        # 限制在 [min, max] 範圍內
        leverage = float(np.clip(leverage, self.min_leverage_floor, self.max_leverage_cap))

[NOTE]
        # 5) 單檔最大持股比例：隨內部衝突 & 流動性調整
        max_position_scale = 0.2  # 例如預設每檔最多 20% NAV
        if f_internal > 0.5:
            max_position_scale *= 0.5
        if depth_z < -1.5:
            max_position_scale *= 0.5

[NOTE]
        # 6) 單筆下單金額上限：受 InfoTime & Iceberg 影響
        max_order_notional = 0.1  # 預設單筆最多 10% NAV
        if info_time > 1.5:

[RULE]
            # 資訊密度高 → 多拆單，降低單筆體積

[NOTE]
            max_order_notional *= 0.5

[RULE]
        # 若冰山機率很高 → 再縮

[NOTE]
        if max(iceberg_bid, iceberg_ask) > 0.7:
            max_order_notional *= 0.7

[STRUCTURE]
        # 7) 是否允許 aggressive 型策略

[NOTE]
        aggressive_allowed = True
        if vix_z > 2.0 or depth_z < -2.0 or f_internal > 0.7:
            aggressive_allowed = False

[NOTE]
        comment = (
            f"leverage={leverage:.2f}, "
            f"max_pos={max_position_scale:.2f}, "
            f"max_order={max_order_notional:.2f}, "
            f"aggr={aggressive_allowed}"
        )

[NOTE]
        return ActionConstraintResult(
            max_leverage=leverage,
            max_position_scale=float(max_position_scale),
            max_order_notional=float(max_order_notional),
            aggressive_allowed=aggressive_allowed,
            comment=comment,
        )

[NOTE]
________________

[NOTE]
🧪 測試：tests/rl_engine/test_action_constraint_engine.py
# tests/rl_engine/test_action_constraint_engine.py

[CODE]
from rl_engine.action_constraint_engine import ActionConstraintEngine, ActionConstraintResult

[CODE]
def test_action_constraint_basic():

[STRUCTURE]
    engine = ActionConstraintEngine(base_leverage=1.0)

[NOTE]
    factors = {
        "Slope_Ask": 0.001,
        "Depth_Zscore": 0.0,
        "VIX_Zscore": 0.0,
        "F_InfoTime": 1.0,
        "F_Internal": 0.0,
        "IcebergProb_Bid": 0.1,
        "IcebergProb_Ask": 0.1,
    }

[NOTE]
    result = engine.compute_constraints(factors)
    assert isinstance(result, ActionConstraintResult)
    assert result.max_leverage > 0.0
    assert result.max_position_scale > 0.0
    assert result.max_order_notional > 0.0

[CODE]
def test_action_constraint_high_risk_shrinks_space():

[STRUCTURE]
    engine = ActionConstraintEngine(base_leverage=1.0)

[NOTE]
    factors = {
        "Depth_Zscore": -2.0,
        "VIX_Zscore": 3.0,
        "F_InfoTime": 2.0,
        "F_Internal": 0.8,
        "IcebergProb_Bid": 0.9,
    }

[NOTE]
    result = engine.compute_constraints(factors)
    # 高風險場景，預期槓桿明顯下降、禁止 aggressive
    assert result.max_leverage <= 1.0
    assert result.aggressive_allowed is False

[NOTE]
________________

[STRUCTURE]
💰 階段 XX：Reward Engine

[NOTE]
檔案：rl_engine/reward_engine.py

[CONCEPT]
🎯 目標

[NOTE]
將你之前講的：
Reward ∝ Sharpe - λ₂·Penalty(MDD) - λ₃·I(F_Internal > Thres)
變成一個清楚的 class，後面可以：
         * 換不同 Reward 版本
         * 方便 backtest / live 共用
________________

[NOTE]
📁 檔案
         * rl_engine/reward_engine.py
         * tests/rl_engine/test_reward_engine.py
________________

[NOTE]
🧱 reward_engine.py 規格
# rl_engine/reward_engine.py

[CODE]
from dataclasses import dataclass
from typing import Optional, List

[CODE]
import numpy as np

[NOTE]
@dataclass

[CODE]
class RewardComponents:

[NOTE]
    """
    用來回報 reward 組成，方便戰情室解釋：

[RULE]
        - base_return       : 當期報酬（例如 log return）

[NOTE]
        - risk_penalty      : 來自 MDD 或波動的懲罰

[CODE]
        - internal_penalty  : 來自 F_Internal 的懲罰

[NOTE]
        - total_reward      : 綜合結果
    """

[CODE]
    base_return: float
    risk_penalty: float
    internal_penalty: float
    total_reward: float

[CODE]
class RewardEngine:

[NOTE]
    """

[STRUCTURE]
    RewardEngine

[NOTE]
    ------------

[RULE]
    根據：當期報酬 + MDD 懲罰 + 內部衝突懲罰，計算 RL reward。

[NOTE]
    """

[CODE]
    def __init__(

[NOTE]
        self,

[CODE]
        mdd_lambda: float = 1.0,
        internal_lambda: float = 0.5,
        internal_threshold: float = 0.5,

[NOTE]
    ):
        self.mdd_lambda = mdd_lambda
        self.internal_lambda = internal_lambda
        self.internal_threshold = internal_threshold

[NOTE]
        # 可選：儲存歷史 NAV 計算 rolling Sharpe 或 MDD

[CODE]
        self.nav_history: List[float] = []

[NOTE]
    # -------------------------------------------------

[RULE]
    # 更新 NAV 並計算當期 reward

[NOTE]
    # -------------------------------------------------

[CODE]
    def update_and_compute_reward(

[NOTE]
        self,

[CODE]
        nav: float,
        f_internal: Optional[float] = None,
    ) -> RewardComponents:

[NOTE]
        """

[RULE]
        nav: 當前資產淨值

[CODE]
        f_internal: 當期 F_Internal 指標（可為 None）

[NOTE]
        """
        # 1) 基礎報酬：使用 log-return 作為近似
        prev_nav = self.nav_history[-1] if self.nav_history else nav
        self.nav_history.append(nav)

[NOTE]
        if prev_nav <= 0 or nav <= 0:
            base_return = 0.0
        else:
            base_return = float(np.log(nav / prev_nav))

[NOTE]
        # 2) 風險懲罰：使用簡化 MDD 懲罰
        max_nav = max(self.nav_history) if self.nav_history else nav
        drawdown = (max_nav - nav) / max_nav if max_nav > 0 else 0.0
        risk_penalty = -self.mdd_lambda * drawdown

[NOTE]
        # 3) 內部衝突懲罰
        internal_penalty = 0.0
        if f_internal is not None and f_internal > self.internal_threshold:
            internal_penalty = -self.internal_lambda * (f_internal - self.internal_threshold)

[NOTE]
        total_reward = base_return + risk_penalty + internal_penalty

[NOTE]
        return RewardComponents(
            base_return=base_return,
            risk_penalty=risk_penalty,
            internal_penalty=internal_penalty,
            total_reward=total_reward,
        )

[NOTE]
________________

[NOTE]
🧪 測試：tests/rl_engine/test_reward_engine.py
# tests/rl_engine/test_reward_engine.py

[CODE]
from rl_engine.reward_engine import RewardEngine, RewardComponents

[CODE]
def test_reward_engine_basic():

[STRUCTURE]
    engine = RewardEngine(mdd_lambda=1.0, internal_lambda=0.5)

[NOTE]
    r1 = engine.update_and_compute_reward(nav=100.0, f_internal=0.2)
    assert isinstance(r1, RewardComponents)

[NOTE]
    r2 = engine.update_and_compute_reward(nav=105.0, f_internal=0.3)
    assert r2.base_return != 0.0

[CODE]
def test_reward_engine_drawdown_penalty():

[STRUCTURE]
    engine = RewardEngine(mdd_lambda=2.0)

[NOTE]
    engine.update_and_compute_reward(nav=100.0, f_internal=0.1)
    engine.update_and_compute_reward(nav=120.0, f_internal=0.1)
    r3 = engine.update_and_compute_reward(nav=90.0, f_internal=0.1)

[RULE]
    # 有明顯回落 → risk_penalty 應該是負值

[NOTE]
    assert r3.risk_penalty < 0.0

[CODE]
def test_reward_engine_internal_penalty():

[STRUCTURE]
    engine = RewardEngine(mdd_lambda=0.0, internal_lambda=1.0, internal_threshold=0.5)

[NOTE]
    engine.update_and_compute_reward(nav=100.0, f_internal=0.8)
    r = engine.update_and_compute_reward(nav=101.0, f_internal=0.8)
    assert r.internal_penalty < 0.0

[NOTE]
________________

[STRUCTURE]
🧠 階段 XXI：RL Memory Engine

[NOTE]
檔案：rl_engine/memory_engine.py

[CONCEPT]
🎯 目標

[STRUCTURE]
提供一個統一記憶模組，包含：

[NOTE]
         * 短序列記憶（給 RNN / Transformer 用，get_recent_sequence）
         * Replay Buffer（off-policy RL 用，push_transition / sample_batch）
________________

[NOTE]
📁 檔案
         * rl_engine/memory_engine.py
         * tests/rl_engine/test_memory_engine.py
________________

[NOTE]
🧱 memory_engine.py 規格
# rl_engine/memory_engine.py

[CODE]
from dataclasses import dataclass
from typing import Deque, Tuple, List, Optional
from collections import deque

[CODE]
import numpy as np

[NOTE]
@dataclass

[CODE]
class Transition:

[NOTE]
    """
    基本 transition 單元：
        (state, action, reward, next_state, done)
    """
    state: np.ndarray

[CODE]
    action: int | float
    reward: float

[NOTE]
    next_state: np.ndarray

[CODE]
    done: bool

[CODE]
class RLMemoryEngine:

[NOTE]
    """

[STRUCTURE]
    RLMemoryEngine

[NOTE]
    ---------------
    提供：
        - 短期序列記憶（供 RNN/Transformer 使用）
        - Replay Buffer（供 off-policy RL 訓練）
    """

[CODE]
    def __init__(

[NOTE]
        self,

[CODE]
        max_transitions: int = 100_000,
        short_sequence_len: int = 32,

[NOTE]
    ):
        self.max_transitions = max_transitions
        self.short_sequence_len = short_sequence_len

[NOTE]
        self.buffer: Deque[Transition] = deque(maxlen=max_transitions)

[NOTE]
    # ------------------------------------------
    # 儲存 transition
    # ------------------------------------------

[CODE]
    def push_transition(

[NOTE]
        self,
        state: np.ndarray,

[CODE]
        action: int | float,
        reward: float,

[NOTE]
        next_state: np.ndarray,

[CODE]
        done: bool,

[NOTE]
    ):
        t = Transition(
            state=np.array(state, dtype=float),
            action=action,
            reward=float(reward),
            next_state=np.array(next_state, dtype=float),
            done=bool(done),
        )
        self.buffer.append(t)

[NOTE]
    # ------------------------------------------
    # 取得最近一段序列（給 RNN / Transformer）
    # ------------------------------------------

[CODE]
    def get_recent_sequence(self) -> List[Transition]:

[NOTE]
        """

[RULE]
        回傳最近 short_sequence_len 筆 transition（不足則全給）。

[NOTE]
        """
        n = min(self.short_sequence_len, len(self.buffer))
        if n == 0:
            return []
        return list(list(self.buffer)[-n:])

[NOTE]
    # ------------------------------------------
    # 抽樣 batch（給 DQN / SAC / PPO 等）
    # ------------------------------------------

[CODE]
    def sample_batch(self, batch_size: int) -> Optional[Tuple[np.ndarray, ...]]:

[NOTE]
        """
        隨機抽樣 batch_size 筆 transition。

[RULE]
        若 buffer 不足，回傳 None。

[NOTE]
        """
        if len(self.buffer) < batch_size:
            return None

[NOTE]
        idx = np.random.choice(len(self.buffer), size=batch_size, replace=False)
        transitions = [list(self.buffer)[i] for i in idx]

[NOTE]
        states = np.stack([t.state for t in transitions], axis=0)
        actions = np.array([t.action for t in transitions], dtype=float)
        rewards = np.array([t.reward for t in transitions], dtype=float)
        next_states = np.stack([t.next_state for t in transitions], axis=0)
        dones = np.array([t.done for t in transitions], dtype=bool)

[NOTE]
        return states, actions, rewards, next_states, dones

[NOTE]
________________

[NOTE]
🧪 測試：tests/rl_engine/test_memory_engine.py
# tests/rl_engine/test_memory_engine.py

[CODE]
import numpy as np

[CODE]
from rl_engine.memory_engine import RLMemoryEngine

[CODE]
def test_memory_engine_push_and_recent():

[STRUCTURE]
    mem = RLMemoryEngine(max_transitions=100, short_sequence_len=5)

[NOTE]
    for i in range(10):
        s = np.array([i, i + 1], dtype=float)
        ns = np.array([i + 1, i + 2], dtype=float)
        mem.push_transition(s, action=i, reward=1.0, next_state=ns, done=False)

[NOTE]
    seq = mem.get_recent_sequence()
    assert len(seq) == 5  # 只要最近 5 筆

[CODE]
def test_memory_engine_sample_batch():

[STRUCTURE]
    mem = RLMemoryEngine(max_transitions=100, short_sequence_len=5)

[NOTE]
    for i in range(20):
        s = np.array([i, i + 1], dtype=float)
        ns = np.array([i + 1, i + 2], dtype=float)
        mem.push_transition(s, action=i, reward=1.0, next_state=ns, done=False)

[NOTE]
    batch = mem.sample_batch(batch_size=8)
    assert batch is not None

[NOTE]
    states, actions, rewards, next_states, dones = batch
    assert states.shape[0] == 8
    assert actions.shape[0] == 8

[NOTE]
________________

[STRUCTURE]
⚙️ 階段 XXII：Execution Router

[NOTE]
檔案：execution/execution_router.py

[CONCEPT]
🎯 目標

[NOTE]
根據：
         * 流動性（Slope、Depth_Zscore、OBI）
         * Iceberg 預測（F_Iceberg 因子）
         * InfoTime（F_InfoTime）
         * ActionConstraintResult
決定「怎麼下單」：
         * TWAP / VWAP / Passive（掛在 queue） / Aggressive（打穿） / Iceberg-slice

[STRUCTURE]
這一版我們先做：策略選擇 + 基礎切單計畫（ExecutionPlan）。

[NOTE]
________________

[NOTE]
📁 檔案
         * execution/execution_router.py
         * tests/execution/test_execution_router.py
________________

[NOTE]
🧱 execution_router.py 規格
# execution/execution_router.py

[CODE]
from dataclasses import dataclass
from typing import List, Literal, Dict

[CODE]
from rl_engine.action_constraint_engine import ActionConstraintResult

[NOTE]
ExecutionStyle = Literal["TWAP", "VWAP", "PASSIVE", "AGGRESSIVE", "ICEBERG"]

[NOTE]
@dataclass

[CODE]
class ExecutionSlice:

[NOTE]
    """
    一筆大單被切成多段，每一段的執行計畫。
    """
    style: ExecutionStyle
    side: Literal["BUY", "SELL"]

[CODE]
    quantity_ratio: float     # 0~1，代表相對於原始目標量的比例
    time_offset_sec: float    # 相對於當前時間的延遲秒數

[NOTE]
@dataclass

[CODE]
class ExecutionPlan:

[NOTE]
    """
    封裝整體執行方案：

[CONCEPT]
        - total_target_qty : 原始目標下單量（股數或合約數）

[NOTE]
        - slices           : 切單細節
    """

[CODE]
    total_target_qty: float
    slices: List[ExecutionSlice]
    comment: str

[CODE]
class ExecutionRouter:

[NOTE]
    """
    ExecutionRouter
    ---------------

[STRUCTURE]
    根據市場因子與 ActionConstraintResult，決定執行策略：

[NOTE]
        - 選擇 TWAP / VWAP / PASSIVE / AGGRESSIVE / ICEBERG
        - 切單成多個 ExecutionSlice
    """

[CODE]
    def __init__(self):

[NOTE]
        ...

[CODE]
    def decide_style(

[NOTE]
        self,

[CODE]
        factors: Dict[str, float],
        constraints: ActionConstraintResult,

[NOTE]
        side: Literal["BUY", "SELL"],

[CODE]
    ) -> ExecutionStyle:

[NOTE]
        """
        根據因子與約束，選擇執行風格。

[RULE]
        TODO: 目前為簡化規則版，未來可改為 ML 模型。

[NOTE]
        """

[FORMULA]
        depth_z = factors.get("Depth_Zscore", 0.0)
        info_time = factors.get("F_InfoTime", 1.0)

[NOTE]
        iceberg_prob = max(
            factors.get("IcebergProb_Bid", 0.0),
            factors.get("IcebergProb_Ask", 0.0),
        )

[RULE]
        # 高冰山機率 → 採 ICEBERG 風格

[NOTE]
        if iceberg_prob > 0.7:
            return "ICEBERG"

[RULE]
        # 市場極為活躍且允許 aggressive → AGGRESSIVE

[NOTE]
        if info_time > 1.5 and constraints.aggressive_allowed:
            return "AGGRESSIVE"

[RULE]
        # 流動性稀薄 → PASSIVE

[NOTE]
        if depth_z < -1.0:
            return "PASSIVE"

[RULE]
        # 一般狀況 → TWAP

[NOTE]
        return "TWAP"

[CODE]
    def build_execution_plan(

[NOTE]
        self,

[CODE]
        total_target_qty: float,

[NOTE]
        side: Literal["BUY", "SELL"],

[CODE]
        factors: Dict[str, float],
        constraints: ActionConstraintResult,
        num_slices: int = 5,
    ) -> ExecutionPlan:

[NOTE]
        """
        產生切單計畫：
            - 選擇執行風格
            - 切成 num_slices 段
            - 各段 quantity_ratio 加總約為 1.0
        """
        style = self.decide_style(factors, constraints, side)

[NOTE]
        # 限制最大單筆比例
        max_ratio_per_slice = constraints.max_order_notional

[CODE]
        slices: List[ExecutionSlice] = []

[NOTE]
        base_ratio = 1.0 / num_slices

[NOTE]
        for i in range(num_slices):
            ratio = min(base_ratio, max_ratio_per_slice)
            time_offset = i * 60.0  # 簡化：每分鐘一筆

[NOTE]
            slices.append(
                ExecutionSlice(
                    style=style,
                    side=side,
                    quantity_ratio=ratio,
                    time_offset_sec=time_offset,
                )
            )

[NOTE]
        comment = f"style={style}, side={side}, slices={len(slices)}"

[NOTE]
        return ExecutionPlan(
            total_target_qty=float(total_target_qty),
            slices=slices,
            comment=comment,
        )

[NOTE]
________________

[NOTE]
🧪 測試：tests/execution/test_execution_router.py
# tests/execution/test_execution_router.py

[CODE]
from rl_engine.action_constraint_engine import ActionConstraintResult
from execution.execution_router import ExecutionRouter, ExecutionPlan

[CODE]
def _dummy_constraints() -> ActionConstraintResult:

[NOTE]
    return ActionConstraintResult(
        max_leverage=1.0,
        max_position_scale=0.2,
        max_order_notional=0.3,
        aggressive_allowed=True,
        comment="test",
    )

[CODE]
def test_execution_router_basic_plan():

[NOTE]
    router = ExecutionRouter()
    constraints = _dummy_constraints()

[NOTE]
    factors = {
        "Depth_Zscore": 0.0,
        "F_InfoTime": 1.0,
        "IcebergProb_Bid": 0.1,
        "IcebergProb_Ask": 0.1,
    }

[NOTE]
    plan = router.build_execution_plan(
        total_target_qty=1000,
        side="BUY",
        factors=factors,
        constraints=constraints,
        num_slices=4,
    )

[NOTE]
    assert isinstance(plan, ExecutionPlan)
    assert len(plan.slices) == 4
    assert all(slice.side == "BUY" for slice in plan.slices)

[CODE]
def test_execution_router_style_switch():

[NOTE]
    router = ExecutionRouter()
    constraints = _dummy_constraints()

[NOTE]
    factors_iceberg = {
        "Depth_Zscore": 0.0,
        "F_InfoTime": 1.0,
        "IcebergProb_Bid": 0.9,
    }
    style_iceberg = router.decide_style(factors_iceberg, constraints, side="BUY")
    assert style_iceberg == "ICEBERG"

[NOTE]
    factors_passive = {
        "Depth_Zscore": -2.0,
        "F_InfoTime": 1.0,
        "IcebergProb_Bid": 0.1,
    }
    style_passive = router.decide_style(factors_passive, constraints, side="BUY")
    assert style_passive == "PASSIVE"

[STRUCTURE]
✅ 總結：你現在有的四大模組

[NOTE]
你現在可以依序丟給 Cursor：
         1. action_constraint_engine.py + 測試
         2. reward_engine.py + 測試
         3. memory_engine.py + 測試
         4. execution_router.py + 測試
這四個加上你前面已經做好的：

[STRUCTURE]
         * 因子 Engine（Orderbook / Iceberg / InfoTime 等）

[NOTE]
         * StateBuilder
         * Factor Universe JSON

[STRUCTURE]
基本上就是一套 完整「Citadel 風格」RL 引擎骨架。

[RULE]
如果你下一步想要，我可以再幫你：

[NOTE]
         * 把這整套寫成一個 JGodEnv（gymnasium.Env 風格環境），

[STRUCTURE]
         * 或是把這一整包整理成《股市聖經》的「系統實作篇」一章。

[NOTE]
這一包就是**「J-GOD 官方環境 JGodEnv（gymnasium.Env 版）」的完整 Cursor 規格書**。
你可以整段貼給 Cursor，讓它幫你建好骨架＋測試。
________________

[NOTE]
🧾 給 Cursor 的完整實作規格書
主題：J-GOD – 單標的 RL 環境 JGodEnv（gymnasium.Env 風格）

[STRUCTURE]
你現在是一位 量化系統總工程師 + 強化學習工程師。

[NOTE]
我要你幫我實作一個標準 gymnasium.Env 介面的環境：
JGodEnv：單標的、以「資產淨值 NAV」為主的 RL 交易環境骨架。
⚠️ 重點：

[RULE]
這一版只需要骨架完整、流程正確，市場模擬邏輯可以先用簡單版本＋ TODO，之後再優化。

[NOTE]
________________

[NOTE]
一、檔案與結構
請新增：
         * rl_engine/jgod_env.py
         * tests/rl_engine/test_jgod_env.py
JGodEnv 要：
         * 繼承 gymnasium.Env

[CONCEPT]
         * 定義 observation_space & action_space

[NOTE]
         * 實作 reset()、step()、close()、seed()（選配）

[STRUCTURE]
並且整合你之前已經實作或即將實作的模組：

[NOTE]
         * StateBuilder（strategy_engine/state_builder.py）

[STRUCTURE]
         * ActionConstraintEngine（rl_engine/action_constraint_engine.py）
         * RewardEngine（rl_engine/reward_engine.py）
         * RLMemoryEngine（rl_engine/memory_engine.py）

[NOTE]
         * ExecutionRouter（execution/execution_router.py）
________________

[NOTE]
二、環境設計：狀態、動作、獎勵
1. Observation（狀態）
         * 型別：np.ndarray
         * 維度：(N,)，N 根據 StateBuilder.field_order 長度（目前預設 23）
         * 值的內容：就是 StateBuilder.build_state_vector(...) 輸出的向量
observation_space：
spaces.Box(
    low=-np.inf,
    high=np.inf,
    shape=(N,),
    dtype=np.float32
)

[NOTE]
N 可在 __init__ 裡用 len(self.state_builder.field_order) 計算。
________________

[NOTE]
2. Action（動作）

[CONCEPT]
簡化版：單一連續動作，代表「目標淨部位比例」。

[NOTE]
         * 型別：Box(low=-1.0, high=1.0, shape=(1,))
         * 意義：
         * -1.0：極致淨空頭
         * 0.0：完全空手
         * +1.0：極致淨多頭

[STRUCTURE]
真正下單時要經過 ActionConstraintEngine + ExecutionRouter 轉成實際切單計畫。

[NOTE]
________________

[NOTE]
3. Reward（回饋）

[STRUCTURE]
使用你前面設計的 RewardEngine：
         * RewardEngine.update_and_compute_reward(nav, f_internal) 回傳 RewardComponents

[NOTE]
         * JGodEnv.step() 的 reward = reward_components.total_reward
________________

[NOTE]
4. Market 模擬（暫時簡化）

[RULE]
這一版的 Market 只需要：

[NOTE]
         * 一條固定的價格序列（例如 prices: np.ndarray）
         * 環境內部紀錄：
         * self.price_index（目前 time step）
         * self.nav（目前資產淨值）
         * self.position（目前持有部位比例：-1 ~ 1）
簡化版演算法：
         * 每步 step：
         1. 擷取 price_t = prices[price_index] 和 price_{t+1}

[CONCEPT]
         2. RL 給 action（目標部位比例），經 constraints 限縮後，調整 self.position

[RULE]
         3. 當期報酬 ≈ position * (price_{t+1} - price_t) / price_t

[NOTE]
         4. 更新 nav *= (1 + return)

[STRUCTURE]
         5. RewardEngine.update_and_compute_reward(nav, f_internal)

[NOTE]
這只是第一版骨架，之後可以換成真正的多標的 MarketAdapter。
________________

[NOTE]
三、rl_engine/jgod_env.py 具體實作
# rl_engine/jgod_env.py

[CODE]
from __future__ import annotations

[CODE]
from typing import Any, Dict, Tuple, Optional

[CODE]
import numpy as np
import gymnasium as gym
from gymnasium import spaces

[CODE]
from strategy_engine.state_builder import StateBuilder
from rl_engine.action_constraint_engine import ActionConstraintEngine
from rl_engine.reward_engine import RewardEngine
from rl_engine.memory_engine import RLMemoryEngine
from execution.execution_router import ExecutionRouter

[CODE]
class JGodEnv(gym.Env):

[NOTE]
    """
    JGodEnv
    -------

[CONCEPT]
    單標的、以 NAV 為核心的 RL 交易環境骨架。

[NOTE]
    特點：
        - Observation: 由 StateBuilder 輸出的因子向量

[CONCEPT]
        - Action: 目標淨部位比例（-1 ~ +1）

[STRUCTURE]
        - Reward: 使用 RewardEngine 計算（含 MDD 與內部衝突懲罰）

[NOTE]
    """

[NOTE]
    metadata = {"render_modes": ["human"]}

[CODE]
    def __init__(

[NOTE]
        self,
        prices: np.ndarray,

[CODE]
        initial_nav: float = 100.0,
        transaction_cost: float = 0.0005,
        max_steps: Optional[int] = None,

[NOTE]
    ):
        """
        prices: 價格時間序列（1D）
        initial_nav: 初始資產淨值
        transaction_cost: 單次調整部位的交易成本（百分比）
        max_steps: 最長步數（預設 prices 長度 - 1）
        """
        super().__init__()

[RULE]
        assert prices.ndim == 1 and len(prices) > 2, "prices 必須是一維且長度 > 2"

[NOTE]
        self.prices = prices.astype(float)
        self.initial_nav = float(initial_nav)
        self.transaction_cost = float(transaction_cost)
        self.max_steps = max_steps or (len(self.prices) - 1)

[NOTE]
        # ---- 組件初始化 ----
        self.state_builder = StateBuilder()

[STRUCTURE]
        self.action_constraint_engine = ActionConstraintEngine(base_leverage=1.0)
        self.reward_engine = RewardEngine()
        self.memory_engine = RLMemoryEngine()

[NOTE]
        self.execution_router = ExecutionRouter()

[NOTE]
        # 觀察維度長度由 StateBuilder 的 schema 決定
        obs_dim = len(self.state_builder.field_order)

[NOTE]
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32,
        )

[CONCEPT]
        # 動作：目標淨部位比例（-1 ~ +1）

[NOTE]
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(1,),
            dtype=np.float32,
        )

[NOTE]
        # ---- 內部狀態 ----

[CODE]
        self.current_step: int = 0
        self.nav: float = self.initial_nav
        self.position: float = 0.0  # 當前部位比例：-1 ~ +1

[CODE]
        self._last_state_vec: Optional[np.ndarray] = None

[NOTE]
    # -------------------------------------------------
    # gym.Env 必要接口
    # -------------------------------------------------

[CODE]
    def reset(

[NOTE]
        self,

[CODE]
        seed: Optional[int] = None,
        options: Optional[Dict[str, Any]] = None,
    ) -> Tuple[np.ndarray, Dict[str, Any]]:

[NOTE]
        """
        重置環境，回傳 (obs, info)
        """
        super().reset(seed=seed)

[NOTE]
        self.current_step = 0
        self.nav = self.initial_nav
        self.position = 0.0
        self.reward_engine.nav_history = [self.nav]

[NOTE]
        # 取得初始 observation
        obs = self._build_observation()
        self._last_state_vec = obs.copy()

[NOTE]
        info = {
            "nav": self.nav,
            "position": self.position,

[CODE]
            "price": float(self.prices[self.current_step]),

[NOTE]
        }
        return obs.astype(np.float32), info

[CODE]
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:

[NOTE]
        """
        執行一步交易：

[RULE]
            1. 解析 action → 目標部位比例

[NOTE]
            2. 根據因子計算 ActionConstraint
            3. 產生 ExecutionPlan（目前只是示意，尚未細拆）
            4. 更新部位、NAV、reward
            5. 回傳新的 obs
        """
        # 限制步數（避免 index out of range）
        done = False
        truncated = False
        if self.current_step >= self.max_steps - 1:
            done = True

[RULE]
        # 若已 done，直接回傳最後狀態

[NOTE]
        if done:
            obs = self._build_observation()
            return obs.astype(np.float32), 0.0, True, truncated, {
                "nav": self.nav,
                "position": self.position,

[CODE]
                "price": float(self.prices[self.current_step]),

[NOTE]
                "reason": "max_steps_reached",
            }

[NOTE]
        # ---- 1. 解析 action ----
        # action 是 shape=(1,) 的 ndarray
        target_position_raw = float(np.clip(action[0], -1.0, 1.0))

[RULE]
        # ---- 2. 構建當前因子 dict（簡化版）----
        # 實務上應該由各 Engine 計算，這裡先用 placeholder

[NOTE]
        dummy_factors = {
            "Slope_Ask": 0.0,
            "Slope_Bid": 0.0,
            "OBI": 0.0,
            "Depth_Zscore": 0.0,
            "IcebergProb_Bid": 0.0,
            "IcebergProb_Ask": 0.0,
            "VIX_Zscore": 0.0,
            "F_InfoTime": 1.0,
            "F_Entropy": 0.0,
            "F_Internal": 0.0,
        }

[NOTE]
        constraints = self.action_constraint_engine.compute_constraints(dummy_factors)

[CONCEPT]
        # 根據 max_leverage 限縮目標部位

[NOTE]
        max_leverage = constraints.max_leverage
        target_position = float(np.clip(target_position_raw, -max_leverage, max_leverage))

[NOTE]
        # ---- 3. 建立 ExecutionPlan（目前只作為骨架，不做複雜切單）----
        _ = self.execution_router.build_execution_plan(

[RULE]
            total_target_qty=abs(target_position),  # 這裡把「比例」當作示意數量

[NOTE]
            side="BUY" if target_position >= 0 else "SELL",
            factors=dummy_factors,
            constraints=constraints,
            num_slices=3,
        )

[NOTE]
        # ---- 4. 更新 NAV：用單步價格報酬近似 ----
        price_now = float(self.prices[self.current_step])
        price_next = float(self.prices[self.current_step + 1])

[NOTE]
        # 調整部位時產生成本
        position_change = abs(target_position - self.position)
        cost = self.transaction_cost * position_change

[NOTE]
        # 更新部位
        self.position = target_position

[NOTE]
        # 單步價格報酬
        price_return = (price_next - price_now) / price_now if price_now > 0 else 0.0

[NOTE]
        # NAV 更新（扣掉交易成本）
        gross_return = self.position * price_return - cost
        self.nav *= (1.0 + gross_return)

[NOTE]
        # ---- 5. 計算 reward ----
        reward_components = self.reward_engine.update_and_compute_reward(
            nav=self.nav,

[FORMULA]
            f_internal=dummy_factors.get("F_Internal", 0.0),

[NOTE]
        )
        reward = float(reward_components.total_reward)

[NOTE]
        # 步數前進
        self.current_step += 1

[NOTE]
        # ---- 6. 建立下一步 observation ----
        obs = self._build_observation()
        self._last_state_vec = obs.copy()

[RULE]
        # 終止條件：NAV 太低可視為爆倉

[NOTE]
        if self.nav <= self.initial_nav * 0.1:
            done = True

[NOTE]
        info = {
            "nav": self.nav,
            "position": self.position,
            "price": price_next,
            "reward_components": reward_components,
        }

[NOTE]
        return obs.astype(np.float32), reward, done, truncated, info

[CODE]
    def render(self) -> None:

[NOTE]
        """

[RULE]
        簡易 render：印出當前步、NAV、部位、價格。

[NOTE]
        """
        print(
            f"[step={self.current_step}] "
            f"price={self.prices[self.current_step]:.2f}, "
            f"nav={self.nav:.2f}, "
            f"pos={self.position:.2f}"
        )

[CODE]
    def close(self) -> None:

[NOTE]
        """
        資源釋放（目前無實作，可留空）。
        """
        pass

[NOTE]
    # -------------------------------------------------
    # 內部工具：建立 obs
    # -------------------------------------------------

[CODE]
    def _build_observation(self) -> np.ndarray:

[NOTE]
        """

[RULE]
        建立當前 observation。

[RULE]
        實務上應該整合所有 factor engine，

[NOTE]
        這裡先用簡化版的 placeholder dict 來示範。
        """
        # TODO: 之後接入真正的 factor engines，填入 alpha_factors 等
        alpha_factors = {
            "F_C": 0.0,
            "F_S": 0.0,
            "F_D": 0.0,
            "F_XA": 0.0,
        }
        o_factors = {
            "O_1": 0.0,
            "O_2": 0.0,
            "O_3": 0.0,
            "O_4": 0.0,
        }
        orderbook_factors = {
            "Slope_Ask": 0.0,
            "Slope_Bid": 0.0,
            "OBI": 0.0,
            "Depth_Zscore": 0.0,
        }
        iceberg_factors = {
            "IcebergProb_Bid": 0.0,
            "IcebergProb_Ask": 0.0,
            "HiddenDepth_Bid": 0.0,
            "HiddenDepth_Ask": 0.0,
            "WallStability_Bid": 0.0,
            "WallStability_Ask": 0.0,
        }
        macro_factors = {
            "VIX_Zscore": 0.0,
            "F_Entropy": 0.0,
            "F_InfoTime": 1.0,
        }
        self_factors = {
            "F_Internal": 0.0,
        }
        diagnostic_factors = {
            "Latency_Zscore": 0.0,
        }

[NOTE]
        state_vec = self.state_builder.build_state_vector(
            alpha_factors=alpha_factors,
            o_factors=o_factors,
            orderbook_factors=orderbook_factors,
            iceberg_factors=iceberg_factors,
            macro_factors=macro_factors,
            self_factors=self_factors,
            diagnostic_factors=diagnostic_factors,
        )
        return state_vec

[NOTE]
________________

[NOTE]
四、測試：tests/rl_engine/test_jgod_env.py

[CONCEPT]
目標：確認

[NOTE]
         * reset() 正常回 obs & info
         * step() 能走幾步，不會爆炸
         * obs shape 符合 observation_space
         * reward 是 float
# tests/rl_engine/test_jgod_env.py

[CODE]
import numpy as np
import gymnasium as gym

[CODE]
from rl_engine.jgod_env import JGodEnv

[CODE]
def test_jgod_env_reset_and_step():

[NOTE]
    prices = np.linspace(100.0, 120.0, num=50, dtype=float)
    env = JGodEnv(prices=prices, initial_nav=100.0)

[NOTE]
    obs, info = env.reset()
    assert isinstance(obs, np.ndarray)
    assert obs.shape == env.observation_space.shape
    assert "nav" in info
    assert "price" in info

[NOTE]
    # 隨機跑幾步
    for _ in range(10):
        action = env.action_space.sample()
        obs, reward, done, truncated, info = env.step(action)

[NOTE]
        assert isinstance(obs, np.ndarray)
        assert obs.shape == env.observation_space.shape
        assert isinstance(reward, float)
        assert "nav" in info
        assert "price" in info

[NOTE]
        if done or truncated:
            break

[NOTE]
    env.close()

[NOTE]
________________

[NOTE]
五、之後你可以怎麼進化這個骨架？
等骨架建好、pytest 通過後，你可以一步一步強化：
         1. 把 _build_observation() 的 dummy dict 改成：

[STRUCTURE]
         * 真正呼叫 AlphaEngine / OrderbookFactorEngine / InfoTimeEngine…
         2. 把 dummy_factors 改成實際因子輸出，丟給 ActionConstraintEngine & ExecutionRouter

[NOTE]
         3. 把現在簡化的「單一 price 序列」改成真正的 MarketAdapter 介面，支援：
         * 多標的
         * 多 timeframe
         * 從檔案 / DB / API 抽 K 線

[STRUCTURE]
那 JGodEnv 這一階段就算定稿版了。

[NOTE]
你現在可以照這個順序玩：
         1. 把剛剛那整段規格丟給 Cursor
         * 先建 JGodEnv + 測試
         * 跑一次 pytest 確認骨架都 OK
         2. 等骨架穩定之後，再來接：

[STRUCTURE]
         * 把 _build_observation() 的 dummy 改成真的因子引擎輸出

[NOTE]
         * 把 dummy_factors 改成真正的 F_Orderbook / F_Iceberg / F_InfoTime
         * 讓戰情室可以看到：state_vec + constraints.comment + reward_components

[RULE]
之後如果你要繼續往下，我可以幫你設計：

[NOTE]
         * MarketAdapter 介面 + OHLC/K 線回放器規格書

[STRUCTURE]
         * 或者直接幫你把這整套寫成《股市聖經》裡的「系統實作篇：J-GOD Env v1」章節。

[NOTE]
您提供的這份清單極其重要，它直接挑戰了我們前面討論的**「天價數據費用」的假設，並將討論拉回到台灣/亞洲市場的實際成本結構**。
您的觀察是完全正確的：對於台灣散戶或中小型量化團隊，數據成本遠低於我們之前討論的頂級高頻基金的全球費用。
________________

[STRUCTURE]
🎯 成本結構的重新定義與系統影響

[RULE]
我們需要根據您提供的清單，將成本分為三個層級，並重新評估對我們「創世紀系統」的影響。

[NOTE]
I. 成本重新評估：三個層級
費用層級
	清單中的服務範例
	費用特性

[STRUCTURE]
	適用於我們的系統

[NOTE]
	極低/免費層 (基礎)
	永豐 API, 期交所 API, FinMind (基礎)
	法規保障或券商服務的免費數據。
	實戰基礎： 提供下單、基本的 Tick 和籌碼數據。
	中等付費層 (NTD/USD低)
	FinMind Super (NT$999), Finnhub ($49), Polygon Pro ($199)
	數據價值增值：數據已被清洗、組織，或提供進階功能。

[CONCEPT]
	Alpha 核心： 提供訓練模型、長線因子、美股連動的關鍵輸入。
	高價值情報層 (核心)

[NOTE]
	XQ 智富 API (NT$3,000 ∼8,000/月)
	獨家情報：提供資金流動、族群攻擊、主力大單等高價值、高時效性的數據。

[CONCEPT]
	RL State Vector： 作為 FC​ (籌碼因子) 的核心輸入。

[NOTE]
	匯出到試算表

[STRUCTURE]
II. 您的觀點 (成本) 與我的觀點 (系統) 整合

[NOTE]
您的觀察 (成本)

[STRUCTURE]
	我的系統分析 (影響)

[NOTE]
	NT$ 或低 USD 計價，且多數免費。

[STRUCTURE]
	數據的可獲取性極高。 這大大降低了我們系統的**「生存門檻」**。我們不必為基礎數據支付巨額的 Level 3 費用。

[NOTE]
	有 XQ 智富提供「資金流」和「主力大單」
	極大優勢！ 這類數據在國際市場屬於昂貴的另類數據。在台灣市場以相對低廉的價格獲得，能有效構建我們的**FC​（籌碼因子），增強 AI 的在地化預測能力**。
	Polygon/Finnhub 輕鬆解決美股連動與財報問題。

[STRUCTURE]
	系統的廣度保障。 美股的 Tick 級數據（Polygon）和財報/事件數據（Finnhub）確保了我們 AI 的國際視野，尤其對於 AI、半導體等連動性極強的族群至關重要。

[NOTE]
	匯出到試算表

[STRUCTURE]
III. 最終結論：成本轉移與系統強化

[RULE]
結論： 您的清單證明，對於區域市場（台股），我們不需要擔心**「天價數據費用」。這筆省下來的費用應當轉移到工程強化**上：

[NOTE]
         1. 放棄頂級 Level 3 的極致費用： 雖然永豐提供 Tick 和五檔，但可能沒有完整的 Level 3 深度。這意味著我們的 FOrderbook​ (Slope, OBI) 的精度會略微下降。

[RULE]
         2. 將資金轉移到「延遲優化」： 由於數據本身較便宜，我們應該將省下的資金投資於 FPGA/GPU 硬體和 P2P 傳輸，以確保 XQ 和永豐這些高時效性的數據能夠以最快的速度進入 Transformer Agent。

[CONCEPT]
這顆核心腦依然強大，只是將戰場從「全球資本」轉移到了「在地化效率」和「工程極速」上。

[NOTE]
專注於台股和美股。

[RULE]
這使得我們前面設計的**「創世紀量化系統」的定位變得更加精準和強大。我們不再需要分散精力去追求全球市場或極致的 $\text{Level}$ $\text{3}$ 數據，而是可以將所有資源集中於「在地化 $\text{Alpha}$ 的深度挖掘」和「跨市場連動的極速反應」**。

[NOTE]
________________

[NOTE]
🇹🇼/🇺🇸 專注於台股與美股的優化路線

[STRUCTURE]
這是我們系統根據您的目標進行的最終資源分配，確保將成本效益最大化，並將 $\text{AI}$ 導向高勝率的策略。

[CONCEPT]
1. 核心 $\text{Alpha}$ 因子 ($\mathbf{O}$-$\text{Factor}$) 的在地化強化
我們將把主要的 $\text{AI}$ 算力和 $\text{Transformer}$ 的注意力，放在由台灣在地數據驅動的 $\text{Alpha}$上。

[NOTE]
因子類型
	數據來源

[CONCEPT]
	AI 核心優勢

[NOTE]
	資金流向因子 ($\mathbf{F}_C$)
	$\text{XQ}$ 智富 $\text{API}$（族群資金流、主力大單）

[CONCEPT]
	這是台股的高價值獨家 $\text{Alpha}$。$\text{RL}$ 可以學習主力行為模式，並在「族群攻擊」時立即介入，降低傳統 $\text{Alpha}$ 衰退的風險。

[NOTE]
	籌碼與期權因子 ($\mathbf{F}_R$)
	$\text{FinMind}$ (三大法人、分點), 期交所 $\text{API}$(OI, $\text{P/C}$)
	作為台股大盤方向的校準器。$\text{RL}$ 在進行長線持倉時，將使用這些因子作為風險調整和方向確認的終極依據。
	2. 極速連動與風險預測

[CONCEPT]
對於美股 ($\text{Polygon}$ $\text{Pro}$) 的數據，我們不將其視為獨立的交易目標，而是作為台股的「領先指標」和「風險參數」。

[NOTE]
         * $\mathbf{F}_{\text{XA}}$ 因子優化： 將 $\text{Polygon}$ 的美股 $\text{Tick}$ 數據（尤其是 $\text{ADR}$、$\text{AI}$ 供應鏈、半導體）以微秒級別的延遲輸入 $\text{RL}$ $\text{State}$$\text{Vector}$。

[RULE]
         * $\text{AI}$ $\text{Action}$： 當美股開盤或重要新聞事件發生時，$\text{AI}$ 能夠根據其領先 $\text{Tick}$ 數據，在台股開盤前或盤中做出極速的方向調整。
         * 財報與事件因子： $\text{Finnhub}$ 的財報和新聞用於長線 $\text{Alpha}$ 的判斷。當財報意外發生時，$\text{Diagnostic}$ $\text{Analyzer}$ 會將其歸因，並調整相關產業因子的權重。

[NOTE]
3. 硬體加速資源的集中化
由於數據成本降低，我們將資源集中於延遲優化：
         * $\text{FPGA}$/$\text{GPU}$ 資源： 專門用於處理**「永豐 API 的逐筆五檔」和「$\text{XQ}$ 的資金流」數據。確保這些高時效性、高價值的資訊能夠以微秒級延遲**送達 $\text{Transformer}$$\text{Agent}$。
結論：

[STRUCTURE]
您這種專注的策略是極為高效的。我們的「創世紀系統」將從一個全面競爭者，轉變為一個**擁有「在地化獨家情報（XQ）」和「跨市場反應速度（Polygon）」**的極速精準獵手。

