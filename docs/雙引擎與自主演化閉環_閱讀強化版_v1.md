# 雙引擎與自主演化閉環：閱讀強化版 v1

> **說明**：本文件是 J-GOD 股神作戰系統的核心設計文件，包含強化學習 (RL) 模型設計、自主演化閉環、系統架構等。

---

## 一、這本書的目的

[CONCEPT]
name: 本書目的
definition: 設計一個具備自主演化能力的 AI 交易系統，透過強化學習 (RL) 讓系統能夠自主學習和調整參數，實現從數據輸入到自主學習的完整閉環。

[NOTE]
本文件旨在將複雜的情境變數轉化為 RL 代理人可以學習和調整的參數，實現系統的自主優化與演化。

---

## 二、核心投資邏輯

### 2.1 強化學習 (RL) 模型設計

[CONCEPT]
name: RL 核心挑戰
definition: 參數爆炸與高效探索。在傳統 AI 中，如果為每一個情境都設定固定規則，規則會越來越多。RL 的目標是讓 AI 自主學習連續的參數空間，而不是離散的規則。

[STRUCTURE]
RL 模型設計包含五個步驟：
1. 定義 RL 的「狀態」（State）空間
2. 定義 RL 的「行動」（Action）空間（參數化）
3. 定義「獎勵」（Reward）函數
4. 迭代與學習（Model Calibration Engine）
5. 結果審核與人工介入（The Human Filter）

### 2.2 RL 狀態空間

[CONCEPT]
name: RL 狀態空間
definition: RL 代理人需要觀察的數據，包含策略績效、診斷與誤差、市場情境、籌碼健康度等。

[TABLE]
columns: 狀態類別 | 關鍵輸入參數 | 數據來源 | 意義
I. 策略績效 | Sharpe Ratio 7 Day, MDD 30 Day, A/B 策略相關性 | strategy_returns | 判斷當前策略組合是否健康
II. 診斷與誤差 | Residual (殘差), FC 貢獻度, FS 貢獻度 | attribution_results | 判斷哪個因子失效、誤差有多大
III. 市場情境 | VIX 絕對值, VIX 波動率, 市場 ATR 均值 | daily_prices / 外部 API | 判斷市場處於恐慌、穩定還是狂熱
IV. 籌碼健康度 | L_LAC_dist (偏離度), Dealer Tier Ratio | calculated_factors | 判斷 LAC 的可信度與新資金質量

### 2.3 RL 行動空間

[CONCEPT]
name: RL 行動空間
definition: RL 代理人可以直接調整和優化的參數，包含因子權重調整、買入/賣出閾值、風險敞口等。

[TABLE]
columns: 行動類別 | 參數名稱 | RL 調整範圍 | 影響的底層邏輯
I. 因子權重調整 | BetaC (籌碼權重) | [0.0,1.0] | 調整籌碼因子在最終預測中的影響力
I. 因子權重調整 | BetaS (情緒權重) | [0.0,1.0] | 調整情緒因子在最終預測中的影響力
II. 買入/賣出閾值 | TFear (恐懼買入閾值) | [10,30] | 調整情緒指數需要多恐懼才出手
II. 買入/賣出閾值 | TRunUp_Dev (跑飛偏差賣出閾值) | [0.20,0.50] | 股價偏離 LAC 多少後應觸發獲利了結預警
III. 風險敞口 | Position_Weight | [0.1,1.0] | 調整每筆交易的建議投入資本比例

### 2.4 RL 獎勵函數

[FORMULA]
Reward = Sharpe_Ratio_30_Day - lambda * Max_Drawdown_30_Day

[CONCEPT]
name: Reward 函數
definition: AI 學習的唯一目標。使用風險調整後收益，Sharpe Ratio 30 Day 是主要的獎勵來源，Max Drawdown 30 Day 是主要的懲罰來源。

[RULE]
IF RL 訓練
THEN Reward = Sharpe_Ratio_30_Day - lambda * Max_Drawdown_30_Day

---

## 三、股票判斷方法（技術面 + 基本面）

[NOTE]
本文件主要聚焦於系統架構與 RL 模型設計，技術面與基本面判斷方法請參考其他股市聖經文件。

[CONCEPT]
name: 因子權重調整
definition: 透過 RL 自主調整 BetaC（籌碼權重）和 BetaS（情緒權重），優化因子在最終預測中的影響力。

---

## 四、交易策略

### 4.1 自主演化策略

[RULE]
IF 系統偵測到 Sharpe Ratio 下降，同時 Residual 貢獻度高且 VIX 高漲
THEN RL 代理人根據歷史經驗，發現降低 BetaS 和提高 TFear 的行動帶來最高的 Reward

[RULE]
IF RL 建議的權重調整幅度超過 20%，或將風險敞口調整到極端值（例如 > 0.8）
THEN 需要人工確認決策的合理性

### 4.2 因子權重調整策略

[RULE]
IF 市場情境改變
THEN RL 自主調整 BetaC 和 BetaS 權重，優化因子影響力

[RULE]
IF 因子失效或誤差增大
THEN RL 降低該因子的權重，減少其對最終預測的影響

---

## 五、風險控管與心理面

### 5.1 風險敞口控制

[RULE]
IF RL 訓練
THEN Position_Weight 調整範圍為 [0.1,1.0]，確保風險敞口在可控範圍內

[RULE]
IF RL 建議的風險敞口 > 0.8
THEN 需要人工確認決策的合理性

### 5.2 最大回撤控制

[RULE]
IF Max_Drawdown_30_Day 超過閾值
THEN 觸發 RL 模型重新校準，降低風險敞口

[FORMULA]
Reward = Sharpe_Ratio_30_Day - lambda * Max_Drawdown_30_Day

[CONCEPT]
name: 風險控管
definition: 透過 Reward 函數中的 Max Drawdown 懲罰項，確保 AI 嚴格控制風險。

---

## 六、開盤／盤中／收盤 SOP

### 6.1 盤前流程

[RULE]
IF 盤前啟動
THEN 執行以下步驟：
  1. 收集市場情境數據（VIX、ATR 等）
  2. 計算策略績效指標（Sharpe Ratio、MDD）
  3. 執行因子歸因分析
  4. RL 讀取狀態空間，決定當日參數調整

### 6.2 盤中監控

[RULE]
IF 盤中運行
THEN 持續監控：
  1. 策略績效指標
  2. 因子貢獻度與誤差
  3. 市場情境變化
  4. RL 參數調整效果

### 6.3 盤後處理

[RULE]
IF 盤後
THEN 執行：
  1. 更新策略績效數據
  2. 執行因子歸因分析
  3. 更新 RL 模型（如需要）
  4. 審核 RL 建議的參數調整

---

## 七、實戰案例整理

[NOTE]
以下為系統設計案例，實際交易案例請參考其他文件。

### 案例 1：因子權重自主調整

[RULE]
IF 市場恐慌（VIX 高漲）AND 情緒因子失效（Residual 貢獻度高）
THEN RL 降低 BetaS（情緒權重），提高 TFear（恐懼買入閾值），要求更極端的恐懼才出手

### 案例 2：風險敞口調整

[RULE]
IF Max_Drawdown_30_Day 超過閾值
THEN RL 降低 Position_Weight，減少風險敞口

### 案例 3：人工介入審核

[RULE]
IF RL 建議的權重調整幅度超過 20%
THEN 觸發人工審核環節，確認決策的合理性

---

## 八、AI 補充

### 8.1 系統架構設計

[STRUCTURE]
高階 AI 交易系統架構：
- 第零層：數據輸入與基礎設施
  - 數據源 (APIs)
  - 數據庫 (Storage)
  - 數據清洗與校準
- 第一層：因子計算與多策略模組（Alpha Generation）
- 第二層：因子歸因與診斷（Attribution Engine）
- 第三層：強化學習與自主演化（RL Engine）
- 第四層：執行與風控（Execution & Risk Management）

[NOTE]
AI 補充：詳細的系統架構設計請參考原始文件中的完整藍圖。

### 8.2 自主演化閉環

[CONCEPT]
name: 自主演化閉環
definition: 透過「歸因 → 獎勵 → 行動 → 校準」的閉環，讓 AI 系統具備「自我反思」和「適應市場」的能力。

[RULE]
IF 系統偵測到績效下降
THEN 執行以下閉環：
  1. 因子歸因分析（找出失效因子）
  2. 計算 Reward（評估調整效果）
  3. RL 決策（選擇最佳參數調整）
  4. 執行校準（更新模型參數）

---

## 九、可轉程式化的 J-GOD 規則列表

### 9.1 RL 狀態空間規則

[RULE]
IF RL 訓練
THEN State_Vector = [Sharpe_Ratio_7D, MDD_30D, Residual, FC_Contribution, FS_Contribution, VIX_Abs, VIX_Volatility, Market_ATR, L_LAC_dist, Dealer_Tier_Ratio]

### 9.2 RL 行動空間規則

[RULE]
IF RL 訓練
THEN Action_Space = {
  BetaC: [0.0, 1.0],
  BetaS: [0.0, 1.0],
  TFear: [10, 30],
  TRunUp_Dev: [0.20, 0.50],
  Position_Weight: [0.1, 1.0]
}

### 9.3 RL 獎勵函數規則

[RULE]
IF RL 訓練
THEN Reward = Sharpe_Ratio_30_Day - lambda * Max_Drawdown_30_Day

[RULE]
IF Reward 計算
THEN 使用 30 天的 Sharpe Ratio 和 Max Drawdown 作為主要指標

### 9.4 模型校準規則

[RULE]
IF Sharpe_Ratio 下降 AND Residual_Contribution 高 AND VIX 高漲
THEN trigger_RL_calibration = True

[RULE]
IF RL 校準完成
THEN 將新參數寫入 AttributionResult 表格，更新到實盤交易模型

### 9.5 人工介入規則

[RULE]
IF RL_Weight_Adjustment > 20% OR Position_Weight > 0.8
THEN require_human_approval = True

[RULE]
IF require_human_approval = True
THEN 暫停自動更新，等待人工確認

---

## 附錄：系統架構詳解

### A. 數據輸入與基礎設施

[STRUCTURE]
第零層模組：
- 數據源 (APIs)：提供股價、交易量、財報、法人籌碼、社交媒體文本
- 數據庫 (Storage)：PostgreSQL/MySQL 搭配 SQLAlchemy ORM，採用三層 Schema 架構
- 數據清洗與校準：Python DataCleaner 模組，執行時區校準、缺失值處理、異常值過濾和除權息復權

### B. 因子計算與多策略模組

[STRUCTURE]
第一層模組：
- 因子計算引擎
- 多策略組合
- Alpha Generation

### C. 因子歸因與診斷

[STRUCTURE]
第二層模組：
- Attribution Engine
- 因子貢獻度分析
- 誤差診斷

### D. 強化學習與自主演化

[STRUCTURE]
第三層模組：
- RL Engine
- Model Calibration Engine
- 自主演化閉環

### E. 執行與風控

[STRUCTURE]
第四層模組：
- Execution Engine
- Risk Management
- 人工介入審核

---

## 總結

[NOTE]
本文件完整保留了原始內容的所有技術細節，包括：
- 所有 RL 模型設計
- 所有系統架構設計
- 所有公式與計算邏輯
- 所有規則與流程

[CONCEPT]
name: 雙引擎與自主演化閉環
definition: 一個具備自主演化能力的 AI 交易系統，透過強化學習實現從數據輸入到自主學習的完整閉環。

[NOTE]
本文件旨在實現系統的「自我反思」和「適應市場」能力，達到真正的自主演化。

---

*文件建立時間：2024年*
*版本：v1*
*原始文件：雙引擎與自主演化閉環.txt（未修改）*

