#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 4: MASTER_INDEX Builder

å¾ Phase 1-3 çš„æ‰€æœ‰ CORRECTED æ–‡ä»¶ä¸­æå–çŸ¥è­˜ç¯€é»ï¼Œå»ºç«‹çµ±ä¸€çš„ MASTER_INDEXã€‚

è™•ç†æµç¨‹ï¼š
1. æƒæ corrected_md/ ä¸­æ‰€æœ‰å·²æ ¡æ­£ MDï¼ˆå¯¦éš›ä¸Šæ˜¯ structured_books/*_CORRECTED.mdï¼‰
2. ä½¿ç”¨ extractors è§£æç¯€é»
3. å»ºç«‹å®Œæ•´çš„ç´¢å¼• dict
4. ç”¢ç”Ÿ JSONL + Markdown å…©ç¨®æ ¼å¼
5. å®‰å…¨é˜²å‘†ï¼ˆé‡è¤‡ idã€ç¼ºæ¬„ä½ã€ç©ºç¯€é»ï¼‰
"""

from __future__ import annotations

import re
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, field, asdict
from collections import defaultdict
import sys

# å°ˆæ¡ˆæ ¹ç›®éŒ„
REPO_ROOT = Path(__file__).parent.parent
STRUCTURED_BOOKS_DIR = REPO_ROOT / "structured_books"
KNOWLEDGE_BASE_DIR = REPO_ROOT / "knowledge_base"
DOCS_DIR = REPO_ROOT / "docs"


@dataclass
class MasterIndexItem:
    """MASTER_INDEX å–®ä¸€é …ç›®è³‡æ–™çµæ§‹"""
    id: str
    type: str  # RULE / FORMULA / CONCEPT / STRUCTURE / TABLE / CODE / NOTE
    title: str
    source_file: str  # åŸå§‹ MD æ–‡ä»¶åç¨±
    source_phase: str  # STRUCTURED / ENHANCED / CORRECTED
    tags: List[str] = field(default_factory=list)
    description: str = ""
    related_ids: List[str] = field(default_factory=list)
    path: str = ""  # æª”æ¡ˆä½ç½®ï¼ˆç›¸å°è·¯å¾‘ï¼‰
    version: str = "v1"
    line_range: Optional[Tuple[int, int]] = None
    raw_text: str = ""
    structured: Optional[Dict] = None
    
    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸ï¼ˆç”¨æ–¼ JSON è¼¸å‡ºï¼‰"""
        result = asdict(self)
        # è™•ç† Optional æ¬„ä½
        if result["line_range"] is None:
            del result["line_range"]
        if result["structured"] is None:
            del result["structured"]
        return result


class KnowledgeNodeExtractor:
    """çŸ¥è­˜ç¯€é»æå–å™¨
    
    å¾ CORRECTED Markdown æ–‡ä»¶ä¸­æå–çµæ§‹åŒ–çš„çŸ¥è­˜ç¯€é»ã€‚
    """
    
    # ç¯€é»é¡å‹æ¨™è¨˜æ¨¡å¼
    TYPE_PATTERNS = {
        "RULE": re.compile(r'\*\*\[RULE\]\*\*', re.IGNORECASE),
        "FORMULA": re.compile(r'\*\*\[FORMULA\]\*\*', re.IGNORECASE),
        "CONCEPT": re.compile(r'\*\*\[CONCEPT\]\*\*', re.IGNORECASE),
        "STRUCTURE": re.compile(r'\*\*\[STRUCTURE\]\*\*', re.IGNORECASE),
        "TABLE": re.compile(r'\*\*\[TABLE\]\*\*', re.IGNORECASE),
        "CODE": re.compile(r'\*\*\[CODE\]\*\*', re.IGNORECASE),
        "NOTE": re.compile(r'\*\*\[NOTE\]\*\*', re.IGNORECASE),
    }
    
    # è‡ªå‹•è­˜åˆ¥æ¨¡å¼
    FORMULA_PATTERN = re.compile(r'\$\$.*?\$\$|\$[^\n$]+\$', re.DOTALL)
    CODE_BLOCK_PATTERN = re.compile(r'```(\w+)?\n.*?```', re.DOTALL)
    TABLE_PATTERN = re.compile(r'\|.*\|.*\n\|[-\s\|]+\|', re.MULTILINE)
    
    def __init__(self):
        """åˆå§‹åŒ–æå–å™¨"""
        self.nodes: List[Dict] = []
    
    def extract_from_file(self, file_path: Path, source_phase: str = "CORRECTED") -> List[Dict]:
        """
        å¾å–®ä¸€æ–‡ä»¶ä¸­æå–æ‰€æœ‰çŸ¥è­˜ç¯€é»
        
        Args:
            file_path: Markdown æ–‡ä»¶è·¯å¾‘
            source_phase: ä¾†æºéšæ®µï¼ˆSTRUCTURED / ENHANCED / CORRECTEDï¼‰
        
        Returns:
            çŸ¥è­˜ç¯€é»åˆ—è¡¨
        """
        if not file_path.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        
        print(f"ğŸ“– è™•ç†æ–‡ä»¶: {file_path.name}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            content = ''.join(lines)
        
        nodes = []
        current_node = None
        current_type = None
        current_lines = []
        start_line = 0
        
        source_file_basename = file_path.stem.replace('_CORRECTED', '').replace('_ENHANCED', '').replace('_STRUCTURED', '')
        
        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç¯€é»é¡å‹æ¨™è¨˜
            node_type = self._detect_node_type(stripped)
            
            if node_type:
                # ä¿å­˜å‰ä¸€å€‹ç¯€é»
                if current_node and current_type:
                    node_dict = self._finalize_node(
                        current_node, current_type, current_lines, 
                        start_line, line_num - 1, file_path, source_file_basename, source_phase
                    )
                    if node_dict:
                        nodes.append(node_dict)
                
                # é–‹å§‹æ–°ç¯€é»
                current_type = node_type
                current_lines = [line]
                start_line = line_num
                current_node = {
                    "type": node_type,
                    "raw_lines": []
                }
            elif current_node is not None:
                # ç´¯ç©ç¯€é»å…§å®¹
                current_lines.append(line)
                current_node["raw_lines"].append(line)
            else:
                # æª¢æŸ¥æ˜¯å¦ç‚ºè‡ªå‹•è­˜åˆ¥çš„ç¯€é»ï¼ˆç„¡æ¨™è¨˜çš„å…¬å¼ã€ç¨‹å¼ç¢¼ç­‰ï¼‰
                auto_node = self._detect_auto_node(stripped, line_num, file_path, source_file_basename, source_phase)
                if auto_node:
                    nodes.append(auto_node)
        
        # ä¿å­˜æœ€å¾Œä¸€å€‹ç¯€é»
        if current_node and current_type:
            node_dict = self._finalize_node(
                current_node, current_type, current_lines,
                start_line, len(lines), file_path, source_file_basename, source_phase
            )
            if node_dict:
                nodes.append(node_dict)
        
        print(f"  âœ… æå–äº† {len(nodes)} å€‹çŸ¥è­˜ç¯€é»")
        return nodes
    
    def _detect_node_type(self, line: str) -> Optional[str]:
        """åµæ¸¬ç¯€é»é¡å‹æ¨™è¨˜"""
        for node_type, pattern in self.TYPE_PATTERNS.items():
            if pattern.search(line):
                return node_type
        return None
    
    def _detect_auto_node(self, line: str, line_num: int, file_path: Path, 
                          source_file: str, source_phase: str) -> Optional[Dict]:
        """è‡ªå‹•è­˜åˆ¥ç„¡æ¨™è¨˜çš„ç¯€é»ï¼ˆå…¬å¼ã€ç¨‹å¼ç¢¼ç­‰ï¼‰"""
        # æª¢æŸ¥å…¬å¼
        if self.FORMULA_PATTERN.search(line):
            return {
                "type": "FORMULA",
                "raw_lines": [line],
                "start_line": line_num,
                "end_line": line_num,
                "source_file": source_file,
                "source_phase": source_phase,
                "path": str(file_path.relative_to(REPO_ROOT))
            }
        
        # æª¢æŸ¥ç¨‹å¼ç¢¼å€å¡Šï¼ˆç°¡åŒ–ç‰ˆï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„é‚è¼¯ï¼‰
        if self.CODE_BLOCK_PATTERN.search(line):
            return {
                "type": "CODE",
                "raw_lines": [line],
                "start_line": line_num,
                "end_line": line_num,
                "source_file": source_file,
                "source_phase": source_phase,
                "path": str(file_path.relative_to(REPO_ROOT))
            }
        
        return None
    
    def _finalize_node(self, node: Dict, node_type: str, lines: List[str],
                       start_line: int, end_line: int, file_path: Path,
                       source_file: str, source_phase: str) -> Optional[Dict]:
        """å®Œæˆç¯€é»çš„æå–å’Œçµæ§‹åŒ–"""
        raw_text = ''.join(lines).strip()
        
        if not raw_text or len(raw_text) < 10:  # éæ¿¾ç©ºç¯€é»
            return None
        
        # æå–æ¨™é¡Œ
        title = self._extract_title(lines)
        
        # æå–æ¨™ç±¤
        tags = self._extract_tags(lines, raw_text)
        
        # æå–æè¿°ï¼ˆå‰ 200 å­—å…ƒï¼‰
        description = self._extract_description(raw_text)
        
        # è§£æçµæ§‹åŒ–è³‡æ–™
        structured = self._parse_structured(node_type, raw_text, lines)
        
        return {
            "type": node_type,
            "title": title,
            "source_file": source_file,
            "source_phase": source_phase,
            "tags": tags,
            "description": description,
            "line_range": (start_line, end_line),
            "path": str(file_path.relative_to(REPO_ROOT)),
            "raw_text": raw_text,
            "structured": structured
        }
    
    def _extract_title(self, lines: List[str]) -> str:
        """æå–æ¨™é¡Œ"""
        for line in lines[:10]:  # åªæª¢æŸ¥å‰ 10 è¡Œ
            stripped = line.strip()
            # æª¢æŸ¥ Markdown æ¨™é¡Œ
            if stripped.startswith('#'):
                return stripped.lstrip('#').strip()
            # æª¢æŸ¥ç²—é«”æ–‡å­—
            if stripped.startswith('**') and stripped.endswith('**'):
                title = stripped[2:-2].strip()
                if len(title) < 100:  # é¿å…æå–éé•·çš„å…§å®¹
                    return title
            # æª¢æŸ¥ç¬¬ä¸€è¡Œéç©ºæ–‡å­—
            if stripped and not stripped.startswith('[') and len(stripped) < 100:
                return stripped[:100]
        return "æœªå‘½åç¯€é»"
    
    def _extract_tags(self, lines: List[str], raw_text: str) -> List[str]:
        """æå–æ¨™ç±¤"""
        tags = set()
        
        # å¾å…§å®¹ä¸­è‡ªå‹•è­˜åˆ¥æ¨™ç±¤
        text_lower = raw_text.lower()
        
        tag_keywords = {
            "risk": ["é¢¨éšª", "é¢¨æ§", "åœæ", "drawdown", "risk"],
            "strategy": ["ç­–ç•¥", "strategy", "äº¤æ˜“", "trading"],
            "entry": ["é€²å ´", "è²·å…¥", "entry", "buy"],
            "exit": ["å‡ºå ´", "è³£å‡º", "exit", "sell"],
            "performance": ["ç¸¾æ•ˆ", "å ±é…¬", "performance", "return", "sharpe"],
            "path_a": ["path a", "å›æ¸¬", "backtest", "æ­·å²"],
            "alpha": ["alpha", "å› å­", "factor"],
            "optimizer": ["optimizer", "å„ªåŒ–", "optimization"],
        }
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.add(tag)
        
        return sorted(list(tags))
    
    def _extract_description(self, raw_text: str, max_length: int = 200) -> str:
        """æå–æè¿°ï¼ˆè‡ªå‹•æ‘˜è¦ï¼‰"""
        # ç§»é™¤ Markdown æ ¼å¼
        text = re.sub(r'#{1,6}\s+', '', raw_text)  # ç§»é™¤æ¨™é¡Œæ¨™è¨˜
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # ç§»é™¤ç²—é«”
        text = re.sub(r'`([^`]+)`', r'\1', text)  # ç§»é™¤ç¨‹å¼ç¢¼æ¨™è¨˜
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)  # ç§»é™¤é€£çµ
        text = re.sub(r'\$\$.*?\$\$', '', text, flags=re.DOTALL)  # ç§»é™¤å…¬å¼
        
        # æå–å‰ max_length å­—å…ƒ
        description = text.strip()[:max_length]
        
        # ç¢ºä¿åœ¨å–®è©é‚Šç•Œæˆªæ–·
        if len(text) > max_length:
            last_space = description.rfind(' ')
            if last_space > max_length * 0.8:  # åªåœ¨ 80% å¾Œæ‰æˆªæ–·
                description = description[:last_space]
        
        return description.strip()
    
    def _parse_structured(self, node_type: str, raw_text: str, lines: List[str]) -> Optional[Dict]:
        """æ ¹æ“šç¯€é»é¡å‹è§£æçµæ§‹åŒ–è³‡æ–™"""
        if node_type == "FORMULA":
            return self._parse_formula(raw_text)
        elif node_type == "RULE":
            return self._parse_rule(raw_text)
        elif node_type == "CONCEPT":
            return self._parse_concept(raw_text)
        elif node_type == "CODE":
            return self._parse_code(raw_text)
        elif node_type == "TABLE":
            return self._parse_table(raw_text)
        else:
            return None
    
    def _parse_formula(self, raw_text: str) -> Dict:
        """è§£æå…¬å¼"""
        # æå– LaTeX å…¬å¼
        formulas = self.FORMULA_PATTERN.findall(raw_text)
        
        expression = formulas[0] if formulas else ""
        
        # æå–è®Šæ•¸èªªæ˜ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        variables = {}
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„è®Šæ•¸æå–é‚è¼¯
        
        return {
            "expression": expression,
            "variables": variables,
            "notes": ""
        }
    
    def _parse_rule(self, raw_text: str) -> Dict:
        """è§£æè¦å‰‡"""
        # ç°¡åŒ–ç‰ˆï¼šå¾æ–‡å­—ä¸­æå– if-then é‚è¼¯
        # å¯¦éš›éœ€è¦æ›´è¤‡é›œçš„ NLP è™•ç†
        
        # å°‹æ‰¾ã€Œå¦‚æœã€ã€Œç•¶ã€ã€Œå‰‡ã€ã€Œæ‡‰è©²ã€ç­‰é—œéµå­—
        if_match = re.search(r'(å¦‚æœ|ç•¶|è‹¥|ç•¶.*?æ™‚)(.+?)(å‰‡|æ‡‰è©²|å¿…é ˆ|è¦)', raw_text, re.DOTALL)
        
        condition = if_match.group(2).strip() if if_match else ""
        action = ""
        
        return {
            "if": condition,
            "then": action,
            "priority": 5,  # é è¨­å„ªå…ˆç´š
            "scope": "general"
        }
    
    def _parse_concept(self, raw_text: str) -> Dict:
        """è§£ææ¦‚å¿µ"""
        # æå–å®šç¾©å’Œç¯„ä¾‹
        definition = self._extract_description(raw_text, 500)
        examples = []
        
        # å°‹æ‰¾ç¯„ä¾‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        example_pattern = re.compile(r'ç¯„ä¾‹[ï¼š:]\s*(.+?)(?:\n|$)', re.IGNORECASE)
        examples_match = example_pattern.findall(raw_text)
        if examples_match:
            examples = [ex.strip() for ex in examples_match[:3]]
        
        return {
            "name": self._extract_title([raw_text.split('\n')[0]]),
            "definition": definition,
            "examples": examples
        }
    
    def _parse_code(self, raw_text: str) -> Dict:
        """è§£æç¨‹å¼ç¢¼"""
        # æå–ç¨‹å¼ç¢¼å€å¡Š
        code_match = self.CODE_BLOCK_PATTERN.search(raw_text)
        
        if code_match:
            language = code_match.group(1) or "python"
            code = code_match.group(0).strip('`').strip()
            # ç§»é™¤èªè¨€æ¨™è¨˜
            code = re.sub(r'^(\w+)\n', '', code, flags=re.MULTILINE)
            return {
                "language": language,
                "code": code
            }
        
        return {
            "language": "python",
            "code": raw_text
        }
    
    def _parse_table(self, raw_text: str) -> Dict:
        """è§£æè¡¨æ ¼"""
        # ç°¡åŒ–ç‰ˆï¼šæå–è¡¨æ ¼çµæ§‹
        table_match = self.TABLE_PATTERN.search(raw_text)
        
        if table_match:
            table_text = table_match.group(0)
            lines = [l.strip() for l in table_text.split('\n') if '|' in l]
            
            if len(lines) >= 2:
                columns = [col.strip() for col in lines[0].split('|') if col.strip()]
                rows = []
                for line in lines[2:]:  # è·³éåˆ†éš”ç·š
                    cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                    if cells:
                        rows.append(cells)
                
                return {
                    "columns": columns,
                    "rows": rows
                }
        
        return {
            "columns": [],
            "rows": []
        }


class MasterIndexBuilder:
    """MASTER_INDEX å»ºæ§‹å™¨
    
    æ•´åˆæ‰€æœ‰çŸ¥è­˜ç¯€é»ï¼Œå»ºç«‹çµ±ä¸€çš„ç´¢å¼•ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å»ºæ§‹å™¨"""
        self.extractor = KnowledgeNodeExtractor()
        self.master_index: Dict[str, MasterIndexItem] = {}
        self.by_type: Dict[str, List[str]] = defaultdict(list)
        self.by_source_file: Dict[str, List[str]] = defaultdict(list)
        self.by_tags: Dict[str, List[str]] = defaultdict(list)
        self.id_counter: Dict[str, int] = defaultdict(int)
        self.used_ids: Set[str] = set()
    
    def scan_corrected_files(self) -> List[Path]:
        """æƒææ‰€æœ‰ CORRECTED æ–‡ä»¶"""
        corrected_files = list(STRUCTURED_BOOKS_DIR.glob("*_CORRECTED.md"))
        print(f"ğŸ“š æ‰¾åˆ° {len(corrected_files)} å€‹ CORRECTED æ–‡ä»¶")
        return corrected_files
    
    def build_index(self) -> None:
        """å»ºç«‹å®Œæ•´ç´¢å¼•"""
        print("=" * 60)
        print("é–‹å§‹å»ºç«‹ MASTER_INDEX...")
        print("=" * 60)
        
        # æƒææ–‡ä»¶
        corrected_files = self.scan_corrected_files()
        
        # å¾æ¯å€‹æ–‡ä»¶æå–ç¯€é»
        all_nodes = []
        for file_path in corrected_files:
            nodes = self.extractor.extract_from_file(file_path, source_phase="CORRECTED")
            all_nodes.extend(nodes)
        
        print(f"\nğŸ“Š ç¸½å…±æå–äº† {len(all_nodes)} å€‹çŸ¥è­˜ç¯€é»")
        
        # è½‰æ›ç‚º MasterIndexItem ä¸¦åˆ†é… ID
        for node in all_nodes:
            item = self._create_master_index_item(node)
            if item:
                self.master_index[item.id] = item
                self._update_indices(item)
        
        print(f"âœ… å»ºç«‹äº† {len(self.master_index)} å€‹ç´¢å¼•é …ç›®")
        
        # å»ºç«‹é—œè¯é—œä¿‚
        self._build_relationships()
        
        # å®‰å…¨é˜²å‘†æª¢æŸ¥
        self._validate_index()
    
    def _create_master_index_item(self, node: Dict) -> Optional[MasterIndexItem]:
        """å»ºç«‹ MasterIndexItem"""
        node_type = node.get("type", "NOTE")
        source_file = node.get("source_file", "unknown")
        
        # ç”Ÿæˆå”¯ä¸€ ID
        item_id = self._generate_unique_id(node_type, source_file)
        
        # å»ºç«‹é …ç›®
        item = MasterIndexItem(
            id=item_id,
            type=node_type,
            title=node.get("title", "æœªå‘½å"),
            source_file=source_file,
            source_phase=node.get("source_phase", "CORRECTED"),
            tags=node.get("tags", []),
            description=node.get("description", ""),
            related_ids=[],  # ç¨å¾Œå»ºç«‹
            path=node.get("path", ""),
            version="v1",
            line_range=node.get("line_range"),
            raw_text=node.get("raw_text", ""),
            structured=node.get("structured")
        )
        
        return item
    
    def _generate_unique_id(self, node_type: str, source_file: str) -> str:
        """ç”Ÿæˆå”¯ä¸€ ID"""
        # ç°¡åŒ–æª”å
        file_base = source_file.replace('_AIçŸ¥è­˜åº«ç‰ˆ_v1', '').replace('_CORRECTED', '')
        file_base = file_base.replace(' ', '_').replace('-', '_')
        file_base = re.sub(r'[^\w]', '', file_base)
        
        # è¨ˆæ•¸å™¨
        self.id_counter[f"{node_type}_{file_base}"] += 1
        seq = self.id_counter[f"{node_type}_{file_base}"]
        
        # ç”Ÿæˆ ID
        item_id = f"{node_type}_{file_base}_{seq:03d}"
        
        # æª¢æŸ¥é‡è¤‡
        if item_id in self.used_ids:
            # æ·»åŠ å¾Œç¶´
            counter = 1
            while f"{item_id}_dup{counter}" in self.used_ids:
                counter += 1
            item_id = f"{item_id}_dup{counter}"
        
        self.used_ids.add(item_id)
        return item_id
    
    def _update_indices(self, item: MasterIndexItem) -> None:
        """æ›´æ–°åå‘ç´¢å¼•"""
        self.by_type[item.type].append(item.id)
        self.by_source_file[item.source_file].append(item.id)
        for tag in item.tags:
            self.by_tags[tag].append(item.id)
    
    def _build_relationships(self) -> None:
        """å»ºç«‹é—œè¯é—œä¿‚"""
        print("\nğŸ”— å»ºç«‹é—œè¯é—œä¿‚...")
        
        # ç°¡åŒ–ç‰ˆï¼šæ ¹æ“šæ¨™é¡Œå’Œæè¿°ä¸­çš„é—œéµå­—åŒ¹é…
        # å¯¦éš›éœ€è¦æ›´è¤‡é›œçš„ NLP è™•ç†
        
        all_items = list(self.master_index.values())
        
        for item in all_items:
            related = []
            
            # æª¢æŸ¥å…¶ä»–é …ç›®ä¸­æ˜¯å¦æœ‰ç›¸é—œçš„
            for other_item in all_items:
                if other_item.id == item.id:
                    continue
                
                # ç°¡å–®çš„é—œéµå­—åŒ¹é…
                if self._is_related(item, other_item):
                    related.append(other_item.id)
            
            item.related_ids = related[:5]  # é™åˆ¶æœ€å¤š 5 å€‹é—œè¯
        
        total_relations = sum(len(item.related_ids) for item in all_items)
        print(f"  âœ… å»ºç«‹äº† {total_relations} å€‹é—œè¯é—œä¿‚")
    
    def _is_related(self, item1: MasterIndexItem, item2: MasterIndexItem) -> bool:
        """åˆ¤æ–·å…©å€‹é …ç›®æ˜¯å¦ç›¸é—œ"""
        # ç°¡å–®çš„é—œéµå­—åŒ¹é…ï¼ˆå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„ NLPï¼‰
        
        # æª¢æŸ¥æ¨™é¡Œç›¸ä¼¼åº¦
        title1_words = set(item1.title.lower().split())
        title2_words = set(item2.title.lower().split())
        if len(title1_words & title2_words) >= 2:
            return True
        
        # æª¢æŸ¥æ¨™ç±¤é‡ç–Š
        if set(item1.tags) & set(item2.tags):
            return True
        
        # æª¢æŸ¥æè¿°ä¸­çš„é—œéµå­—
        desc1_words = set(item1.description.lower().split())
        desc2_words = set(item2.description.lower().split())
        common_words = desc1_words & desc2_words
        if len(common_words) >= 3:
            return True
        
        return False
    
    def _validate_index(self) -> None:
        """é©—è­‰ç´¢å¼•ï¼ˆå®‰å…¨é˜²å‘†ï¼‰"""
        print("\nğŸ” é©—è­‰ç´¢å¼•...")
        
        issues = []
        
        for item_id, item in self.master_index.items():
            # æª¢æŸ¥å¿…å¡«æ¬„ä½
            if not item.id:
                issues.append(f"{item_id}: ç¼ºå°‘ ID")
            if not item.type:
                issues.append(f"{item_id}: ç¼ºå°‘ type")
            if not item.title:
                issues.append(f"{item_id}: ç¼ºå°‘ title")
            if not item.raw_text or len(item.raw_text) < 10:
                issues.append(f"{item_id}: raw_text å¤ªçŸ­æˆ–ç‚ºç©º")
        
        if issues:
            print(f"  âš ï¸  ç™¼ç¾ {len(issues)} å€‹å•é¡Œ:")
            for issue in issues[:10]:  # åªé¡¯ç¤ºå‰ 10 å€‹
                print(f"    - {issue}")
            if len(issues) > 10:
                print(f"    ... é‚„æœ‰ {len(issues) - 10} å€‹å•é¡Œ")
        else:
            print("  âœ… ç´¢å¼•é©—è­‰é€šé")
    
    def export_jsonl(self, output_path: Path) -> None:
        """åŒ¯å‡ºç‚º JSONL æ ¼å¼"""
        print(f"\nğŸ’¾ åŒ¯å‡º JSONL: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in self.master_index.values():
                json_line = json.dumps(item.to_dict(), ensure_ascii=False)
                f.write(json_line + '\n')
        
        print(f"  âœ… å·²åŒ¯å‡º {len(self.master_index)} å€‹é …ç›®")
    
    def export_markdown(self, output_path: Path) -> None:
        """åŒ¯å‡ºç‚º Markdown æ ¼å¼"""
        print(f"\nğŸ’¾ åŒ¯å‡º Markdown: {output_path}")
        
        lines = []
        
        # æ¨™é¡Œ
        lines.append("# J-GOD MASTER_INDEX v1\n")
        lines.append("> **èªªæ˜**ï¼šæœ¬ç´¢å¼•æ•´åˆäº† Phase 1-3 æ‰€æœ‰çµæ§‹åŒ–çŸ¥è­˜ç¯€é»ã€‚\n")
        lines.append("> **ç”Ÿæˆæ™‚é–“**ï¼šè‡ªå‹•ç”Ÿæˆ\n")
        lines.append("\n---\n\n")
        
        # ç¸½è¦½
        lines.append("## ğŸ“Š ç¸½è¦½\n\n")
        lines.append(f"- **ç¸½ç¯€é»æ•¸**ï¼š{len(self.master_index)}\n")
        lines.append(f"- **æŒ‰é¡å‹çµ±è¨ˆ**ï¼š\n")
        for node_type, items in sorted(self.by_type.items()):
            lines.append(f"  - {node_type}: {len(items)} å€‹\n")
        lines.append(f"- **æŒ‰ä¾†æºæ–‡ä»¶çµ±è¨ˆ**ï¼š{len(self.by_source_file)} å€‹æ–‡ä»¶\n")
        lines.append(f"- **æ¨™ç±¤æ•¸é‡**ï¼š{len(self.by_tags)} å€‹\n\n")
        
        # æŒ‰é¡å‹ç€è¦½
        lines.append("## ğŸ” æŒ‰é¡å‹ç€è¦½\n\n")
        for node_type in sorted(self.by_type.keys()):
            lines.append(f"### {node_type}\n\n")
            item_ids = self.by_type[node_type]
            for item_id in item_ids[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
                item = self.master_index[item_id]
                lines.append(f"- **[{item_id}]** {item.title}\n")
                lines.append(f"  - ä¾†æºï¼š{item.source_file}\n")
                if item.tags:
                    lines.append(f"  - æ¨™ç±¤ï¼š{', '.join(item.tags)}\n")
                if item.related_ids:
                    lines.append(f"  - ç›¸é—œï¼š{', '.join(item.related_ids[:3])}\n")
                lines.append(f"  - æè¿°ï¼š{item.description[:100]}...\n\n")
            if len(item_ids) > 20:
                lines.append(f"  ... é‚„æœ‰ {len(item_ids) - 20} å€‹é …ç›®\n\n")
        
        # ç´¢å¼•åˆ—è¡¨
        lines.append("## ğŸ“‘ å®Œæ•´ç´¢å¼•åˆ—è¡¨\n\n")
        lines.append("### æŒ‰ ID æ’åº\n\n")
        for item_id in sorted(self.master_index.keys()):
            item = self.master_index[item_id]
            lines.append(f"- [{item_id}] {item.title} ({item.type})\n")
        
        # å¯«å…¥æª”æ¡ˆ
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        
        print(f"  âœ… å·²åŒ¯å‡º Markdown æ–‡ä»¶ ({len(lines)} è¡Œ)")


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("Phase 4: MASTER_INDEX Builder")
    print("=" * 60)
    
    # å»ºç«‹å»ºæ§‹å™¨
    builder = MasterIndexBuilder()
    
    # å»ºç«‹ç´¢å¼•
    builder.build_index()
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    KNOWLEDGE_BASE_DIR.mkdir(parents=True, exist_ok=True)
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    
    # åŒ¯å‡º JSONL
    jsonl_path = KNOWLEDGE_BASE_DIR / "jgod_master_index_v1.jsonl"
    builder.export_jsonl(jsonl_path)
    
    # åŒ¯å‡º Markdown
    md_path = DOCS_DIR / "J-GOD_MASTER_INDEX_v1.md"
    builder.export_markdown(md_path)
    
    print("\n" + "=" * 60)
    print("âœ… Phase 4: MASTER_INDEX å»ºç«‹å®Œæˆï¼")
    print("=" * 60)
    print(f"\nè¼¸å‡ºæª”æ¡ˆï¼š")
    print(f"  - JSONL: {jsonl_path}")
    print(f"  - Markdown: {md_path}")


if __name__ == "__main__":
    main()

